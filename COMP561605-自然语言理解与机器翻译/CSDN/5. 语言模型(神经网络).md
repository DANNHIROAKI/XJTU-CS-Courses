[toc]
有关[$\text{Github}$仓库](https://github.com/DANNHIROAKI/XJTU-CS-Courses)，欢迎来$\text{Star}$ 

# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://i-blog.csdnimg.cn/direct/b33c1e3773074243844bb1daed7cb04f.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://i-blog.csdnimg.cn/direct/cd2eafe2b6dd4819b1d7ac337df7b23a.png" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{1. }$词表示$\textbf{\&NN}$语言模型

> ## $\textbf{1.1. }$离散式表示
>
> > :one:$\text{One-Hot}$编码
> >
> > 1. 构建方法：
> >
> >    - 构建词汇表：包含所有要处理的独立词，表大小(词数量)决定了向量维度
> >
> >      ```text
> >      Vocabulary = [apple, banana, pineapple]
> >      ```
> >
> >    - 向量分配：每个词分配一个唯一的向量，即每词对应一个维度并设为$1$(其余设为$0$)
> >
> >      ```txt
> >      [1,0,0] = apple
> >      [0,1,0] = banana
> >      [0,0,1] = pineapple
> >      ```
> >
> > 2. 缺点：高维且稀疏，无法体现语义关系(例如$\text{cos}\langle\text{apple, banana}\rangle\text{=0}$即使这二者关系很大)
> >
> > :two:$\text{WordNet}$ 
> >
> > 1. 语义关系
> >
> >    |                      关系                      |                            含义                            |                    实例                     |
> >    | :--------------------------------------------: | :--------------------------------------------------------: | :-----------------------------------------: |
> >    | 上位词$\text{}\xleftrightarrow{}\text{}$下位词 | 更一般的概念$\text{}\xleftrightarrow{}\text{}$更具体的概念 | $\text{animal}\xleftrightarrow{}\text{dog}$ |
> >    | 部分类$\text{}\xleftrightarrow{}\text{}$整体类 | 某物组成部分$\text{}\xleftrightarrow{}\text{}$某物整体组合 | $\text{wheel}\xleftrightarrow{}\text{car}$  |
> >    |                     反义词                     |                        意义相反的词                        | $\text{small}\xleftrightarrow{}\text{big}$  |
> >    |                     多义词                     |                     一个词具有多重含义                     |         $\text{bank}$表示河岸和银行         |
> >
> > 2. $\text{WordNet}$概述：
> >
> >    - 概念：一大型英语词汇库，将名词$/$动词$/$形容词$/$副词组织为一系列同义词集(如下)
> >
> >      ```txt
> >      car, automobile, machine, motorcar
> >      ```
> >
> >    - 层次：语义网络，即结点(同义词集)$+$边(同义词集间的语义关系)
> >
> > 3. 缺点：更新困难$/$设计时具有主观性$/$多义词的存在......
>
> ## $\textbf{1.2. }$分布式表示
>
> > :one:基本概念​
> >
> > 1. 目的：将词$/$句子$\xrightarrow{编码}$稠密低维向量
> > 2. 核心：通过词的上下文(**词固定窗口范围内的内容**)提取词的含义，并将其含义编码在自身向量中 
> >
> > :two:相似性度量：余弦相似度$\text{cosine}(A, B)\text{=}\cfrac{A \text{×} B}{\|A\|\|B\|}$ 
> >
> > :three:分布式模型：基于神经网络的语言模型
> >
> > |      原理      | 实例                                     |
> > | :------------: | ---------------------------------------- |
> > |    基于预测    | $\text{Word2Vec(CBOW, Skip-gram)/GloVe}$ |
> > | 基于上下文表示 | $\text{BiLSTM/BERT}$                     |
>
> ## $\textbf{1.3. }$神经网络语言模型的结构
>
> > <img src="https://i-blog.csdnimg.cn/direct/3722bf55740d461aa2134e5eddacce3a.png" alt="image-20241120203700965" width=500 />  
> >
> > |  步骤  | 描述                                                         |
> > | :----: | ------------------------------------------------------------ |
> > | 词向量 | 通过分布式表示等，得到$\{x^{(1)}, x^{(2)}, \text{...}, x^{(t)}\}$ |
> > | 词嵌入 | $\{x^{(1)}, x^{(2)}, \text{...}, x^{(t)}\}\xrightarrow{\text{embedding}}\{e^{(1)}, e^{(2)},e^{(3)}, \text{...}\}$ |
> > | 隐藏层 | 获得$h\text{=}f(We\text{+}b_1)$                              |
> > | 输出层 | 获得$\hat{y}\text{=Softmax}\left(U h\text{+}b_2\right) \text{ ∈ }\mathbb{R}^{|V|}$ |

# $\textbf{2. Word2vec}$模型

> ## $\textbf{2.1. }$模型概述
>
> > :one:基本思想：通过词的上下文来学习其语义，而每个单一词向量无具体含义
> >
> > <img src="https://i-blog.csdnimg.cn/direct/3728b85f33574e32b330325bfb10fc72.png" alt="image-20241122081043926" width=550/> 
> >
> > 1. 构建词汇表：大小固定，其中每个词用词向量表示
> > 2. 文本的表示：每个词的位置$t$被视为中心词$c$，词$t$所在窗口内其它词视为上下文$o$
> > 3. 优化的途径：不断计算$c/o$之间的相似度$P(o|c)$或$P(c|o)$，调整$c/o$词向量使概率最大化
> >
> > :two:目标函数：给定待优化参数集$\theta$和上下文窗口$[-m,m]$
> >
> > 1. 最大似然：$\displaystyle{}L(\theta)\text{}=\prod_{t=1}^T \prod_{\substack{-m \leq j \leq m}} P\left(w_{t+j} | w_t ; \theta\right)$
> >
> >    |                             公式                             | 含义                                                |
> >    | :----------------------------------------------------------: | --------------------------------------------------- |
> >    | $\displaystyle{} \prod_{\substack{-m \leq j \leq m}} P\left(w_{t+j} \mid w_t ; \theta\right)$ | 以$t$为中心位置，生成$[-m,m]$范围内所有上下文的概率 |
> >    | $\displaystyle{}\prod_{t=1}^T \prod_{\substack{-m \leq j \leq m}} P\left(w_{t+j} \mid w_t ; \theta\right)$ | 在句子中滑动$t$，将每个$t$位置的上下文生成概率累乘  |
> >
> > 2. 目标函数：$\displaystyle{}J(\theta)\text{=}-\cfrac{1}{T}\log{L(\theta)}\text{=}-\cfrac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m} \log P\left(w_{t+j} | w_t ; \theta\right)$
> >
> >    - 含义：对模型预测错误的惩罚，需要使其最小之
> >
> > 3. 预测函数：即$\text{Softmax}$函数$\displaystyle{}P\left(w_{t+j} | w_t\right)\text{=}\cfrac{\exp \left(u_{w_{t+j}}^T v_{w_t}\right)}{\displaystyle{}\sum_{w \in V} \exp \left(u_w^T v_{w_t}\right)}$
> >
> >    - 含义：计算词$w_{t+j} / w_t$间的相似度，并将除以$w_t$**在整个词汇表上计算概率分布**以归一化
> >
> > :three:模型训练：
> >
> > 1. 目标：最小化目标函数$J(\theta)$，其中参数向量$\theta{}$包括所有词的中心向量$+$上下词向量
> > 2. 优化：依$\theta_j^{\text {new }}\text{= }\theta_j^{\text {old }}-\alpha \cfrac{\partial J(\theta)}{\partial \theta_j}$梯度下降，或者随机梯度下降
>
> ## $\textbf{2.2. }$两种训练方法
>
> > :one:$\text{CBOW}$：根据上下文预测中心词  
> >
> > <img src="https://i-blog.csdnimg.cn/direct/2c58633ab4dc437fa88df6d1b9537e7c.png" alt="image-20241122084136875" width=300 /> 
> >
> > 1. 模型流程：得到中心词的概率分布$\hat{y}\text{→}$优化损失函数最大化$p(c|o)$
> >
> >    |  流程  | 描述                                               | 形式化                                                       |
> >    | :----: | -------------------------------------------------- | ------------------------------------------------------------ |
> >    |  输入  | 上下文共$2m$个词的$\text{One-Hot}$向量             | $\{x^{c-m}\text{,...,}x^{c-1}, x^{c+1}\text{,...,}x^{c+m}\} \text{∈} \mathbb{R}^{\|V\|}$ |
> >    | 词嵌入 | 使用嵌入矩阵$W$将高维编码转为低维嵌入              | $v_i\text{=}(x_{i}\text{×}W)\text{∈}\mathbb{R}^{N}$          |
> >    |  池化  | 计算所有上下文嵌入向量平均(综合向量)               | $\hat{v}\text{=}\cfrac{v_{c-m}\text{+···+}v_{c+m}}{2 m}\text{∈}\mathbb{R}^N$ |
> >    |  打分  | 将综合向量$\hat{v}$映射为得分向量$z$               | $z\text{=}(\hat{v} \text{×} W^{T}) \text{∈}\mathbb{R}^{\|V\|}$ |
> >    | 概率化 | 用$\text{Softmax}$将打分$z$映射为概率分布$\hat{y}$ | $\hat{y}\text{=Softmax}(z)$                                  |
> >
> > 2. 损失函数
> >    - 目标/似然函数：最小化$P\left(x_c | x_{c-m}\text{,...,}x_{c+m}\right)$
> >    - 损失函数：$\displaystyle{}J\text{=}-\log{P\left(x_c | x_{c-m}\text{,...,}x_{c+m}\right)}\xrightarrow[\text{Softmax}]{展开/池化}-x_c^T \text{×} \hat{v}\text{+}\log \sum_{j=1}^{|V|} \exp \left(x_j^T \text{×} \hat{v}\right)$ 
> >
> > :two:$\text{Skip-gram(SG)}$： 根据中心词预测上下文  
> >
> > <img src="https://i-blog.csdnimg.cn/direct/291a9e79298b4148bf816e24386d02cc.png" alt="image-20241122090921206" width=300 /> 
> >
> > 1. 模型流程：得到上下文的概率分布$\hat{y}\text{→}$优化损失函数最大化$p(o|c)$
> >
> >    |  流程  | 描述                                               | 形式化                                                       |
> >    | :----: | -------------------------------------------------- | ------------------------------------------------------------ |
> >    |  输入  | 中心词的$\text{One-Hot}$向量                       | $x^c \text{∈} \mathbb{R}^{\|V\|}$                            |
> >    | 词嵌入 | 使用嵌入矩阵$W$将高维编码转为低维嵌入              | $v_c\text{=}(x_{c}\text{×}W)\text{∈}\mathbb{R}^{N}$          |
> >    |  打分  | 将综合向量$\hat{v}$映射为得分向量$z$               | $z\text{=}({v_c} \text{×} W^{T}) \text{∈}\mathbb{R}^{\|V\|}$ |
> >    | 概率化 | 用$\text{Softmax}$将打分$z$映射为概率分布$\hat{y}$ | $\hat{y}\text{=Softmax}(z)$                                  |
> >
> > 2. 损失函数
> >
> >    - 目标/似然函数：最小化$P\left(x_{c-m}\text{,...,}x_{c+m}|x_c \right)$
> >    - 损失函数：$\displaystyle{}J\text{=}-\log{P}\xrightarrow[\text{Softmax}]{展开}-\left(\sum_{j=0, j \neq m}^{2 m} v_{c-m+j}^T \text{×}v_c\text{+}2 m \log \sum_{k=1}^{|V|} \exp \left(v_k^T \text{×}v_c\right)\right)$ 
>
> ## $\textbf{2.3. }$两种训练优化
>
> > :one:层次$\text{Softmax}$
> >
> > 1. 二叉树构造：根节点表示整个词汇表$V$，不断二分(例如用$\text{Huffman}$树)到叶节点表示单个词汇$w_i$ 
> >
> > 2. 概率计算：$\displaystyle{}P\left(w\text{=}w_o\right)\text{=}\prod_{j=1}^{L(w)-1} \sigma\left([[n(w, j\text{+}1)\text{=}\text{ch}(n(w, j))]] \text{×} v_{n(w, j)}^T h\right)$ 
> >
> >    - 参数含义：
> >
> >      |   参数   | 含义                                                    |
> >      | :------: | ------------------------------------------------------- |
> >      |  $L(w)$  | 从根节点到目标单词$w$，所经过的节点数                   |
> >      | $n(w,j)$ | 到$w$的路径中的第$j$个结点，其子节点为$n(w,j\text{+}1)$ |
> >      | $\sigma$ | $\text{Sigmoid}$激活函数                                |
> >
> >    - 随机游走：
> >
> >      | 路径中第$\boldsymbol{j\text{}+1}$个结点为第$\boldsymbol{j}$个结点的 | $\boldsymbol{[[n(w, j\text{+}1)\text{=}\text{ch}(n(w, j))]] \text{×} v_{n(w, j)}^T h}$含义 |
> >      | :----------------------------------------------------------: | :----------------------------------------------------------: |
> >      |                           左子节点                           |          乘以 $\sigma\left(v_{n(w, j)}^T h\right)$           |
> >      |                           右子节点                           |          乘以 $\sigma\left(-v_{n(w, j)}^T h\right)$          |
> >
> > 3. 示例：
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/a3456e94ca7c4012bdbfc097c714c89b.png" alt="image-20241122094155815" width=380 /> 
> >
> >    - 游走路径：$P\left(n\left(w_2, 1\right), \text {left}\right) \text{×} P\left(n\left(w_2, 2\right), \text {left}\right) \text{×} P\left(n\left(w_2, 3\right), \text {right}\right)$
> >    - 概率展开：$P\left(w\text{=}w_2\right)\text{=}\sigma\left(v_{n\left(w_2, 1\right)}^T h\right) \text{×} \sigma\left(v_{n\left(w_2, 2\right)}^T h\right) \text{×} \sigma\left(-v_{n\left(w_2, 3\right)}^T h\right)$
> >
> > :two:负采样
> >
> > 1. 核心思想：
> >
> >    - 不遍历整个词汇表$V$ ​
> >    - 只遍历一个与词汇频率顺序匹配的噪声概率分布$p_n(w)$中采样的几个$\text{Negative}$例子
> >
> > 2. 模型定义：
> >
> >    | 实例 |          含义           | 优化目标                                                     |
> >    | :--: | :---------------------: | ------------------------------------------------------------ |
> >    | 正例 | 实际出现词对$(w_c,w_o)$ | 最大化$(w_c,w_o)$出现概率$J_{\mathrm{pos}}\text{=}\log \mathrm{P}(\mathrm{D}\text{=}1 | w, c)$ |
> >    | 负例 | 随机采样词对$(w_c,w_j)$ | 最小化$(w_c,w_o)$不出现概率$J_{\mathrm{pos}}\text{=}\log \mathrm{P}(\mathrm{D}\text{=}0 |w, c)$ |
> >
> > 3. 损失函数：
> >
> >    |      训练方式      | 损失函数                                                     |
> >    | :----------------: | ------------------------------------------------------------ |
> >    |   $\text{CBOW}$    | $\displaystyle{}J\text{=}-\log \sigma\left(v_c^T \text{×} \hat{v}\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{v}_k^T \text{×} \hat{v}\right)$ |
> >    | $\text{Skip-Gram}$ | $\displaystyle{}J\text{=}-\log \sigma\left(v_{c-m+j}^T \text{×} v_c\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{v}_k^T \text{×} v_c\right)$ |

# $\textbf{3. }$其它模型

> ## $\textbf{3.1. }\textbf{C\&W}$模型
>
> > :one:模型目标：抛弃传统语言模型对条件概率的计算，转而通过打分函数衡量一段词序的合理性
> >
> > :two:层次结构：
> >
> > <img src="https://i-blog.csdnimg.cn/direct/56f78cdf46b1439794df5826e9a3425d.png" alt="img" width=300 />    
> >
> > |    结构    | 描述                                               |
> > | :--------: | -------------------------------------------------- |
> > |   输入层   | 是一个大小为$n$的连续窗口，输入序列包含$n$个词向量 |
> > |  查找表层  | 对每个词进行查找$\text{→}$获得每个词的稠密词向量   |
> > |   卷积层   | 对输入序列的每个窗口应用卷积操作，提取局部特征     |
> > | 最大池化层 | 从卷积层提取最显著的特征，输出固定维度向量         |
> > |   分类层   | 对生成的向量进行打分，高分者视为语言上自然的       |
> >
> > :three:模型训练：
> >
> > 1. 输入：对于一个窗口内的连续词$\left[w_{i-n}\text{,...,} w_i\text{,...,} w_{i+n}\right]$
> >
> >    | 序列类型 | 操作                                                         | 性质           |
> >    | :------: | ------------------------------------------------------------ | -------------- |
> >    |  正序列  | 保留$\left[w_{i-n}\text{,...,} w_i\text{,...,} w_{i+n}\right]$ | 在语言上自然   |
> >    |  负序列  | 将窗口中心处词$w_i$换成$w_j$，即$\left[w_{i-n}\text{,...,} w_j\text{,...,} w_{i+n}\right]$ | 在语言上不自然 |
> >
> > 2. 目标：最大化正序列$\left[w_{i-n}\text{,...,} w_i\text{,...,} w_{i+n}\right]$得分$+$最小化负序列$\left[w_{i-n}\text{,...,} w_j\text{,...,} w_{i+n}\right]$得分
> >
> > 3. 损失：$\mathcal{L}\text{=}\max \left(0,1-s\left(w_{\text {true }}\right)\text{+}s\left(w_{\text {false }}\right)\right)$，正列得分$\text{≫}$负序列$\xLeftrightarrow{等价于}\mathcal{L}\text{→0}$ 
>
> ## $\textbf{3.2. GloVe}$模型
>
> >  :one:核心思想
> >
> >  1. 目标：学习词的一种分布式表示，使词向量可以捕捉词语的语义关系
> >  2. 假设：语义可从其共现信息中推断，例如$\text{ice/cold}$共现比$\text{ice/steam}$频率高$\xrightarrow{认为}$反映了语义关系
> >
> >  :two:模型要点
> >
> >  1. 共现矩阵$\textbf{X}$：
> >
> >     - 无权值情况：$\textbf{X}_{ij}$表示词$i$与词$j$在某滑动窗口共现的和，每共现一次增加$1$
> >
> >       ```text
> >       the cat sat on the mat (窗口大小=3)
> >       [the cat sat] -> X_the_sat=1
> >       [cat sat on]  -> X_the_sat=1+0
> >       [sat on the]  -> X_the_sat=1+0+1
> >       [on the mat]  -> X_the_sat=1+0+1+0
> >       ```
> >
> >     - 有权值情况：在原有基础上，每共现一次增加一权值$\text{decay=}\cfrac{1}{d}$(其中$d$为词$ij$在窗口中的距离)
> >
> >       ```txt
> >       the cat sat on the mat (窗口大小=3)
> >       [the cat sat] -> X_the_sat=1/2
> >       [cat sat on]  -> X_the_sat=1/2+0
> >       [sat on the]  -> X_the_sat=1/2+0+1/2
> >       [on the mat]  -> X_the_sat=1/2+0+1/2+0
> >       ```
> >
> >  2. 近似关系：$\textbf{X}_{ij}\xrightarrow{变换+分解}\log \left(X_{i j}\right)\text{=}w_i^T \tilde{w}_j\text{+}b_i\text{+}\tilde{b}_j\text{→}$用$w_i^T \tilde{w}_j$表示词$ij$的近似关系$+$偏置修正
> >
> >  3. 损失函数：$\displaystyle{}J\text{=}\sum_{i, j=1}^V f\left(X_{i j}\right)\left(w_i^T \tilde{w}_j\text{+}b_i\text{+}\tilde{b}_j-\log \left(X_{i j}\right)\right)^2$ 
> >
> >     |                        项                         | 含义                             |
> >     | :-----------------------------------------------: | -------------------------------- |
> >     | $w_i^T \tilde{w}_j\text{+}b_i\text{+}\tilde{b}_j$ | 所预测的词$ij$共现次数           |
> >     |                 $\log (X_{i j})$                  | 实际的词$ij$共现次数             |
> >     |                   $f (X_{i j})$                   | 加权函数(较少共现词对模型的影像) |
> >
> >       - 对于共现次数阈值$x_{\text{max}}$有$f\left(X_{i j}\right)\text{=}\begin{cases}\left(\cfrac{X_{i j}}{x_{\max }}\right)^\alpha  \text {, } X_{i j}<x_{\max } \\\\ 1  \text {, otherwise }\end{cases}$ 