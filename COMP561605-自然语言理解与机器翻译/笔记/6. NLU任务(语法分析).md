# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119154414877.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/24950624a141b9941650b6187682aa9.jpg" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{1. }$词性标注

> ## $\textbf{1.1. }$基本概念
>
> > :one:词性标注概念：
> >
> > 1. 概念：给定一个句子，为其中的每个词分配适当的词性
> >
> > 2. 示例：$\small\text{Mr.\textcolor{red}{/NNP} Vinken\textcolor{red}{/NNP} is\textcolor{orange}{/VBZ} chairman\textcolor{Purple}{/NN} of\textcolor{green}{/IN} Elsevier\textcolor{red}{/NNP} N.V.\textcolor{red}{/NNP}}$ 
> >
> > 3. 顺序：分词$\text{→}$==词性标注==$\text{→}$句法分析$/$命名实体识别$/$情感分析
> >
> > :two:词性的消息来源：词语$\xrightarrow{本身词性}$预选语料库中最高频词性$\xrightarrow[消除多义词歧义造成的误差]{上下文}$精确词性
>
> ## $\textbf{1.2. }$马尔可夫模型标注器
>
> > :one:模型概述
> >
> > 1. 基本要素：
> >
> >    | 序列 | 含义                                |          对应马可夫模型中          |
> >    | :--: | :---------------------------------- | :--------------------------------: |
> >    | $T$  | 单词的词性集(形容词/名词/动词.....) | <font color=orange>状态空间</font> |
> >    | $O$  | 输入的单词集                        |  <font color=red>观测空间</font>   |
> >    | $W$  | 对单词词性的标注序列                |              状态序列              |
> >
> > 2. 基本假设：有限历史(一词の词性只依赖于其前一词の词性)$\text{+}$时间不变性(这种依赖不随时间改变)
> >
> > :two:模型原理：通过最大化$P(T|W)\xrightarrow{找到}$最佳的词性标注序列$T\text{=}\{t_1,t_2,\text{...},t_n\}$ 
> >
> > 1. 贝叶斯分解：$P(T | W) \text{=}\cfrac{P(W | T) \text{×} P(T)}{P(W)} 
> >    \text{ ∝ } P(W | T) \text{×} P(T)$ 
> >
> >    | $\textbf{Item}$ |                         联合概率分解                         | 含义                     |
> >    | :-------------: | :----------------------------------------------------------: | ------------------------ |
> >    |  $P(W\mid{}T)$  |  $\displaystyle{}\prod_{i=1}^n P\left(w_i \mid t_i\right)$   | 每个单词由对应的词性生成 |
> >    |     $P(T)$      | $\displaystyle{}\prod_{i=1}^n P\left(t_i \mid t_{i-1}\right)$ | 词性序列之间具有转移关系 |
> >
> > 2. 联合概率分解：$\displaystyle{}P(T | W) \text{=}\prod_{i=1}^n P\left(w_i | t_i\right) P\left(t_i | t_{i-1}\right)$ 
> >
> > :three:模型训练与预测
> >
> > 1. 模型训练：
> >
> >    |   概率类型   |                             公式                             | 含义                                                 |
> >    | :----------: | :----------------------------------------------------------: | :--------------------------------------------------- |
> >    | 标注转移概率 | $P\left(t^k \mid t^j\right)\text{=}\cfrac{C\left(t^j, t^k\right)}{C\left(t^j\right)}$ | 词性$t^j$转移到$t^k$的次数$/$词性$t^j$的总出现次数   |
> >    |  词生成概率  | $P\left(w^l \mid t^j\right)\text{=}\cfrac{C\left(w^l, t^j\right)}{C\left(t^j\right)}$ | 词$w^l$被标注为词性$t^j$的次数$/$词性$t^j$总出现次数 |
> >
> > 2. 模型预测：示例
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241120022501955.png" alt="image-20241120022501955" width=550 /> 
> >
> > :four:其它事项
> >
> > 1. 未登录词：即训练语料库中从未出现过的词，可认为$P(w | t)\text{=}\cfrac{1}{ \text {|可能词性|}}/$假设其词性可任选......
> > 2. 平滑问题：采用$\text{Laplace/Good-Turing}$平滑技术，或者收集更数据
>
> ## $\textbf{1.3. }$基于转换的词性标注  
>
> > :one:概述
> >
> > 1. 基本思想：
> >
> >    - 修正规则：正确结果要不断修正得到，修正方式有迹可循
> >    - 转换规则：计算机可以学习修正过程(记录为转换规则)，并用学得转换规则进行词性标注
> >
> > 2. 转换规则：
> >
> >    |   组成   | 含义                       | 示例                                                         |
> >    | :------: | :------------------------- | :----------------------------------------------------------- |
> >    | 改写规则 | 将一个词性转换成另一个词性 | 将一个词性从<font color=red>动词</font>改为<font color=orange>名词</font> |
> >    | 激活环境 | 激发改写规则的条件         | 该词左边第一个词的词性是<font color=gree>量词</font>         |
> >
> > :two:转换规则学习器算法
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241120160359566.png" alt="image-20241120160359566" width=550 /> 
> >
> > 1. 生成：生成语料库$\mathbb{C}_{\text{0-raw}}\xrightarrow[标注]{\textcolor{red}{初始标注器  }}$有词性标记的语料$\mathbb{C}_1/\mathbb{C}_2/\mathbb{C}_3\text{......}$ 
> >
> > 2. 找错：$($语料库$\mathbb{C}_1/\mathbb{C}_2/\mathbb{C}_3\text{......}\xleftrightarrow{\textcolor{red}{比较}}$正确标记预料库$\mathbb{C}_0)\xrightarrow{得到}\mathbb{C}_1/\mathbb{C}_2/\mathbb{C}_3\text{......}$中词性标注错误数
> >
> > 3. 循环：更新原有$\{C_1,C_2,\text{...},C_n\}$序列$\xrightarrow{循环}$得到最终的规则序列$\{T_a,T_b,\text{...},T_x\}$
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/wraetyulohil.gjk%2Cfrtg4r3.png" alt="image-20241120165955895" width=480 />  
> >
> >    - 生成：将所有的规则$T_j$应用到$C_i$上$\xrightarrow{得到}mn$个$C_i^j$
> >    - 选取：在每列对比所有$(C_i^j\xleftrightarrow{错误数}C_0)\xrightarrow[(假设是C_{i}^{k})]{选取错误数最少的}\begin{cases}认定T_{k}为这次学习得到的转换规则\\\\将C_{i}^{k}作为新的待修改语料库\end{cases}$ 
> >    - 迭代：让$\{C_1^{k_1},C_2^{k_2},\text{...},C_n^{k_n}\}$代替$\{C_1,C_2,\text{...},C_n\}$，重复以上步骤
> >    - 终止：迭代到错误率小于阈值，输出最终的规则序列$\{T_a,T_b,\text{...},T_x\}$ 

# $\textbf{2. }$句法解析

> ## $\textbf{2.1. }$基本概念与概述
>
> > :one:模型描述：
> >
> > 1. 条件：给定一个句子$s$及其语法$G$，以$P(t|(s,G))$概率生成分析树$t$，并且$\displaystyle{}\sum_t P(t | (s,G))\text{=}1$ 
> >
> > 2. 目的：找出最大化$P(t|(s,G))$的$t$，即最有可能的句法树
> >
> > :two:与语言模型：
> >
> > 1. 句子概率：语言模型中句子以$P(s)$概率生成，若考虑句法结构则有$\displaystyle{}P(s)\text{=}\sum_t P(s, t)$ 
> >
> > 2. 最优分析：句法分词旨在最大化$P(t|s)\xrightarrow[P(s)是关于t的常数]{P(t|s)\text{=}\Large{}\frac{P(t,s)}{P(s)}}$变为直接最大化$P(t,s)$ 
>
> ## $\textbf{2.2. }$概率上下文无关文法  
>
> > ### $\textbf{2.2.1. }$一些基本概念与假设
> >
> > > :one:句子结构：
> > >
> > > |   结构   | 含义                                | 示例                                                         |
> > > | :------: | :---------------------------------- | :----------------------------------------------------------- |
> > > | 非终止符 | 抽象语法成分，不直接出现在句子中    | $\text{S}$(句子)$/\text{NP}$(名词短语)$/\text{VP}$(动词短语) |
> > > |  终结符  | 实际出现的单词或符号                | $\text{cat, eats, fish...}$                                  |
> > > |   规则   | 非终止符如何进一步被分为符序列/短语 | $\text{NP}\xrightarrow{规则}\text{Det(限定词)+N}$            |
> > > | 层次结构 | 规则逐步展开形成的树状结构          | 句法树                                                       |
> > >
> > > :two:上下文无关文法($\text{CFG}$)：
> > >
> > > | $\text{Item}$ | 含义                     | 例子                                             |
> > > | :-----------: | :----------------------- | :----------------------------------------------- |
> > > | $\text{CFG}$  | 细分非终止符的语法规则集 | $\text{NP→Det+N/VP→V+NP}$                        |
> > > | $\text{PCFG}$ | 为每条规则赋予一个概率   | $P\text{(NP→Det+N)=0.9/}$$P(\text{VP→V+NP)=0.1}$ |
> > >
> > > :three:句法树：用树状结构表示句子内部语法层次
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241120231655826.png" alt="image-20241120231655826" width=400 /> 
> > >
> > > |   结构   | 内容                                                         |
> > > | :------: | :----------------------------------------------------------- |
> > > |  根结点  | 整个句子                                                     |
> > > | 中间结点 | 包括非终结结点(如$\text{NP/VP}$等语法成分)$\text{+}$终结结点(如$\text{N/V}$等具体单词词性) |
> > > |  叶结点  | 实际的单词，与终结结点$\text{1-1}$对应                       |
> > >
> > > :four:模型假设
> > >
> > > |    假设    | 含义                         | 示例                                                   |
> > > | :--------: | :--------------------------- | :----------------------------------------------------- |
> > > |  位置不变  | 子树概率与在句子中位置无关   | 名词短语$\text{NP}$在句首/尾时，其结构概率相同         |
> > > | 上下文无关 | 子树概率不依赖不属于该子树词 | 动词短语$\text{VP}$生成概率不依赖于句中主语$\text{NP}$ |
> > > |  祖先无关  | 子树概率与其父/祖先节点无关  | 嵌套从句$\text{CP}$生成概率与更高层句法树无关          |
> >
> > ### $\textbf{2.2.2. }$概率上下文无关文法基本问题
> >
> > > #### $\textbf{2.2.2.1. }$问题$\textbf{1: }$句子概率$\boldsymbol{P(w_{1:m}|G)}$计算
> > >
> > > > :one:$\text{Chomsky}$范式语法
> > > >
> > > > 1. 两种规则：
> > > >
> > > >    |   规则   | 含义                                                        | 规则概率                      |
> > > >    | :------: | :---------------------------------------------------------- | :---------------------------- |
> > > >    | 二元规则 | $N^i$(一个非终结符)$\xrightarrow{生成}N^iN^k$(一个非终结符) | $P(N^i\text{→}N^jN^k\mid{}G)$ |
> > > >    | 一元规则 | $N^i$(一个非终结符)$\xrightarrow{生成}w^j$(一个终结符)      | $P(N^i\text{→}w^j\mid{}G)$    |
> > > >
> > > > 2. 参数空间：对于空间$\{N^1,N^2,\text{...},N^n,w^1,w^2,\text{...},w^V\}$ 
> > > >
> > > >    - 规则数量：二元规则共$n^3$个，一元规则共$nV$个
> > > >    - 规则概率：需满足$\displaystyle{}\sum_{r, s} P\left(N^j \text{→} N^r N^s\right)\text{+}\sum_k P\left(N^j \text{→} w^k\right)\text{=}1$
> > > >
> > > > :two:句子概率$\displaystyle{P(w_{1:m})\text{=}\sum_{t: \operatorname{yield}(t)=w_{1 m}} P(t)}$ 
> > > >
> > > > |                 项                 | 含义                                                         |
> > > > | :--------------------------------: | ------------------------------------------------------------ |
> > > > |      $P\left(w_{1:m}\right)$       | 生成句子(词序列)$w_{1:m}\text{=}\{w_1,w_2\text{,...,}w_m\}$的概率 |
> > > > | $t\text{:yield}(t)\text{=}w_{1:m}$ | 句法树的叶节点序列是$\{w_1,w_2\text{,...,}w_m\}$             |
> > > > |    $\displaystyle{}\sum{}P(t)$     | 所有叶节点序列是$\{w_1,w_2,...,w_m\}$的句法树生成的概率总和  |
> > > > |               $P(t)$               | 某一句法树生成的概率，==为生成句法树所有规则概率的乘积==     |
> > > >
> > > > :three:示例：考虑句子$\text{astronomers saw stars with ears}$ 
> > > >
> > > > 1. 句法树：
> > > >
> > > >    |        |  $\boldsymbol{t_1}\textbf{: with ears}$修饰$\textbf{stars}$  |   $\boldsymbol{t_1}\textbf{: with ears}$修饰$\textbf{saw}$   |
> > > >    | :----: | :----------------------------------------------------------: | :----------------------------------------------------------: |
> > > >    | 句法树 | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121011314031.png" alt="image-20241121011314031" width=255 /> | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121011358988.png" alt="image-20241121011358988" width=255 /> |
> > > >
> > > > 2. 规则概率：
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121012724305.png" alt="image-20241121012724305" width=450 /> 
> > > >
> > > > 3. 生成概率
> > > >
> > > >    |   概率   | 计算                                                         |
> > > >    | :------: | :----------------------------------------------------------- |
> > > >    | $P(t_1)$ | $1.0 \times 0.1 \times 0.7 \times 1.0 \times 0.4 \times 0.18 \times 1.0 \times 1.0 \times 0.18$ |
> > > >    | $P(t_2)$ | $1.0 \times 0.1 \times 0.3 \times 0.7 \times 1.0 \times 0.18 \times 1.0 \times 1.0 \times 0.18$ |
> > > >    |  $P(w)$  | $P(t_1)+P(t_2)$                                              |
> > >
> > > #### $\textbf{2.2.2.2. }$问题$\textbf{2: }$最佳句法分析  
> > >
> > > > :one:问题描述
> > > >
> > > > 1. 目的：找到使句子概率最大的句法树，即最优句法树
> > > > 2. 形式化：
> > > >    - 定义$\delta_i(p,q)$：即以非终结符$N^i$且覆盖字句$w_{p:q}$情况下，最佳解析树的概率
> > > >    - 求解方法：动态规划
> > > >
> > > > :two:类$\text{Viterbi}$风格的动态规划求解
> > > >
> > > > 1. 二元规则：$\displaystyle{}\delta_i(p, q)\xleftarrow{N_{p:q}^i子树由N_{p:r}^j/N_{r\text{+}1:q}^k构成}\max _{j, k, r}\left(P\left(N^i \text{→} N^j N^k\right) \text{×} \delta_j(p, r) \text{×} \delta_k(r\text{+}1, q)\right)$ 
> > > > 2. 一元规则：$\delta_i(p, p)\xleftarrow{由叶节点N_{p:p}^i直接生成w_j}P\left(N^i \text{→} w_p\right)$
> > >
> > > #### $\textbf{2.2.2.3. }$问题$\textbf{2: }$文法训练
> > >
> > > > :one:$\text{Inside-Outside}$算法
> > > >
> > > > 1. 内部概率$\&$外部概率 
> > > >
> > > >    | $P$  | 公式                                                         | 含义                                                  |
> > > >    | :--: | :----------------------------------------------------------- | :---------------------------------------------------- |
> > > >    |  内  | $\beta_j(p, q)\text{=}P\left(w_{p: q} \mid \left(N_{p: q}^j, G\right)\right)$ | 由$N_{p: q}^j\xrightarrow[语法G]{生成}w_{p: q}$的概率 |
> > > >    |  外  | $\alpha_j(p, q)=P\left(\left(w_{1:(p-1)}, N_{p: q}^j, w_{(q+1): m}\right) \mid G\right)$ | 句子$pq$以外(基于$G$)的生成概率                       |
> > > >
> > > > 2. 算法公式：$\displaystyle{}P\left(规则N\text{→}\alpha\text{在}w_{p:q}\right)\text{=}\alpha_i(p,q)\text{×}P(N\text{→}\alpha)\text{×}\prod\beta(\text{子结构})\text{×}\frac{1}{P\left(w_{1:m}\right)}$ 
> > > >
> > > > :two:$\text{EM}$算法：优化规则的概率$P(N\text{→}\alpha)$
> > > >
> > > > 1. $\text{E}$步：使用$\text{Inside-Outside}$算法，算出规则在未标注语料中出现次数的期望
> > > > 2. $\text{M}$步：更新每条规则中的概率为$P(N \text{→} \alpha)\text{=}\cfrac{\text {规则} N \text{→} \alpha \text {的期望值}}{\text {所有以} N \text {为左部规则的期望值总和}}$ 
>
> ## $\textbf{2.3. }$其它有关内容
>
> > :one:依存句法
> >
> > 1. 含义：在一句话中选一个词为中心，然后用词之间的依存关系来描述语言结构
> >
> > 2. 表示：带箭头的曲线表示  
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121022845482.png" alt="image-20241121022845482" width=340 /> 
> >
> > :two:句法消歧分析：统计方法用概率代替语义规则，自动完成句法树的消歧和选择过程
> >
> > :three:树库：包含已正确句法分析的句子及其对应解析树的集合，用于构建和训练统计句法分析器

