# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119154414877.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/24950624a141b9941650b6187682aa9.jpg" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{2. }$命名实体识别($\textbf{NER}$)

> ## $\textbf{2.1. IE}$与$\textbf{NER}$概述
>
> > :one:信息抽取($\text{IE}$)
> >
> > 1. 含义：从非结构化文本中提取结构化信息的技术，如下例子从文章$\xrightarrow{提取}$实体$/$关系$/$事件分类
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121163753657.png" alt="image-20241121163753657" width=600 /> 
> >
> > 2. 核心任务：以句子$\text{John invested in Tesla in 2022}$为例
> >
> >    |   任务   | 描述                         | 示例                                                         |
> >    | :------: | :--------------------------- | :----------------------------------------------------------- |
> >    | 实体识别 | 自动识别并分类特定类型的实体 | $\text{John}$(人名)/$\text{Tesla}$(公司)/$\text{2022}$(时间) |
> >    | 关系提取 | 自动识别实体之间的关系       | $\text{Invested\_in(John, Tesla)}$                           |
> >    | 事件抽取 | 自动识别事件的实例           | 投资事件实例：$\text{John→Tesla→2022}$                       |
> >
> > :two:命名实体识别($\text{NER}$)：从文本中识别并分类实体到预定义类别，如下例子
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121165624561.png" alt="image-20241121165624561" width=600 /> 
>
> ## $\textbf{2.2. }$基于规则的$\textbf{NER}$ 
>
> > :one:基于字典的$\text{NER}$
> >
> > 1. 含义：使用预定义的词典，为词语匹配实体类型
> >
> > 2. 工具：
> >
> >    |         工具         | 描述                                                         |
> >    | :------------------: | :----------------------------------------------------------- |
> >    | $\text{StanfordNER}$ | 广泛使用的$\text{NER}$工具，支持多种实体类型                 |
> >    |    $\text{SpaCy}$    | 结合规则和词典的方法，可自定义训练                           |
> >    |    $\text{NLTK}$     | 基于$\text{Python}$的$\text{NLP}$库，包含简单的$\text{NER}$功能 |
> >    |    $\text{GATE}$     | 综合$\text{NLP}$框架，支持复杂的$\text{NER}$配置             |
> >
> > :two:基于句法树的$\text{NER}$
> >
> > 1. 含义：利用句法结构模式从文本中提取命名实体
> >
> > 2. 示例：$\textbf{[}$名词短语($\text{NP}$)$\text{+}$介词短语($\text{PP}$)$\textbf{]}\xrightarrow{识别为}$地理实体($\text{GPE}$)，还如$\text{University of Melbourne}$
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121174446475.png" alt="image-20241121174446475" width=300 />  
> >
> > :three:基于正则式($\text{regex}$)的$\text{NER}$
> >
> > 1. 正则式：一种模式匹配工具，在文本中搜索和提取符合特定模式的片段
> >
> >    - 基本字符匹配：
> >
> >      | 模式 |        匹配内容         |     示例     |     匹配     |
> >      | :--: | :---------------------: | :----------: | :----------: |
> >      | `.`  |   单个字符(除换行符)    |    `h.t`     | `hot`/`hat`  |
> >      | `\d` |      $\text{0→9}$       |   `\d{4}`    |    `2024`    |
> >      | `\D` |         非数字          |    `\D+`     |   `hello`    |
> >      | `\w` |    字母/数字/下划线     |    `\w+`     |  `hello123`  |
> >      | `\W` | 非字母/非数字/非下划线  |    `\W+`     |    `@#$`     |
> >      | `\s` | 空白字符(空格/制表符等) | $\text{N/A}$ | $\text{N/A}$ |
> >      | `\S` |       非空白字符        |    `\S+`     |   `abc123`   |
> >
> >    - 数量匹配：
> >
> >      | 模式      | 匹配内容(单字符重复次数)            | 示例       | 匹配                   |
> >      | :-------: | :-----------------------------: | :--------: | :--------------------: |
> >      | `*`       | $0\text{→}n$ | `a*`       | `aaa` /空串      |
> >      | `+`       | $1\text{→}n$ | `a+`       | `aaa`                 |
> >      | `?`       | $0\text{→}1$ | `a?`       | `a` /空串        |
> >      | `{n}`   | $n$       | `a{3}`     | `aaa`                 |
> >      | `{n,}`  | $n\text{→}n\text{+}k$ | `a{2,}`    | `aa`/`aaa`/`aaaa` |
> >      | `{n,m}` | $n\text{→}m$ | `a{2,4}` | `aa`/`aaa`/`aaaa` |
> >
> >    - 字符集与范围：
> >    
> >      |   模式   |     匹配内容     |   示例   |    匹配     |
> >      | :------: | :--------------: | :------: | :---------: |
> >      | `[abc]`  | 字符集中任一字符 | `[abc]`  | `a`/`b`/`c` |
> >      | `[^abc]` | 字符集外任一字符 | `[^abc]` | `d`/`e`/`f` |
> >      | `[a-z]`  |  `a-z`任一字符   | `[a-z]+` |   `hello`   |
> >      | `[A-Z]`  |  `A-Z`任一字符   | `[A-Z]+` |   `HELLO`   |
> >      | `[0-9]`  |  `0-9`任一字符   | `[0-9]+` |    `123`    |
> >    
> >    - 位置匹配：
> >    
> >      | 模式 |     匹配内容     |    示例    |             匹配             |
> >      | :--: | :--------------: | :--------: | :--------------------------: |
> >      | `^`  |   字符串的开始   |  `^Hello`  | `Hello world` 开头的 `Hello` |
> >      | `$`  |   字符串的结尾   |  `world$`  | `Hello world` 结尾的 `world` |
> >      | `\b` | 字符位于单词边界 | `\bword\b` |    独立出现的的单词`word`    |
> >      | `\B` | 字符不在单词边界 | `\Bword\B` |    `swordplay`中的`word`     |
> >    
> >    - 分组与引用：
> >    
> >      | 模式   |       匹配规则       |      示例       |      匹配       |
> >      | ------ | :------------------: | :-------------: | :-------------: |
> >      | `()`   |     将表达式分组     |     `(ab)+`     |     `abab`      |
> >      | `(?i)` | 启用大小写不敏感匹配 | `(?i)hello`匹配 | `Hello`/`hello` |
> >
> > 2. 示例：
> >
> >    | 正则式                                                | 可匹配 | 示例                 |
> >    | :---------------------------------------------------- | :----: | :------------------- |
> >    | `(?:[A-Z][a-z]*)*[A-Z][a-z]+`                         |  位置  | $\text{New York}$    |
> >    | `[A-Z][a-z]+(?:[A-Z][a-z]+)*(?:Col\.\|Inc\.\|Ltd\.)?` |  组织  | $\text{Google Inc.}$ |
> >    | `[A-Z][a-z]+[A-Z][a-z]+`                              |  人名  | $\text{John Smith}$  |
> >
>
> ## $\textbf{2.3. }$基于特征的$\textbf{NER}$: $\textbf{CRF}$ 
>
> > :one:基本概念：$\text{CRF(Conditional Random Fields)}$ 
> >
> > 1. $\text{CRF}$基本思想：将$\text{NER}$建模为==序列标注问题==，为每个实体分配一个标签
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121193016275.png" alt="image-20241121193016275" width=680 /> 
> >
> > 2. 线性链$\text{CRF}$：认为每个单词的标签$y_i$只依赖于前一个单词的标签$y_{i+1}$ 
> >
> > :two:特征函数：$f(x,y_i,y_{i-1},i)$
> >
> > 1. 函数参数：
> >    - 含义：$i$(当前单词索引)，$y_i$(当前单词标签)，$y_{i-1}$(前一个单词的标签)，$x$(整个句子)
> >    - 示例：$f(\text{Ram is cool},\text{O},\text{PER},1)\text{=}0$
> > 2. 函数定义：可任意定义(但最好结合上下文)，如$f_j\left(x, y_i, y_{i-1}, i\right)= \begin{cases}1,  \text { 第} i \text {个单词首字母大写 } \\\\ 0,  \text { 否则 }\end{cases}$ 
> >
> > :three:$\text{CRF}$概率建模公式：$\displaystyle{}p_\theta(y | x)\text{=}\cfrac{\exp \left(\displaystyle{}\sum_j w_j F_j(x, y)\right)}{\displaystyle{}\sum_{y^{\prime}} \exp \left(\sum_j w_j F_j\left(x, y^{\prime}\right)\right)}$  
> >
> > 1. 公式各部分含义：
> >
> >    |     $\text{Item}$      | 含义                                                         |
> >    | :--------------------: | :----------------------------------------------------------- |
> >    | $p_\theta(y \mid{} x)$ | 给定句子输入$x$的情况下，生成标签序列$y$的概率               |
> >    |         $w_j$          | 第$j$个特征函数的权值                                        |
> >    |      $F_j(x, y)$       | 对于标签序列$y$，第$j$个特征函数在**整句**的累积，即$\displaystyle{}\sum_{i=1}^L f_j\left(x, y_i, y_{i-1}, i\right)$ |
> >    |  $F_j(x, y^{\prime})$  | 遍历所有可能的标签序列$y^{\prime}$并累加，以保证生成概率分布 |
>
> ## $\textbf{2.4. }$基于深度学习的$\textbf{NER}$
>
> > ### $\textbf{2.4.0. }$概述
> >
> > > :one:$\text{NER}$的三个子任务
> > >
> > > |      任务      | 描述                      |                             示例                             |
> > > | :------------: | :------------------------ | :----------------------------------------------------------: |
> > > |  平面实体识别  | 提取简单/非嵌套的命名实体 | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121205153565.png" alt="image-20241121204208894" width=300 /> |
> > > |  嵌套实体识别  | 提取嵌套的命名实体        | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121205451157.png" alt="image-20241121205451157" width=180 /> |
> > > | 不连续实体识别 | 提取不连续片段组成的实体  | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121210034591.png" alt="image-20241121210034591" width=300 /> |
> > >
> > > :two:基于深度学习$\text{NER}$的四种范式
> > >
> > > |        范式         | 描述                                 | 实现                                              |
> > > | :-----------------: | :----------------------------------- | :------------------------------------------------ |
> > > |      序列标注       | 视单词为序列输入并分配标签           | 用$\text{BiLSTM/BERT}$编码$\text{→CRF}$标记       |
> > > |    机器阅读理解     | 将$\text{NER}$问题转换为对话         | 构造问题$\text{-}$文本对，用$\text{BERT}$提取答案 |
> > > | $\text{Token-Pair}$ | 建立文本中每两个$\text{Token}$的关系 | 用$\text{Transformer/BERT}$为$\text{Token}$编码   |
> > > |        生成         | 通过生成序列的方式识别命名实体       | 用$\text{GPT}$来输出实体及其类型                  |
> >
> > ### $\textbf{2.4.1. }$序列标注法：平面实体✅$/$嵌套实体❌$/$不连续实体❌
> >
> > > :one:$\textcolor{green}{\text{BIO}}/\textcolor{red}{\text{BIOES}}$实体标注方法：
> > >
> > > |                符号                 | 含义                          | 示例                                               |
> > > | :---------------------------------: | :---------------------------- | :------------------------------------------------- |
> > > | <font color=green>$\text{B}$</font> | 实体的开头($\text{Begin}$)    | $\text{John}$标记为$\text{B-PER}$表示人名的开始    |
> > > | <font color=green>$\text{I}$</font> | 实体的内部($\text{Inside}$)   | $\text{Sumith}$标记为$\text{I-PER}$表示人名的内部  |
> > > | <font color=green>$\text{O}$</font> | 非实体                        | 剧中$\text{lives/in/and...}$等都应标记为$\text{O}$ |
> > > |  <font color=red>$\text{E}$</font>  | 实体的结尾($\text{End}$)      | $\text{Sumith}$也可标记为$\text{E-PER}$            |
> > > |  <font color=red>$\text{S}$</font>  | 单个词的实体($\text{Single}$) | $\text{N/A}$                                       |
> > >
> > > 1. $\textcolor{green}{\text{BIO}}\textcolor{red}{\text{}}$示例：
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121215314971.png" alt="image-20241121215314971" height=90 /> 
> > >
> > > 2. $\textcolor{green}{\text{}}\textcolor{red}{\text{BIOES}}$示例：
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121215448316.png" alt="image-20241121215448316" height=90 /> 
> > >
> > > :two:$\text{LSTM-CRF}$架构：
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121224115336.png" alt="image-20241121224115336" width=480 /> 
> > >
> > > 1. 输入：将句子$X\text{=}\{X_1,X_2\text{,..,}X_n\}$转化为词嵌入向量
> > > 2. 编码：将词嵌入向量通过双向$\text{LSTM}$得到编码(上下文表示)$h_t\text{=BiLSTM}(X)$
> > > 3. 解码：通过特征函数计算得到$\displaystyle{}P(Y|X)=\cfrac{\exp \left(\displaystyle{}\sum_{t=1}^n f\left(y_t, y_{t-1}, h_t\right)\right)}{\displaystyle{}\sum_{Y^{\prime}} \exp \left(\sum_{t=1}^n f\left(y_t^{\prime}, y_{t-1}^{\prime}, h_t\right)\right)}$ 
> > > 4. 输出：使得$P(Y|X)$最大的$Y$ 
> > >
> > > :three:$\text{BERT-CRF}$架构：就是把$\text{BiLSTM}$换成$\text{BERT}$，更能捕获深层次的语义特征
> >
> > ### $\textbf{2.4.2. }$$\textbf{Token}$对法：平面实体✅$/$嵌套实体✅$/$不连续实体✅
> >
> > > :one:基本概念：
> > >
> > > |        概念         | 含义                                              | 示例                               |
> > > | :-----------------: | :------------------------------------------------ | ---------------------------------- |
> > > |   $\text{Token}$    | 输入文本中的基本单元，可以是单词/字符             | $\text{University, of, Melbourne}$ |
> > > | $\text{Token-Pair}$ | 标有起始的一对$\text{Token}$，此处指实体的起止    | $\text{Universit, Melbourne}$      |
> > > |    $\text{Span}$    | 起始$\text{Token}$间的文本，用起始索引$(i,j)$表示 | $\text{University of Melbourne}$   |
> > >
> > > :two:模型概述：
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241121230550210.png" alt="image-20241121230550210" width=400 /> 
> > >
> > > 1. 预处理：将输入文本分词($\text{Token}$化)$\text{+}$词嵌入，得到词向量$\{x_1,x_2,x_3\text{,...}\}$
> > >
> > > 2. 编码层：用$\text{BERT}$生成每个$\text{Token}$的上下文表示，得到$h_i\text{=}\text{BERT}(x_i)$ 
> > >
> > > 3. 交互层：将所有任意$h_ih_j$拼接$/$加权求和$/$注意力，得到初始$\text{Span}$及得分$f\left(h_i, h_j, t\right)\text{=score}_{i j, t}$ 
> > >
> > >    |     $\textbf{Item}$     | 含义                                    |
> > >    | :---------------------: | :-------------------------------------- |
> > >    |         $(i,j)$         | $\text{Span}$的起始($i$)和结束($j$)位置 |
> > >    | $\text{score}_{i j, t}$ | 表示片段$(i,j)$属于$t$类实体的分数      |
> > >
> > > 4. 分类层：依据$\text{score}_{i j, t}$将不同$\text{Span}$分类到各自的$t$类实体中，最终组成预测矩阵$\text{[L,L,N]}$ 
> > >
> > >    | $\textbf{Item}$ | 含义                                                         |
> > >    | :-------------: | :----------------------------------------------------------- |
> > >    |    维度含义     | $\text{L}$为输入文本$\text{Token}$长度，$\text{N}$为实体类别的数量 |
> > >    |    矩阵含义     | $\text{[L=i,L=j,N=t]}$表示位于$(i,j)$间$\text{Span}$，属于实体类别$t$的分数值$\text{score}_{i j, t}$ |
> > >
> > > :three:交互层的几种方法
> > >
> > > |   方法   |           $\boldsymbol{f\left(h_i, h_j, t\right)}$           | 含义                             |
> > > | :------: | :----------------------------------------------------------: | :------------------------------- |
> > > |  乘法式  | $(W_{i, t} h_i\text{+}b_{i, t})^{T}\text{×}(W_{j, t} h_j\text{+}b_{j, t})$ | 用点积测量头部和尾部的相似性     |
> > > |  加法式  | $W_t\text{×}[\tanh \left(W_h\left[h_i \text{⊕} h_j\right]\text{+}b_h\right)]\text{+}b_t$ | 将$h_ih_j$首尾相接→线性变换→激活 |
> > > | 双仿射式 | $h_i^T U_t h_j\text{+}W_t\left[h_i \text{⊕} h_j\right]\text{+}b_t$ | 同时捕捉头部和尾部的高维语义     |
> > >
> > > :four:示例：嵌套$/$不连续实体识别问题得以解决
> > >
> > > 1. 平面实体识别：$\text{[L,L,N=Person]}$层得分矩阵
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122000757166.png" alt="image-20241122000757166" width=290 /> 
> > >
> > > 2. 嵌套实体识别：$\text{[L,L,N=Location/Person]}$层得分矩阵，嵌套实体得以识别
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122001350427.png" alt="image-20241122001350427" width=200 /><img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122001403296.png" alt="image-20241122001403296" width=200 /> 
> > >
> > > 3. 不连续实体识别：$\text{[L,L,N=Disorder/Middle of Disorder]}$层得分矩阵，不连续实体得以识别
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122002011697.png" alt="image-20241122002011697" width=290 /><img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122002021482.png" alt="image-20241122002021482" width=290 /> 
> >
> > ### $\textbf{2.4.3. MRC}$法: 平面实体✅$/$嵌套实体✅$/$不连续实体✅
> >
> > > :one:模型描述
> > >
> > > 1. 核心思想：将每种实体类型表示为自然语言查询，并通过回答这些查询来提取实体
> > >
> > > 2. 主要流程：
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122004515978.png" alt="image-20241122004515978" width=500 /> 
> > >
> > >    | 流程 | 描述                                                  |
> > >    | :--: | :---------------------------------------------------- |
> > >    | 输入 | 问题/查询($\text{E.g.}$文本中的人名是哪个)$+$原始文本 |
> > >    | 处理 | 用$\text{NERT/GPT}$等对文本进行编码                   |
> > >    | 输出 | 通过分类层，得到最佳的答案                            |
> > >
> > > 3. 示例：
> > >
> > >    - 对平面实体的抽取：对于人名的查询
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122004853072.png" alt="image-20241122004853072" height=60 />   
> > >
> > >    - 对嵌套实体的抽取：分别对于人名/地点的查询
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122005009278.png" alt="image-20241122005009278" height=60 /> 
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122005019819.png" alt="image-20241122005019819" height=63 /> 
> > >
> > >    - 对不连续实体的抽取：对于疾病名字/疾病中间名字的查询
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122005235724.png" alt="image-20241122005235724" height=49 /> 
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122005258231.png" alt="image-20241122005258231" height=49.8 /> 
> >
> > ### $\textbf{2.4.4. }$生成法: 平面实体✅$/$嵌套实体✅$/$不连续实体✅
> >
> > > :one:模型概述
> > >
> > > 1. 含义：采用$\text{seq2seq}$模式，通过生成式得到实体
> > > 2. 示例：
> > >    - 输入：$\text{have muscle pain and fatigue}$
> > >    - 生成：$\text{disorder: “muscle pain”, “muscle fatigue”}$
> > >
> > > :two:模型结构：懒得说了，自己看吧
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122010113167.png" alt="image-20241122010113167" width=700 /> 

# $\textbf{3. }$语义关系抽取

> ## $\textbf{3.1. }$概念与概论
>
> > :one:$\text{RE(Relationship Extraction)}$概念
> >
> > 1. 概念：从文本中提取实体之间的语义关系
> >
> > 2. 示例：从以下关系中提取出关系$\text {Founding-Year(IBM, 1911)/Location(IBM, New York)}$
> >
> >    ```txt
> >    International Business Machines Corporation (IBM or the company) was incorporated in the State of New York on June 16, 1911, as the Computing-Tabulating-Recording Co. (C-T-R)...
> >    ```
> >
> > :two:$\text{NE}$的主要目标
> >
> > |   目标   | 描述                                                         |
> > | :------: | :----------------------------------------------------------- |
> > | 填充模板 | 将自然语言文本$\xrightarrow{转化}$结构化数据，以填充预定义的关系模板 |
> > | 信息组织 | 将文本中的信息组织成对人类和计算机有用的形式                 |
> > |  知识库  | 执行大规模的抽取，以构建/增强(特定领域)结构化知识库          |
> >
> > :three:$\text{NE}$的主要任务
> >
> > |   目标   | 描述                                                      |
> > | :------: | :-------------------------------------------------------- |
> > | 关系分类 | 实体对分类为特定关系类型$/$无关系                         |
> > | 事件抽取 | 按照何人$/$何时$/$何地$/$做了什么的模板，识别文本中的数据 |
>
> ## $\textbf{3.2. RE}$的主要方法
>
> > ### $\textbf{3.2.1. }$人工构建的模式
> >
> > > :one:核心思想：通过人工构建一些词汇$\text{-}$句法($\text{Lexico-Syntactic}$)模式，以实现对实体关系的抽取
> > >
> > > :two:典型模式：$\text{Hearst Patterns}$ 
> > >
> > > |              模式              |          匹配定义          | 示例                                                         |
> > > | :----------------------------: | :------------------------: | :----------------------------------------------------------- |
> > > |     $Y \text{ such as } X$     |   $Y\xrightarrow{示例}X$   | *"The bow lute, such as the Bambara ndang..."*               |
> > > | $\text{Such } Y \text{ as } X$ |   $Y\xrightarrow{子类}X$   | *"...such authors as Herrick, Goldsmith, and Shakespeare."*  |
> > > |    $X \text{ and other } Y$    | $X\xleftrightarrow{同类}Y$ | *"...temples, treasuries, and other important civic buildings."* |
> > > |    $X \text{ or other } Y$     | $X\xleftrightarrow{替代}Y$ | *"bruises, wounds, broken bones or other injuries..."*       |
> > > |    $Y \text{ including } X$    |   $Y\xrightarrow{包含}X$   | *"...common-law countries, including Canada and England..."* |
> > > |   $Y, \text{ especially } X$   |   $Y\xrightarrow{特例}X$   | *"European countries, especially France, England..."*        |
> > >
> > > :three:特点
> > >
> > > 1. 优点：高精度$/$可解释性强$/$可针对特定领域优化
> > > 2. 缺点：低召回$/$难以扩展$/$维护成本高
> >
> > ### $\textbf{3.2.2. }$监督机器学习方法：$\textbf{RE}\boldsymbol{\xrightarrow{转化}}$分类问题
> >
> > > :one:特征方法
> > >
> > > 1. 特征设计：赋予每个实体不同参数，以便捕捉实体间关系
> > >
> > >    |  特征类型  | 含义                                                         |
> > >    | :--------: | :----------------------------------------------------------- |
> > >    | 词级别特征 | 实体周围的单词$/$实体之间的上下文词                          |
> > >    |  实体类型  | 即$\text{NER}$的结果，如$\text{Person/GPE}$                  |
> > >    |  提及级别  | 实体在文本中被提及时的表达形式，有三种(名称$/$普通名词$/$代词) |
> > >    |  重叠特征  | 两个实体之间的距离(词数$/$句法路径)                          |
> > >    | 句法树特征 | 实体短语对应的句法树的结构                                   |
> > >
> > > 2. 整体流程：在预设关系类型$+$标注训练数据基础上
> > >
> > >    |     步骤     | 描述                                                         |
> > >    | :----------: | :----------------------------------------------------------- |
> > >    |    预处理    | 分词$\text{→}$清晰$\text{→}$规范化实体表示                   |
> > >    | $\text{NER}$ | 识别所有实体的类型                                           |
> > >    | 运行辅助工具 | 在之前的基础上，再进行词性标注$/$句法解析等                  |
> > >    |   实体配对   | 将所有实体两两配对，然后逐个二元分类为$(\text{Related/Unrelated})$ |
> > >    |  关系分类器  | 将有关实体对进一步分类为(==预设的==)具体关系类$(\text{Born-in/Located-at...})$ |
> > >
> > > 3. 分类器：机器学习模型($\text{Naïve Bayes}/$最大熵$/\text{SVM}$)，深度学习模型$\text{BiLSTM/BERT}$ 
> > >
> > > :two:核方法
> > >
> > > 1. 核心：通过核函数计算两实体间的相似度，而无需设计特征
> > > 2. 常用：序列核$/$句法树核$/$依存图路径核$/$复合核
> >
> > ### $\textbf{3.2.2. }$半监督机器学习方法: 规避标记大量数据
> >
> > > :one:自举分类法
> > >
> > > 1. 核心思想：以一个小的种子集合为起点$\text{→}$反复扩展数据$\&$模式
> > >
> > > 2. $\text{DIPRE}$算法：
> > >
> > >    - 获取种子集$S$：$S$包含少量已知的**实体对**及其**关系类型**
> > >
> > >      ```text
> > >      <William Shakespeare, The Comedy of Errors>
> > >      <Isaac Asimov, The Robots of Dawn>
> > >      ```
> > >
> > >    - 实例查找：从$S$为起点，在$\text{Web}$上查找种子中元组的所有实例
> > >
> > >      ```txt
> > >      The Comedy of Errors, by William Shakespeare, was...
> > >      The Comedy of Errors, one of William Shakespeare's...
> > >      ```
> > >
> > >    - 提取模式：从查找到的实例中归纳模式
> > >
> > >      ```txt
> > >      ?x, by ?y
> > >      ?x, one of ?y's
> > >      ```
> > >
> > >    - 迭代：将新发现的元组加入集合$S$，不断重复上述步骤直到$S$不再增长
> > >
> > > :two:其它方法
> > >
> > > 1. 主动学习：主动选择一些未标注的数据$\text{→}$向管理员请求真实标签$\text{→}$赋予真实标签后模型显著提升
> > > 2. 标签传播：通过实体关系图(结点$\xleftrightarrow{对应}$实体$/$边权值$\xleftrightarrow{对应}$相似度)的边，将已知标签传给未标注节点
> > > 3. 迁移学习：利用一个任务中学习的关系抽取知识，来提高其他任务中的性能
> >
> > ### $\textbf{3.2.3. }$无监督机器学习方法: 无需标记任何标签
> >
> > >  :one:无监督方法
> > >
> > >  1. 核心思想：不依赖任何标记数据，仅通过聚类自动发现实体潜在的关系
> > >
> > >  2. 总体流程：
> > >
> > >       - 进行$\text{NER}$：标记文本中所有的命名实体
> > >
> > >         ```txt
> > >         Edwin Hubble (PERSON), Marshfield (LOCATION)
> > >         ```
> > >
> > >       - 实体对$/$上下文：根据$\text{NER}$结果生成实体对，并记录下实体对的上下文
> > >
> > >         ```txt
> > >         句子1: Hubble was born in Marshfield
> > >         句子2: Einstein was born at Ulm
> > >         
> > >         实体对1: <Edwin Hubble, Marshfield> 
> > >         实体对2: <Albert Einstein, Ulm>     
> > >         
> > >         上下文1: was born in
> > >         上下文2: was born at
> > >         ```
> > >
> > >       - 相似度计算：以每个实体对的上下文作为特征，计算实体对之间的相似度
> > >
> > >         ```text
> > >         Dist(实体对1,实体对2)=dist(上下文1,上下文2)
> > >         ```
> > >
> > >       - 聚类：根据上述相似度执行聚类，聚类后的每个簇自动生成一个标签
> > >
> > >  :two:远程监督方法
> > >
> > >  1. 核心思想：利用现有知识库中的实体关系，对文本自动标记
> > >
> > >  2. 总体流程：
> > >
> > >       - 关系选取：从==知识库==中选择感兴趣的关系及其实体对
> > >
> > >         ```txt
> > >         Relation: Born-In
> > >         Entity Pairs: <Edwin Hubble, Marshfield>, <Albert Einstein, Ulm>
> > >         ```
> > >
> > >       - 标注文本：在未标注的文本中查找包含这些实体对的句子
> > >
> > >         ```txt
> > >         Hubble was born in Marshfield. -> Einstein, born (1879), Ulm.
> > >         ```
> > >
> > >       - 提取特征：从标注的句子中提取特征，例如依存路径$/$上下文等
> > >
> > >         ```txt
> > >         依存路径: PER -> was born in -> LOC
> > >         上下文: was born in
> > >         ```
> > >
> > >       - 训练分类器：.......
> > >
> >
> > ### $\textbf{3.2.4. }$深度学习方法
> >
> > > $\small\begin{array}{|c|l|c|}
> > > \hline \text { Model } & \text { Feature Set } & \text { F1 } \\
> > > \hline \text { SVM } & \text { POS, prefixes, morphological, WordNet, dependency parse, } & \\
> > > \text { (Rink and Harabagiu, 2010) } & \text { Levin classed, ProBank, FramNet, NomLex-Plus, } \\
> > > & \text { Google n-gram, paraphrases, TextRunner } & 82.2 \\
> > > \hline \text { CNN } & \text { WV (Turian et al., 2010) (dim=50) } & 69.7 \\
> > > \text { (Zeng et al., 2014) } & \text { + PF + WordNet } & 82.7 \\
> > > \hline \text { RNN } & \text { WV (Turian et al., 2010) (dim=50) + PI } & 80.0 \\
> > > \text { (Zhang and Wang, 2015) } & \text { WV (Mikolov et al., 2013) (dim=300) + PI } & 82.5 \\
> > > \hline \text { SDP-LSTM } & \text { WV (pretrained by word2vec) (dim=200), syntactic parse } & 82.4 \\
> > > \text { (Yan et al., 2015) } & \text { + POS + WordNet + grammar relation embeddings } & 83.7 \\
> > > \hline \text { BLSTM } & \text { WV (Pennington et al., 2014) (dim=100) } & 82.7 \\
> > > \text { (Zhang et al., 2015) } & \text { + PF + POS + NER + WNSYN + DEP } & 84.3 \\
> > > \hline \text { BLSTM } & \text { WV (Turian et al., 2010) (dim=50) + PI } & 80.7 \\
> > > \text { Att-BLSTM } & \text { WV (Turian et al., 2010) (dim=50) + PI } & 82.5 \\
> > > \text { BLSTM } & \text { WV (Pennington et al., 2014) (dim=100) + PI } & 82.7 \\
> > > \text { Att-BLSTM } & \text { WV (Pennington et al., 2014) (dim=100) + PI } & 84.0 \\
> > > \hline
> > > \end{array}$ 
>
> ## $\textbf{3.3. }$共同抽取($\textbf{Joint Extraction}$)
>
> > :one:动机
> >
> > 1. 传统$\text{Pipeline}$结构方法：先独立进行$\text{NER}$再独立进行$\text{RE}$，缺点在于$\text{NER}$步的错误会在$\text{RE}$步放大
> > 2. $\text{JE}$方法的核心思想：**同时抽取实体和它们之间的关系** 
> >
> > :two:$\text{JE}$的新型标注方案
> >
> > 1. 输入输出：包含多个实体及其关系的自然语言句子$\text{→}$以三元组的形式表示的关系
> >
> >    ```txt
> >    (United States, Country-President, Trump)
> >    ```
> >
> > 2. 标签组成：
> >
> >    |   组成   | 解释                                                         |
> >    | :------: | :----------------------------------------------------------- |
> >    | 位置详细 | $\text{B}$(实体开头)$/\text{I}$(实体中间)$/\text{E}$(实体结尾)$/\text{O}$(单个词的实体) |
> >    | 关系类型 | 从预定义的关系类型集合中获取                                 |
> >    | 关系角色 | 用`1`$/$`2`表示三元组中的第$1/2$个实体                       |
> >
> >    ```txt
> >    United -> B-CP-1 (United States的起始/与Country-President有关/三元组第一个实体) 
> >    Trump  -> S-CP-2 (Trump为单一词的实体/与Country-President有关/三元组第二个实体)
> >    ```
> >
> > 3. 特殊规则：最近原则$\text{+}$不考虑重叠关系
> >
> > :three:其它内容：这部分内容都来自三篇文献**要是考了我倒立洗头**，文献列出来了具体就不多说了
> >
> > - [$\text{ACL’17}$](https://doi.org/10.48550/arXiv.1706.05075)$/$[$\text{EMNLP’17}$](https://doi.org/10.18653/v1/D17-1182)$/$[$\text{ACL’20}$](https://doi.org/10.48550/arXiv.1909.03227) 

# $\textbf{4. }$共指消解

> ## $\textbf{4.1. }$概论与概述
>
> > :one:基本概念：以句$\text{Mary loves her dog. She takes it for a walk every morning}$为例
> >
> > |             概念             | 含义                             | 示例                                |
> > | :--------------------------: | :------------------------------- | :---------------------------------- |
> > |     提及$\text{Mention}$     | 文本中指代某一实体的词或短语     | $\text{Mary/her dog/She/it}$        |
> > |  先行词$\text{Antecedent}$   | 某一实体的最早提及               | $\text{Mary}$是$\text{She}$的先行词 |
> > | 共指关系$\text{Coreference}$ | 不同的提及指向同一个实体         | $\text{Mary/She}$为共指             |
> > |      簇$\text{Cluster}$      | 一个文档中所有共指提及组成的集合 | $\{\text{Mary, She}\}$              |
> >
> > :two:共指消解及其类型
> >
> > |       类型       | 含义                               | 示例                               |
> > | :--------------: | :--------------------------------- | :--------------------------------- |
> > |   代词共指消解   | 识别代词与其指代的实体             | *Mary loves her dog. She...*       |
> > | 命名实体共指消解 | 识别命名实体之间是否指向同一实体   | *Barack Obama... Obama...*         |
> > |   事件共指消解   | 识别文本中不同提及是否指向同一事件 | *An earthquake... The disaster...* |
> >
> > :three:共指消解的评价指标
> >
> > |     指标      | 基于什么 | 定义                                                   |
> > | :-----------: | :------: | :----------------------------------------------------- |
> > | $\text{MUC}$  |    边    | 衡量预测簇和真实簇中提及之间的连接关系                 |
> > | $\text{B}^3$  |   提及   | 衡量每个提及所属预测簇和真实簇的交集比例               |
> > | $\text{CEAF}$ |    簇    | 通过最优对齐(如匈牙利算法)计算预测簇与真实簇的匹配得分 |
>
> ## $\textbf{4.2. }$共指消解方法
>
> > :one:经典方法
> >
> > |           方法           | 原理概述                                                     |
> > | :----------------------: | :----------------------------------------------------------- |
> > |         基于规则         | 通过定义手工规则(性别/数的一致性)来判断提及之间的共指关系    |
> > |         特征方法         | 提取特征(距离/句法结构/实体类型等)，并使用分类器(如$\text{SVM}$)预测共指关系 |
> > |  $\text{Mention-Pair}$   | 将每个提及对作为一个独立的实例进行二分类(共指/非共指)        |
> > | $\text{Cluster-Ranking}$ | 通过对簇进行排序和合并，从全局优化共指关系的匹配             |
> >
> > :two:深度学习方法
> >
> > |          方法           | 原理概述                                                     |
> > | :---------------------: | :----------------------------------------------------------- |
> > | $\text{End-to-End}$模型 | 直接输入文本，提取提及并预测共指关系                         |
> > |    $\text{BERT}$方法    | 利用$\text{BERT}$进行上下文嵌入，结合分类器进行预测          |
> > |      基于图的模型       | 以提及为节点$/$边为潜在共指关系构建图，用$\text{GNN}$进行训练和预测 |
