# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119154414877.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/24950624a141b9941650b6187682aa9.jpg" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{1. }$关于语言模型的预备知识

> ## $\textbf{1.1. }$语言模型概念
>
> > :one:含义：自然语言在不同语言单位上的数学模型$\text{→}$实现自然语言的可计算性
> >
> > :two:类型概览
> >
> > |       模型       | 含义                                   |
> > | :--------------: | :------------------------------------- |
> > |   词袋语言模型   | 用文中词汇表示文本                     |
> > |   概率语言模型   | 根据给定词汇序列来预测下一个词汇的概念 |
> > |   主题语言模型   | 利用非监督方法获得文档中隐含的主题     |
> > | 神经网络语言模型 | 利用神经网络学习词汇$/$句子$/$字符     |
>
> ## $\textbf{1.2. }$语言模型的评价指标
>
> > :one:召回率与精确度
> >
> > 1. 真假性$\text{\&}$阴阳性：
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109152520072.png" alt="image-20241109152520072" width=199 /> 
> >
> >    |      **实际\预测**      |         **$\mathbf{C}_1$**         |      **$\neg \mathbf{C}_1$**       |
> >    | :---------------------: | :--------------------------------: | :--------------------------------: |
> >    |   **$\mathbf{C}_1$**    | $\text{True Positives(TP)}$真阳性  | $\text{False Negatives(FN)}$假阴性 |
> >    | **$\neg \mathbf{C}_1$** | $\text{False Positives(FP)}$假阳性 | $\text{True Negatives(TN)}$真阴性  |
> >
> > 2. 召回率与精确率
> >
> >    - $\text{Recall}\text{=}\cfrac{\text{TP}}{\text{TP+FN}}$：表示真实阳性中/被预测为阳性的比率
> >    - $\text{Precision}\text{=}\cfrac{\text{TP}}{\text{TP+FP}}$：表示被预测为阳性中/真实阳性的比率，一词多义会使之降低
> >    - $\displaystyle\text { F1-score =}2 \text{×} \cfrac{\text { Precision } \text{×} \text { Recall }}{\text { Precision +} \text { Recall }}$：二者的调和平均
> >
> > 3. 一词多义$\&$一义多词
> >
> >    - 一词多义$\text{→}$多义间不相关的含义被认为相关$\text{→}$假阳性增加$\text{→Precision}$降低
> >    - 一义多词$\text{→}$多词间相同的含义被认为是不同$\text{→}$假阴性增加$\text{→Recall}$降低
> >
> > :two:困惑度
> >
> > 1. 含义：反应不确定性，即困惑度越低$\text{→}$模型预测下一个元素时选择更少$\to$预测越准确
> > 2. 公式：$\displaystyle{}\text { Perplexit=}2^{\text {Cross-Entropy}}\text{=}\exp \left(-\cfrac{1}{N} \sum_{i=1}^N \ln P\left(w_i | w_1^{i-1}\right)\right)$ 
> >    - $P\left(w_i | w_1^{i-1}\right)$是谟型给定前序$w_1^{i-1}$条件下，预测词$w_i$的概率
>
> ## $\textbf{1.3. }$自然语言的统计特性
>
> > ### $\textbf{1.3.1. Zipf}$定律
> >
> > > :one:$\text{Zipf}$定律
> > >
> > > 1. 内容：令出现频率第$r$高的词汇出现频率为$f(r)$，则有$f(r)\text{=}\cfrac{\text{Const}}{r^{s}}$其中$s\text{≈}1$
> > >
> > > 2. 含义：对于词频分布，最常见词的分布极为普遍$+$大多数词出现频率极低
> > >
> > > 3. 解释：
> > >
> > >    |   解释模型   | 含义                                                         |
> > >    | :----------: | ------------------------------------------------------------ |
> > >    |  米勒猴实验  | 胡乱生成的带有字母$+$空格的序列，词频和排名也符合幂律关系    |
> > >    | 最小努力原则 | 通过词频差异最小化交流的成本                                 |
> > >    | 优先连接机制 | 网络结构中，新节点倾向于连接度数更大的点，与$\text{Zipf}$类似 |
> > >
> > > :two:$\text{Zipf}$定律的实验
> > >
> > > 1. 符合程度：$f(r)\text{=}\cfrac{\text{Const}}{r^{s}}\to{}\log{f(r)\text{=}}\log{C}-s\log{r}$故可通过检测后者线性程度
> > >
> > > 2. 实验结论：幂律分布很常见$+$排名靠中间的术语会更符合
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107220033751.png" alt="image-20241107220033751" width=570 />  
> > >
> > > :three:$\text{Zipf}$定律与索引
> > >
> > > 0. 倒排索引：用于快速全文检索的数据结构，示例如下
> > >
> > >    - 文档
> > >
> > >      ```txt
> > >      Doc1: fat cat rat rat 
> > >      Doc2: fat cat 
> > >      Doc3: fat
> > >      ```
> > >
> > >     - 构建的倒排索引
> > >
> > >       ```txt
> > >       fat: Doc1 Doc2 Doc3
> > >       cat: Doc1 Doc2
> > >       rat: Doc1
> > >       ```
> > >
> > >
> > > 1. 词频太高/太低的词都不适合索引，会导致返回太多/太少的文档，适中的才最有价值
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107221536310.png" alt="image-20241107221536310" width=320 /> 
> > >
> > > 2. 基于$\text{Zipf}$定律，去处高频$\text{Stopword}$能优化倒排索引时空开销，如下为倒排索引的一个实例
> >
> > ### $\textbf{1.3.2. Heaps}$定律
> >
> > > :one:$\text{Heaps}$定律
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241122155620347.png" width=600 />   
> > >
> > > 1. 内容：词汇表大小$V$与文本词数$n$满足$V\text{=}Kn^{\beta}$ 
> > > 2. 参数：$10\text{≤}K\text{≤}100$且$0.4\text{≤}\beta\text{≤}0.6$，当$K\text{=}44$与$\beta\text{=}0.49$最匹配
> > >
> > > :two:用途：预测随文本增长词汇表$\&$倒排索引大小的变化
> >
> > ### $\textbf{1.3.1. Benford}$定律(第一数字法则)
> >
> > > :one:$\text{Benford}$定律
> > >
> > > 1. 背景：在许多社会现象中，数据首位数往往分布不均(为$1$概率最大$\xrightarrow{依次递减}$为$9$概率最小)
> > > 2. 定律：令数据集中$d$作为首字母的概率$P(d)=\lg{\left(1+\cfrac{1}{d}\right)}$，$d\text{＞}9$及非十进制时依旧适用
> > >
> > > :two:对$\text{Benford}$定律的一些思考
> > >
> > > 1. 适用：跨数量级变化的数据集，如财务数据和自然现象
> > > 2. 应用：检测数据造假、异常值、验证财务报告真实性
> > > 3. 成因：还不具备完全的可解释性，大概是因为数据==在对数尺的分布==  

# $\textbf{2. }$词袋语言模型

> ## $\textbf{2.1. BoW}$模型
>
> > :one:基本步骤：以句$\text{I love machine learning}$以及$\text{Machine learning is fun}$为例
> >
> > |  步骤  | 示例                                                         |
> > | :----: | ------------------------------------------------------------ |
> > |  分词  | $\text{I \\ love \\ machine \\ learning \\ }\text{Machine \\ learning \\ is \\ fun}$ |
> > | 词汇表 | $V\text{=[I, love, machine,lerning, is, fun]}$               |
> > | 向量化 | 第一句变为$A_1\text{=[1,1,1,1,0,0]}$第二局变为$A_2\text{=[0,0,1,1,1,1]}$ |
> >
> > :two:特点
> >
> > 1. 原理上：完全忽略了语法/词序，默认词与词间的概率分布独立
> > 2. 效果上：
> >    - 优点：实现极其简单，但高效且应用广泛
> >    - 缺点：无法区分$\text{\&}$一义多词，如同义词替换后的两文档相似度低于实际值
>
> ## $\textbf{2.2. TF-IDF}$模型
>
> > :one:$\text{TF-IDF}$值
> >
> > 1. 计算: $\text{TF-IDF}(t,d)\text{=TF}(t,d)\text{×IDF(}t)\text{→}\begin{cases}词频\text{TF}(t,d)=\cfrac{词t在文档d出现次数}{文档d总词数}\\\\逆文档频\text{IDF(t)=}\log\cfrac{文档总数}{\text{DF}(t)(包含t的文档数)\text{+}1}\end{cases}$  
> > 2. 含义: $\text{TF-IDF}(t,d)$越高，代表词$t$对文档$d$越重要
> >
> > :two:$\text{TF-IDF}$值改进：原始词频值往往不是所需的
> >
> > 1. 对原始词频$\text{TF}(t,d)$的改进
> >
> >    | 词频类型 |                             公式                             | 意义                                         |
> >    | :------: | :----------------------------------------------------------: | -------------------------------------------- |
> >    |   对数   |               $1\text{+}\log (\text{TF}(t,d))$               | 压缩较高词频，减少其对相关性影响的夸大       |
> >    |   增强   | $\displaystyle{}0.5\text{+}\cfrac{0.5 \text{×}\text{TF}(t,d)}{\max _{\mathrm{t}}\text{TF}(t,d)}$ | 映射词频到$0.5\text{→1}$，防止高频词权重过大 |
> >    |   布尔   | $\begin{cases}1 \,\text{ if  } \text{TF}(t,d)>0 \\0  \,\text{  otherwise }\end{cases}$ | 不关注具体的词频值，仅表示是否出现           |
> >    | 平均对数 | $\cfrac{1+\log \left(\text{TF}(t,d)\right)}{1+\log \left(\mathrm{ave}_{\mathrm{t∈d}}, \left(\text{TF}(t,d)\right)\right)}$ | 使词频高的词与低的词之间的差距不会过大       |
> >
> > 2. 对文档频率$\text{DF}(t)$的改进：$N$是文档总数
> >
> >    |     文档频率$\text{DF}(t)$     | 公式                                                         | 意义                       |
> >    | :----------------------------: | :----------------------------------------------------------- | -------------------------- |
> >    |   逆文档频率$\text{IDF}(t)$    | 即$\log{}\cfrac{N}{\text{DF}(t)}$者$\log{}\cfrac{N}{\text{DF}(t)\text{+1}}$ | 衡量词在文档集合中的稀有性 |
> >    | 概率文档频率$\text{ProbDF(}t)$ | $\max\left\{0,\log\cfrac{N-\text{DF}(t)}{\text{DF}(t)}\right\}$ | 通过概率角度评估词的稀有性 |
> >
> > 
> >
> > 3. 归一化：对于$\textbf{TF-IDF}\text{=}\begin{bmatrix}
> >    \text{TF-IDF}(t_1,d_1) & \text{TF-IDF}(t_1,d_2) & \cdots & \text{TF-IDF}(t_1,d_n) \\
> >    \text{TF-IDF}(t_2,d_1) & \text{TF-IDF}(t_2,d_2) & \cdots & \text{TF-IDF}(t_2,d_n) \\
> >    \vdots  & \vdots  & \ddots & \vdots  \\
> >    \text{TF-IDF}(t_m,d_1) & \text{TF-IDF}(t_m,d_2) & \cdots & \text{TF-IDF}(t_m,d_n) \\
> >    \end{bmatrix}$ 
> >
> >    | 归一类型 | 公式                                                         | 意义                                      |
> >    | :------: | ------------------------------------------------------------ | ----------------------------------------- |
> >    | 余弦归一 | $\textbf{TF-IDF}\text{×}\cfrac{1}{\displaystyle{}\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}[\text{TF-IDF}(t_i,d_j)]^{2}}}$ | 用于计算文档间的余弦相似度                |
> >    | 基准归一 | $\textbf{TF-IDF}\text{×}\cfrac{1}{mn}$                       | 消除文档集合大小对权重的影响              |
> >    | 字长归一 | $\textbf{TF-IDF}\text{×}\cfrac{1}{\text{(CharLen)}^{\alpha}}$ | 适用于不同长度的文档，且$\alpha\text{<}1$ |
> >
> > :three:基于$\text{TF-IDF}$的余弦相似度
> >
> > 1. $\text{TF-IDF}$值：对于文档$d_1$和$d_2$，词汇表长为$m$
> >    - $\textbf{TF-IDF}\text{=}\begin{bmatrix}
> >      \text{TF-IDF}(t_1,d_1)&\text{TF-IDF}(t_1,d_2)\\
> >      \text{TF-IDF}(t_2,d_1)&\text{TF-IDF}(t_2,d_2)\\
> >      \vdots  & \vdots  \\
> >      \text{TF-IDF}(t_m,d_1) & \text{TF-IDF}(t_m,d_2)\\
> >      \end{bmatrix}\xrightarrow{余弦归一化}\begin{bmatrix}
> >        \text{tf-idf}(t_1,d_1)&\text{tf-idf}(t_1,d_2)\\
> >        \text{tf-idf}(t_2,d_1)&\text{tf-idf}(t_2,d_2)\\
> >        \vdots  & \vdots  \\
> >        \text{tf-idf}(t_m,d_1) & \text{tf-idf}(t_m,d_2)\\
> >      \end{bmatrix}$  
> > 2. 两文档余弦值：
> >    - 未归一化表示：$\text{sim}(d_1, d_2) = \cfrac{\displaystyle{}\sum_{j=1}^m \text{TF-IDF}(t_j, d_1) \cdot \text{TF-IDF}(t_j, d_2)}{\displaystyle{}\sqrt{\sum_{j=1}^m (\text{TF-IDF}(t_j, d_1))^2} \cdot \sqrt{\sum_{j=1}^m (\text{TF-IDF}(t_j, d_2))^2}}$
> >    - 归一化表示 ：$\displaystyle{}\text{sim}(d_1, d_2) = \sum_{j=1}^m \text{tf-idf}(t_j, d_1) \cdot \text{tf-idf}(t_j, d_2)$ 