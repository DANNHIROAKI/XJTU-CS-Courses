# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119154414877.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/24950624a141b9941650b6187682aa9.jpg" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{1. }$概率模型

> ## $\textbf{1.1. }$马可夫模型
>
> > ### $\textbf{1.1.1. }$概述
> >
> > > :one:$\text{Markov}$模型概述
> > >
> > > 1. 定义：描述系统状态的统计模型，用过去一段时间的状态描述当前状态(而忽略更早状态)
> > >
> > > 2. 类型：
> > >
> > >    |            类型            | 含义                           | 示例                         |
> > >    | :------------------------: | :----------------------------- | :--------------------------- |
> > >    | 显马可夫模型($\text{VMM}$) | 状态是可以直接观察到           | 天气预报(晴/雨/雪)           |
> > >    | 隐马可夫模型($\text{HMM}$) | 状态只能通过直接观测到的值推断 | 语音识别(声波$\text{→}$发音) |
> > >
> > > :two:$\text{Markov}$模型的数学描述
> > >
> > > 1. 状态描述：系统有$S_1, S_2,\text{...}, S_N$共$N$个状态$/$任意$t$时的状态表示为$q_t$；例如$q_t\text{=}S_j$ 
> > > 2. 马可夫假设：
> > >    - $k$​阶马可夫：$\begin{align}& {P}\left(q_t\text{=}S_j|q_{t-1}\text{=}S_{i_1}, q_{t-2}\text{=}S_{i_2},\text{...}\right)\text{=}{P}\left(q_t\text{=}S_j|q_{t-1}\text{=}S_{i_1},\text{...},q_{t-k}\text{=}S_{i_k}\right) &\\
> > >      & \Bigg\Downarrow {{k\text{=1}(一阶马可夫)}} &\\
> > >      &{P}\left(q_t\text{=}S_j|q_{t-1}\text{=}S_{i_1}, q_{t-2}\text{=}S_{i_2},\text{...}\right)\text{=}{P}\left(q_t\text{=}S_j|q_{t-1}\text{=}S_{i_1}\right) &
> > >      \end{align}$
> > >    - 时间无关：让上述过程独立于时间，即$\text{∀}t$都有${P}\left(q_t\text{=}S_j|q_{t-1}\text{=}S_{i_1},\text{...},q_{t-k}\text{=}S_{i_k}\right)$ 
> > > 3. 状态转移概率：
> > >    - 含义：一阶马可夫中，从$i$状态转移到$j$状态的概率，即$P(q_t\text{=}S_j|q_{t\text{-}1}\text{=}S_i)\text{=}a_{ij}$
> > >    - 性质：$\displaystyle{}\sum_{j=1}^Na_{ij}\text{=}1$ 
> >
> > ### $\textbf{1.1.2. }$隐马可夫过程及$\textbf{Viterbi}$算法 
> >
> > > :zero:问题描述
> > >
> > > 1. 一个隐马可夫的示例：$\text{From}$ [$\text{Wikipedia}$](https://en.wikipedia.org/wiki/Hidden_Markov_model  )   
> > >
> > >    | $\textbf{Item}$ | 描述                                                         |
> > >    | :-------------: | :----------------------------------------------------------- |
> > >    |      条件       | 只考虑三种活动(散步/购票/购物)，两种天气(阴/晴)              |
> > >    |      假设       | 一位朋友，每天告诉你<font color=red>他的活动</font>          |
> > >    |      问题       | 在他告诉你每天所做事情基础上，你要猜他<font color=orange>那边的天气</font> |
> > >
> > > 2. 两种空间：观测$\xrightarrow{预测}$状态
> > >
> > >    |                空间                |                  符号                  | 示例           |
> > >    | :--------------------------------: | :------------------------------------: | -------------- |
> > >    |  <font color=red>观测空间</font>   | $O \text{=} \{o_1, o_2, \ldots, o_T\}$ | 朋友的活动     |
> > >    | <font color=orange>状态空间</font> |  $X\text{=}\{x_1, x_2, \ldots, x_T\}$  | 朋友那边的天气 |
> > >
> > > 3. 符号：三种矩阵$\textbf{A/B/}\boldsymbol{\pi}$的项
> > >
> > >    |           符号            | 含义                                      | 示例                   |
> > >    | :-----------------------: | ----------------------------------------- | ---------------------- |
> > >    |  初始状态概率$\pi_{x_i}$  | 初始状态($t\text{=}0$时状态)为$x_i$的概率 | 第一天的天气           |
> > >    | 状态转移概率$a_{x_ix_j}$  | 从一状态$x_i$转化为下一状态$x_j$的概率    | 今天晴天明天阴天的概率 |
> > >    | 符号发射概率$b_{x_i o_j}$ | 从状态$x_j$从而导向某种观测$o_j$的概率    | 晴天时朋友散步的概率   |
> > >
> > > :one:问题$1$：给定$\mu\text{=}(A,B,\pi)$求解观察序列$O \text{=} \{o_1, o_2, \ldots, o_T\}$发生的概率$P(O|\mu)$ 
> > >
> > > 1. 分解：$ \displaystyle{}{P}(O | \mu) \xrightarrow{边缘化} \sum_X {P}((O, X)|\mu) \xrightarrow{分解} \sum_X{P}(O| (X, \mu)) \text{×} {P}(X |\mu)$ 
> > > 2. 代入：$\displaystyle{}\mathrm{P}(O| \mu) \xrightarrow [\mathrm{P}(O|(X, \mu)) \text{=} b_{x_1 o_1} \text{×} b_{x_2 o_2} \text{×}\text{...}\text{×} b_{x_T o_T}]{\mathrm{P}(X |\mu) \text{=} \pi_{x_1} \text{×} a_{x_1 x_2} \text{×} a_{x_2 x_3}\text{×} \text{...}\text{×} a_{x_{T-1} x_T}} \sum_{x_1\text{→} x_T} \left(\pi_{x_1} \text{×} b_{x_1 o_1} \text{×} \prod_{t=1}^{T-1} a_{x_t x_{t+1}} \text{×} b_{x_{t+1} o_{t+1}}\right)$ 
> > >
> > > :two:问题$2$：给定参数$\mu\text{=}(A,B,\pi)$与观察所得$O \text{=} \{o_1, o_2, \ldots, o_T\}$，最大化${P}(X | (O, \mu))$
> > >
> > > 1. 含义：找到最可能的状态序列$X\text{=}\{x_1, x_2, \ldots, x_T\}$，以解释观测
> > >
> > > 2. 求解：$\text{Viterbi}$算法，计算每个时刻$t$的最路径概率，最终找出整体最优路径；示例如下
> > >
> > >    - 转换概率：状态$\xleftrightarrow{}$状态$/$状态$\xrightarrow{}$观测
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119210240035.png" alt="image-20241119210240035" width=370 /> 
> > >
> > >    - $\text{Viterbi}$算法路径：
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119215739501.png" alt="image-20241119215739501" width=500 /> 
> > >
> > > :three:问题$3$：观测到$O$，如何调整$\mu\text{=}(A,B,\pi)$最大化$P(O|\mu)$
> > >
> > > 1. 期望值最大化算法$\text{(EM)}$ 
> > > 2. 最大似然估计($\text{MLE}$).........
>
> ## $\textbf{1.2. }$贝叶斯模型
>
> > :one:$\text{Bayes}$定律
> >
> > 1. 不展开形式：$P\left(A| B\right)\text{=}\cfrac{P\left(A\right) P\left(B|A\right)}{P(B)}\Leftarrow\begin{cases}P(A|B)\text{=}\cfrac{P(AB)}{P(B)}\\\\P(B|A)\text{=}\cfrac{P(AB)}{P(A)}\end{cases}$ 
> >
> >    |                             参数                             | 含义                                                         | 示例                 |
> >    | :----------------------------------------------------------: | :----------------------------------------------------------- | :------------------- |
> >    | <font color=red>$P(A)$</font>$/$<font color=green>$P(B)$</font> | 先验概率，即对<font color=red>$A$</font>$/$<font color=green>$B$</font>发生的经验推断(臆测) | 发病率历史数据       |
> >    | <font color=red>$P(A\|B)$</font>$/$<font color=green>$P(B\|A)$</font> | 后验概率，即已知<font color=red>$B$</font>$/$<font color=green>$A$</font>后<font color=red>$A$</font>$/$<font color=green>$B$</font>发生的概率 | 检测后个体的患病概率 |
> >
> > 2. 展开后：$P(A|B)\text{=}\cfrac{P(A)P(B|A)}{\displaystyle\sum_{j}P(B|A_j)P(A_j)}\Leftarrow{}{P(B)}\text{=}\displaystyle\sum_{j}P(BA_j)\text{=}\sum_{j}P(B|A_j)P(A_j)$ 
> >
> > :two:朴素贝叶斯分类器
> >
> > 1. 模型描述：
> >
> >    - 假设：决定各分类的属性之间是相互独立(简单粗暴/但也损失了分类精度)
> >    - 前提：样本$X(a_1,a_2,...,a_n)$有$n$个属性$\{A_1,A_2,...,A_n\}$且有$m$个类$\{C_1,C_2,...,C_m\}$
> >
> >    - 分类：将$X(a_1,a_2,...,a_n)$归类为$C_i\xLeftrightarrow{等价}P\left(C_i | X\right)\text{>}P\left(C_j|X\right)$，其中$C_j$为除$C_i$任一类
> >
> > 2. 模型分析：
> >
> >    - $P\left(C_i|X\right)\text{=}\cfrac{P\left(X|C_i\right) P\left(C_i\right)}{P(X)}\xRightarrow{P(X)为常数}$最大化$P\left(X|C_i\right) P\left(C_i\right)$以分类
> >    - $P(X|C_i)P(C_i)\text{=}P(A_1\text{=}a_1,A_2\text{=}a_2,\cdots,A_n\text{=}a_n|C_i)P(C_i)\xRightarrow{各属性独立}\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)$ 
> >
> > 3. 模流程：
> >
> >    | 阶段 | 操作                                                         |
> >    | :--: | :----------------------------------------------------------- |
> >    | 准备 | 确定样本的属性，获取相应的样本                               |
> >    | 训练 | 对每个类别计算$P(C_i)\text{→}$对每个属性计算$P(a_k|C_i)$     |
> >    | 应用 | 对新样本$\Lambda$计算其对每个类别的$P(\Lambda{}|C_i)P(C_i)\text{→}$找到使之最大的$C_i$以归类之 |
> >
> > 4. 示例：判断学历为大学，年薪$\text{30-40}$，薪水$\text{20000-30000}$的员工的性别
> >
> >    $\begin{array}{|cccccc|}
> >    \hline \text { 样本 } & \text { 性别 } & \text { 工作内容 } & \text { 学历 } & \text { 年龄 } & \text { 薪水 } \\
> >    \hline 1 & \text { 女 } & \text { 送货 } & \text { 大学 } & 20-30 & 20000-30000 \\
> >    \hline 2 & \text { 男 } & \text { 包装 } & \text { 大学 } & >40 & >40000 \\
> >    \hline 3 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & 30-40 & 20000-30000 \\
> >    \hline 4 & \text { 男 } & \text { 包装 } & \text { 高中 } & 30-40 & 20000-30000 \\
> >    \hline 5 & \text { 男 } & \text { 送华 } & \text { 大学 } & >40 & 30000-40000 \\
> >    \hline 6 & \text { 女 } & \text { 烘烤 } & \text { 高中 } & 20-30 & 20000-30000 \\
> >    \hline 7 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & 20-30 & <20000 \\
> >    \hline 8 & \text { 女 } & \text { 包装 } & \text { 大学 } & 30-40 & 20000-30000 \\
> >    \hline 9 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & >40 & 20000-30000 \\
> >    \hline 10 & \text { 男 } & \text { 包装 } & \text { 大学 } & 20-30 & <20000 \\
> >    \hline
> >    \end{array}$
> >
> >    - $\begin{cases}P(包装|女)\text{×}P(大学|女)\text{×}P(30\text{-}40|女)\text{×} P(20000\text{-}30000|女)\text{×}P(女)\text{=}0.0222\\\\P(包装|男)\text{×}P(大学|男)\text{×}P(30\text{-}40|男)\text{×} P(20000\text{-}30000|男)\text{×}P(男)\text{=}0.0315\end{cases}$
> >    - 根据给定条件，应归类为男性
> >
>
> ## $\textbf{1.3. }$平滑技术
>
> > :one:基本概念
> >
> > 1. 稀疏问题：某些变量在训练集未出现(但可能在测试集出现)$\text{→}$概率被估计为$0\text{→}$为后续计算造成麻烦
> > 2. 平滑概念：为所有分配一个非零概率$\text{→}$解决稀疏问题$\text{→}$提高模型泛化能力
> >
> > :two:$\text{Additive}$平滑
> >
> > 1. 方法：普通(频率估计的)概率$p_i\text{=}\cfrac{x_i}{N}\xrightarrow{\text{Additive}平滑}p_{i,\alpha\text{-smoothed}}\text{=}\cfrac{x_i\text{+}\alpha}{N\text{+}\alpha}$ 
> > 2. 参数：$d$是可能变量的总数，$\alpha$为平滑参数($\alpha{=}1$时即变为<font color=red>$\text{Laplace}$变换</font>)
> >
> > :three:图灵($\text{Good-Turing}$)估计
> >
> > 1. 方法：原有计数$r\xrightarrow{调整}r\text{*=}(r\text{+}1)\cfrac{n_{r\text{+}1}}{n_{r}}$，其中$n_r$表示计数为$r$的变量个数
> > 2. 示例：若鱼类及数目为$\text{3 perch/2 white/1 trout/1 salmon/1 eel}\text{→}\text{trout}$数目估计为$(1\text{+}1)\cfrac{1}{3}$

# $\textbf{2. }$语言模型概述

> ## $\textbf{2.1. }$基本概念
>
> > :one:语言模型含义：计算一个句子的概率的概率模型，如已知一个句子已有的词预测下一个词
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241120184557440.png" alt="image-20241120184557440" width=350 /> 
> >
> > :two:数学描述：对句子$\{x^{(1)}, x^{(2)}, \text{...}, x^{(t)}\}$
> >
> > 1. 对单词：计算下一词出现的概率分布$P\left(x^{(t+1)} | x^{(t)}, \text{...}, x^{(1)}\right)$ 
> >
> > 2. 对句子：计算句子出现的概率分布$\displaystyle{}{P}\left(x^{(1)}, \text{...}, x^{(T)}\right)\text{=}\prod_{t=1}^T P\left(x^{(t)} | x^{(t-1)}, \text{...}, x^{(1)}\right)$  
>
> ## $\textbf{2.2. }\textbf{N-gram}$概率模型
>
> > :one:$\text{N-gram}$含义：以$\text{The girl opened her book}$为例
> >
> > |  $\text{N-gram}$  |     含义     | 示例                                                       |
> > | :---------------: | :----------: | :--------------------------------------------------------- |
> > | $\text{Unigrams}$ |    单个词    | $\text{The, girl, opened, her, book}$                      |
> > | $\text{Bigrams}$  | 两个连续的词 | $\text{The girl, girl opened, opened her, her book}$       |
> > | $\text{Trigrams}$ | 三个连续的词 | $\text{The girl opened, girl opened her, opened her book}$ |
> > | $\text{4-grams}$  | 四个连续的词 | $\text{The girl opened her, girl opened her book}$         |
> >
> > :two:$\text{N-gram}$概率计算
> >
> > 1. 假设：$P\left(x^{(t+1)} | x^{(t)}, x^{(t-1)}, \ldots, x^{(1)}\right) \text{≈} P\left(x^{(t+1)} | x^{(t)}, \ldots, x^{(t-n+2)}\right)$即当前词仅依赖前$n\text{-1}$词
> >
> > 2. 概率：$P\left(x^{(t+1)}| x^{(t)},\text{...}, x^{(t-n+2)}\right) \text{≈} \cfrac{\text{count}\left(x^{(t+1)}, x^{(t)}, \text{...}, x^{(t-n+2)}\right)}{\text{count}\left(x^{(t)}, \text{...}, x^{(t-n+2)}\right)}$即$\cfrac{\textcolor{red}{\text{N-gram}数量}}{\textcolor{red}{\text{(N-1)-gram}数量}}$ 
> >
> > 3. 示例：$P(\text {book} | \text {girl opened her})\text{=}\cfrac{\text {count(girl opened herbook})}{\text{count}(\text {girl opened her})}$ 
> >
> > :three:稀疏性问题：
> >
> > |  处理  | 示例                        | 处理                                                         |
> > | :----: | --------------------------- | ------------------------------------------------------------ |
> > | 平滑化 | $w_1,w_2,w_3,w_X$不在语料库 | ${}P({w_X} |w_1,w_2,w_3)\text{=}\cfrac{\text{count}(w_1,w_2,w_3,w_X)\text{+}1}{\text{count}(w_1,w_2,w_3)\text{+}V}$ |
> > |  回退  | $w_1,w_2,w_3$不在语料库     | 用$w_1,w_2,\_\_$的$\text{3-gram}$代替$w_1,w_2,w_3$的$\text{3-gram}$ |
>
> ## $\textbf{2.3. }$神经网络语言模型简述
>
> > :one:结构$\&$流程
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241120203700965.png" alt="image-20241120203700965" width=500 />  
> >
> > 1. 词向量：通过$\text{one-hot}$编码/分布式表示等，得到$\{x^{(1)}, x^{(2)}, \text{...}, x^{(t)}\}$ 
> >    - 在基于窗口的神经网络中词向量通常为定长(窗口)，而$\text{RNN}$正是为了处理任意长度句子而生
> > 2. 词嵌入：$\{x^{(1)}, x^{(2)}, \text{...}, x^{(t)}\}\xrightarrow{\text{embedding}}\{e^{(1)}, e^{(2)},e^{(3)}, \text{...}\}$ 
> > 3. 隐藏层：获得$h\text{=}f(We\text{+}b_1)$
> > 4. 输出层：获得$\hat{y}\text{=Softmax}\left(U h\text{+}b_2\right) \text{ ∈ }\mathbb{R}^{|V|}$ 
> >
> > :two:相比于$\text{N-gram}$：没有稀疏性问题，无需存储所有观察到的$\text{N-gram}$ 