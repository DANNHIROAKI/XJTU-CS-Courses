# æ˜¯é¡¹ç›®çš„ä¸»ç¨‹åºï¼Œæ•´åˆäº†ä¹‹å‰å®šä¹‰çš„å„ä¸ªç»„ä»¶
# é€šè¿‡ Gradio æ¡†æ¶æ­å»ºäº†ä¸€ä¸ª Web ç•Œé¢ï¼Œè®©ç”¨æˆ·å¯ä»¥ä¸è‹æ ¼æ‹‰åº•å¼æé—®æ¨¡å‹è¿›è¡Œäº¤äº’


from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from LLM import ChatGLM4_LLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import gradio as gr


# åŠ è½½é—®ç­”é“¾
# load_chain å‡½æ•°åŠ è½½å‘é‡æ•°æ®åº“ï¼ˆChromaï¼‰å’Œè‡ªå®šä¹‰è¯­è¨€æ¨¡å‹ï¼ˆChatGLM4_LLMï¼‰ï¼Œå¹¶é…ç½®ä¸€ä¸ª Prompt æ¨¡æ¿ï¼Œç¡®ä¿ç³»ç»Ÿä»¥è‹æ ¼æ‹‰åº•å¼æé—®çš„æ–¹å¼å›åº”
# è¯¥å‡½æ•°ä½¿ç”¨ RetrievalQA.from_chain_type åˆ›å»ºæ£€ç´¢é—®ç­”é“¾ï¼Œå¹¶è¿”å› qa_chain å¯¹è±¡
def load_chain():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    persist_directory = 'data_base/vector_db/chroma'

    vectordb = Chroma(
        persist_directory=persist_directory,
        embedding_function=embeddings
    )

    llm = ChatGLM4_LLM(model_name_or_path = "/root/autodl-tmp/ZhipuAI/glm-4-9b-chat")
    # llm.predict("ä½ æ˜¯è°")

    template = """ä½ æ˜¯ä¸€ä¸ªä½¿ç”¨è‹æ ¼æ‹‰åº•å¼æ–¹æ³•æ•™å­¦çš„å¯¼å¸ˆã€‚ä½ çš„ç›®æ ‡æ˜¯é€šè¿‡æé—®å¼•å¯¼å­¦ç”Ÿæ€è€ƒï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œæœ€ç»ˆç›®çš„æ˜¯å¸®åŠ©ä»–ä»¬æ‰¾åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚
    è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
    1. å¦‚æœè¾“å…¥æ˜¯ä¸€ä¸ªæ–°é—®é¢˜ï¼ˆä¸åŒ…å«"æˆ‘è®¤ä¸º"ã€"æˆ‘çš„æƒ³æ³•"ç­‰è¡¨è¿°ï¼‰ï¼Œåº”è¯¥ï¼š
    - ç»å¯¹ä¸è¦ç›´æ¥ç»™å‡ºç­”æ¡ˆ
    - æä¾›1-2ä¸ªä¸é—®é¢˜æ ¸å¿ƒç›¸å…³çš„å¼•å¯¼æ€§é—®é¢˜ï¼Œå¸®åŠ©å­¦ç”Ÿä»ä¸åŒè§’åº¦æ€è€ƒè¿™ä¸ªé—®é¢˜
    - æå‡ºçš„æ¯ä¸ªé—®é¢˜éƒ½åº”è¯¥èšç„¦äºåŸå§‹é—®é¢˜çš„ä¸€ä¸ªå…·ä½“æ–¹é¢

    2. å¦‚æœè¾“å…¥æ˜¯å¯¹ä¹‹å‰é—®é¢˜çš„å›ç­”ï¼ˆé€šå¸¸åŒ…å«è¯¦ç»†è§£é‡Šæˆ–è§‚ç‚¹ï¼‰ï¼Œåº”è¯¥ï¼š
    - å¦‚æœå›ç­”ä¸å®Œæ•´æˆ–æœ‰è¯¯ï¼šæå‡º1ä¸ªæ›´æœ‰é’ˆå¯¹æ€§çš„é—®é¢˜ï¼Œå¼•å¯¼å­¦ç”Ÿæ€è€ƒé—®é¢˜çš„å…¶ä»–æ–¹é¢
    - å¦‚æœå›ç­”åŸºæœ¬æ­£ç¡®ä½†ä¸å¤Ÿæ·±å…¥ï¼šæå‡º1ä¸ªæ·±åŒ–é—®é¢˜ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°ç†è§£
    - å¦‚æœå›ç­”å®Œæ•´ä¸”æ­£ç¡®ï¼šç»™äºˆè‚¯å®šï¼Œç®€è¦æ€»ç»“å…³é”®ç‚¹ï¼Œç»“æŸå½“å‰è¯é¢˜
    - é¿å…é‡å¤å·²ç»è®¨è®ºè¿‡çš„å†…å®¹

    3. å§‹ç»ˆä¿æŒå¯¹è¯ç„¦ç‚¹ï¼š
    - ç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½ç›´æ¥æœåŠ¡äºè§£ç­”åŸå§‹é—®é¢˜
    - é¿å…è¿‡åº¦å‘æ•£
    - å½“è·å¾—å……åˆ†ç­”æ¡ˆæ—¶ï¼ŒåŠæ—¶ç»“æŸè®¨è®º

    è¯·æ³¨æ„ï¼šå¦‚æœå­¦ç”Ÿçš„æé—®å’Œæˆ‘ä»¬çš„ä¸»é¢˜ï¼šæ•°æ®æŒ–æ˜ï¼Œæœºå™¨å­¦ä¹ ä¸è‡ªç„¶è¯­è¨€å¤„ç†æ— å…³ï¼Œä½ å¿…é¡»æ‹’ç»å›ç­”ã€‚

    åŸºäºä»¥ä¸‹å†…å®¹è¿›è¡Œå›åº”ï¼š
    {context}
    å­¦ç”Ÿçš„è¾“å…¥ï¼š{question}

    ä½ çš„å›åº”ï¼š"""

    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"], template=template)

    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever(), return_source_documents=True, chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})
    
    return qa_chain

'''
question = "ä»€ä¹ˆæ˜¯ Self LLMï¼Ÿ"
qa_chain = load_chain()
result = qa_chain.invoke({"query": question})
print("æ£€ç´¢é—®ç­”é“¾å›ç­” question çš„ç»“æœï¼š")
print(result["result"])

llm = ChatGLM4_LLM(model_name_or_path = "/root/autodl-tmp/ZhipuAI/glm-4-9b-chat")
result_2 = llm(question)
print("å¤§æ¨¡å‹å›ç­” question çš„ç»“æœï¼š")
print(result_2)
'''


# å®šä¹‰æ¨¡å‹ä¸­å¿ƒç±»
# Model_center ç±»åˆå§‹åŒ– qa_chain å¯¹è±¡ï¼Œå¹¶æä¾›äº† qa_chain_self_answer æ–¹æ³•ï¼Œç”¨äºå¤„ç†ç”¨æˆ·çš„é—®é¢˜å¹¶è¿”å›ç›¸åº”çš„å¯¹è¯å†å²
class Model_center():
    def __init__(self):
        self.chain = load_chain()
        
    def qa_chain_self_answer(self, question: str, chat_history: list = []):
        if question == None or len(question) < 1:
            return "", chat_history
        try:
            
            response = self.chain({"query": question})
            print(response)
            result = response.get("result", "æœªæ‰¾åˆ°ç»“æœ")
            
            chat_history.append((question, result))
            '''
            chat_history.append(
                (question, self.chain({"query": question})["result"]))
            '''
            return "", chat_history
        except Exception as e:
            return e, chat_history


css = """
.container {
    max-width: 1600px;
    margin: auto;
    padding: 10px;
}

.title {
    text-align: center;
    color: #2c3e50;
    font-family: 'Arial', sans-serif;
    margin-bottom: 10px;
}

.title h1 {
    font-size: 2.2em;
    margin-bottom: 10px;
}

.title h2 {
    font-size: 1.5em;
    color: #7f8c8d;
}

.main-content {
    display: flex;
    justify-content: center;
    width: 100%;
    gap: 15px;
    padding: 0 20px;
}

.chatbot {
    border-radius: 12px;
    border: 1px solid #e0e0e0;
    background-color: #f9f9f9;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    min-width: 1000px;
}

.input-area {
    margin-top: 20px;
    width: 100%;
}

.input-box {
    border-radius: 8px;
    border: 1px solid #ddd;
    padding: 8px;
    font-size: 15px;
}

.button-row {
    display: flex;
    gap: 10px;
    justify-content: flex-start;
    margin-top: 10px;
}

.custom-button {
    background-color: #3498db;
    color: white;
    border: none;
    padding: 10px 20px;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.3s ease;
    min-width: 120px;
    font-size: 16px;
}

.custom-button:hover {
    background-color: #2980b9;
    transform: translateY(-2px);
    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
}

.tips {
    margin-top: 20px;
    padding: 10px;
    background-color: #f8f9fa;
    border-left: 6px solid #3498db;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    font-size: 16px;
    line-height: 1.6;
}

.tips h3 {
    color: #2c3e50;
    margin-bottom: 15px;
}

.tips ul {
    margin-left: 20px;
}
"""


# ä½¿ç”¨ gr.Blocks æ„å»ºæ•´ä¸ªé¡µé¢ï¼Œgr.Markdown ç”¨äºæ˜¾ç¤ºæ ‡é¢˜å’Œæç¤ºä¿¡æ¯
# gr.Chatbot æ˜¾ç¤ºå¯¹è¯å†…å®¹ï¼Œgr.Textbox æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œgr.Button å’Œ gr.ClearButton ç”¨äºå‘é€é—®é¢˜å’Œæ¸…ç©ºå¯¹è¯
# åœ¨ç‚¹å‡»â€œå‘é€â€æŒ‰é’®æ—¶è°ƒç”¨ qa_chain_self_answer æ–¹æ³•å¤„ç†è¾“å…¥é—®é¢˜
# ä½¿ç”¨ CSS å¯¹ç•Œé¢å¸ƒå±€
# demo.launch() å¯åŠ¨ Gradio Web ç•Œé¢ï¼Œæä¾›äº¤äº’å¼é—®ç­”åŠŸèƒ½
model_center = Model_center()
block = gr.Blocks(css=css)
with block as demo:
    with gr.Column(elem_classes="container"):
        gr.Markdown(
            """
            # ğŸ¤– è‹æ ¼æ‹‰åº•æé—®å™¨
            ## æ•°æ®ä»“åº“ä¸æ•°æ®æŒ–æ˜-XJTU
            """,
            elem_classes="title"
        )
        
        with gr.Row():
            with gr.Column(scale=8):
                chatbot = gr.Chatbot(
                    height=500,
                    show_copy_button=True,
                    container=True,
                    elem_classes="chatbot"
                )
                
                with gr.Row():
                    msg = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜...",
                        scale=8,
                        container=False,
                        elem_classes="input-box"
                    )
                    
                with gr.Row(elem_classes="button-row"):
                    db_wo_his_btn = gr.Button(
                        "å‘é€ ğŸ’¬",
                        variant="primary",
                        elem_classes="custom-button"
                    )
                    clear = gr.ClearButton(
                        components=[chatbot],
                        value="æ¸…ç©ºå¯¹è¯ ğŸ—‘ï¸",
                        variant="secondary",
                        elem_classes="custom-button"
                    )

        gr.Markdown(
            """
            ### ğŸ’¡ æ¸©é¦¨æç¤º
            - è¿™æ˜¯ä¸€ä¸ªé¢å‘æ•°æ®æŒ–æ˜ä¸“ä¸šè¯¾çš„æ™ºèƒ½æé—®ç³»ç»Ÿï¼Œæ”¯æŒæ•°æ®æŒ–æ˜ã€æœºå™¨å­¦ä¹ ä¸è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³é—®é¢˜ã€‚
            - é€šè¿‡è‹æ ¼æ‹‰åº•å¼æé—®æ–¹æ³•ï¼Œå¼•å¯¼æ‚¨æ·±å…¥æ€è€ƒé—®é¢˜ã€‚ç›®æ ‡æ˜¯å¸®åŠ©æ‚¨å‘ç°é—®é¢˜æœ¬è´¨ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚
            """,
            elem_classes="tips"
        )

    db_wo_his_btn.click(
        model_center.qa_chain_self_answer,
        inputs=[msg, chatbot],
        outputs=[msg, chatbot]
    )

demo.launch()
