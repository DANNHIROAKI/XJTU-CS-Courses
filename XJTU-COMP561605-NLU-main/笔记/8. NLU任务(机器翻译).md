# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119154414877.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/24950624a141b9941650b6187682aa9.jpg" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{1. }$概述与统计机器翻译

> :one:基于规则的机器翻译$\text{(RBMT)}$ 
>
> 1. 结构：规则库(词法$+$句法$+$语义)，词典(源-目标语言的对应关系)
> 2. 流程：分析$\xrightarrow{词法/句法/语义分析}$中间表示(如中间树)$\xrightarrow{目标转换}$生成
>
> :two:基于统计的机器翻译$\text{(SMT)}$ 
>
> 1. 含义：
>
>    - 目标：给定源语言句子$\text{→}$找到目标语言中最可能的翻译
>
>    - 原理：基于$P(\text{TL}|\text{OL})\text{=}P(\text{OL}|\text{TL})P(\text{TL})$，从而构建以下两个模型并最大化$P(\text{OL}|\text{TL})$
>
>      |     $\textbf{Item}$      | 含义           | 释义                                   |
>      | :----------------------: | -------------- | -------------------------------------- |
>      |      $P(\text{TL})$      | 语言模型的概率 | 表示目标语言句子在目标语言中的自然性   |
>      | $P(\text{OL}|\text{TL})$ | 翻译模型的概率 | 表示从目标语言句子生成源语言句子的概率 |
>
> 2. 几种$\text{SMT}$：
>
>    - $\text{Noisy Channel}$模型：认为源语言$\xleftarrow{噪声信道处理}$目标语言，而翻译为该过程逆向
>    - $\text{IBM}$模型：核心在于对齐机制，通过最大化对齐概率以找到源$\xleftrightarrow{}$目标的最佳对应

# $\textbf{2. RNN}$与$\textbf{LSTM}$ 

> ## $\textbf{2.0. }$一些前情提要
>
> > :one:关于隐状态
> >
> > 1. 含义：对当前$x_t\xrightarrow{认为基于过去的观测}x_t\textasciitilde{}P\left(x_t| x_{t\text{-}1},\ldots,x_1\right)\xrightarrow{将过去观测总结为h_{t\text{-}1}}x_t\textasciitilde{}P\left(x_t| h_{t\text{-}1}\right)$ 
> > 2. 更新：当前隐含状态就是当前值$+$上一隐藏状态，即$h_t\text{=}f(x_t,h_{t\text{-}1})$ 
> >
> > :two:无隐藏状态的神经网络
> >
> > 1. 输入层：给定一批的$n$个$d$维样本$\textbf{X}\text{∈}\mathbb{R}^{n\text{×}d}$ 
> > 2. 隐藏层：$\textbf{X}\text{∈}\mathbb{R}^{n\text{×}d}\xrightarrow[定激活函数\phi,\,h个隐藏单元]{权重参数\textbf{W}_{xh}\text{∈}\mathbb{R}^{d\text{×}h},\,偏置参数\textbf{b}_h\text{∈}\mathbb{R}^{1\text{×}h}}$输出$\textbf{H}\text{=}\phi(\textbf{XW}_{xh}\text{+}\textbf{b}_h)\text{∈}\mathbb{R}^{n\text{×}h}$ 
> > 3. 输出层：
> >    - 原始输出：$\textbf{H}\text{∈}\mathbb{R}^{n\text{×}h}\xrightarrow[q个隐藏单元]{权重参数\textbf{W}_{hq}\text{∈}\mathbb{R}^{h\text{×}q},\,偏置参数\textbf{b}_q\text{∈}\mathbb{R}^{1\text{×}q}}$输出$\textbf{O}\text{=}(\textbf{HW}_{hq}\text{+}\textbf{b}_q)\text{∈}\mathbb{R}^{n\text{×}q}$  
> >    - $\text{Softmax}$输出：$\text{Softmax}(\textbf{O})\mathbb{R}^{n\text{×}q}\text{→}$为概率分布(矩阵所有元素和为$1$)
>
> ## $\textbf{2.1. }$有隐藏状态的神经网络: $\textbf{RNN}$ 
>
> > :one:$\text{RNN}$结构​
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241111165158621.png" alt="image-20241111165158621" width=580 /> 
> >
> > 1. 输入层：输入序列在时间步$t$处的$n$个样本$\textbf{X}_t\text{∈}\mathbb{R}^{n\text{×}d}$ 
> > 2. 隐藏层：全连接$+$带激活函数$\phi$，输出$\textbf{H}_t\text{=}\phi(\textbf{X}_t\textbf{W}_{xh}\text{+}\textbf{H}_{t\text{-}1}\textbf{W}_{hh}\text{+}\textbf{b}_h)\text{∈}\mathbb{R}^{n\text{×}h}$ 
> >    - $\textbf{W}_{hh}\text{∈}\mathbb{R}^{h\text{×}h}$权重参数：描述了如何在当前时间步，使用上一时间步隐藏变量
> >    - 另一个关注点：$\textbf{X}_t\textbf{W}_{xh}\text{+}\textbf{H}_{t\text{-}1}\textbf{W}_{hh}$相当于$(\textbf{X}_t\xleftrightarrow{拼接}\textbf{H}_{t\text{-}1})\text{×}(\textbf{W}_{xh}\xleftrightarrow{拼接}\textbf{W}_{hh})$
> > 3. 输出层：输出$\textbf{O}_t\text{=}(\textbf{H}_t\textbf{W}_{hq}\text{+}\textbf{b}_q)\text{∈}\mathbb{R}^{n\text{×}q}$  
> > 4. 循环机制：
> >    - 参数上：$\textbf{W}_{xh}\text{∈}\mathbb{R}^{d\text{×}h}/\textbf{W}_{hh}\text{∈}\mathbb{R}^{h\text{×}h}/\textbf{W}_{hq}\text{∈}\mathbb{R}^{h\text{×}q}/\textbf{b}_q\text{∈}\mathbb{R}^{1\text{×}q}/\textbf{b}_h\text{∈}\mathbb{R}^{1\text{×}h}$每个时间步相同
> >    - 隐藏态：每个时间步隐藏态的使用方式一样，当前时间步$\textbf{H}_t$也会用于下一时间步$\textbf{H}_t$的计算
> >
> > :two:基于$\text{RNN}$的字符级语言模型
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241111173655351.png" alt="image-20241111173655351" width=520 />  
> >
> > 1. 结构：用当前字符预测下一个字符$\text{→}$批大小$\text{=}1$时当前时间步标签为下一字符(原始序列右移一位)
> > 2. 输出/误差：对每个时间步进行$\text{Softmax}$，并用交叉熵计算其与标签的误差
> > 3. 隐藏状态：隐藏状态$\textbf{O}_t$由$t$时间步及其之前的文本序列决定
> >
> > :three:$\text{RNN}$的通过时间反向传播($\text{BBTT}$) 
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241111183827115.png" alt="image-20241111183827115" width=520 /> 
> >
> > 1. 损失计算：每一时间步输出与标签的(交叉熵)损失$L_t \text{→}$累计每一时间步损失$\displaystyle{}L\text{=}\sum_{t=1}^T L_t$
> > 2. 计算梯度：将每一个时间步展开为一个前馈神经网络
> >    - 输出层：每时间步$\displaystyle{}\cfrac{\partial L}{\partial \mathbf{W}_{h q}}\text{=}\sum_{t\text{=}1}^T \cfrac{\partial L_t}{\partial \mathbf{O}_t} \cdot \cfrac{\partial \mathbf{O}_t}{\partial \mathbf{W}_{h q}}$以及$\displaystyle{}\cfrac{\partial L}{\partial \mathbf{b}_q}\text{=}\sum_{t\text{=}1}^T \cfrac{\partial L_t}{\partial \mathbf{O}_t} \cdot \cfrac{\partial \mathbf{O}_t}{\partial \mathbf{b}_q}$
> >    - 隐藏层：每时间步$\displaystyle{}\cfrac{\partial L}{\partial \mathbf{H}_t}\text{=}\sum_{k=t}^T \cfrac{\partial L_k}{\partial \mathbf{H}_t}\xrightarrow{沿时间步展开}\cfrac{\partial L_k}{\partial \mathbf{H}_t}\text{=}\cfrac{\partial L_k}{\partial \mathbf{O}_k} \cdot \cfrac{\partial \mathbf{O}_k}{\partial \mathbf{H}_k} \cdot \prod_{j=t+1}^k \cfrac{\partial \mathbf{H}_j}{\partial \mathbf{H}_{j-1}}$  
> > 3. 更新隐藏层参数：
> >    - 反向累积：$\displaystyle{}\cfrac{\partial L}{\partial \mathbf{W}_{h h}}\text{=}\sum_{t=1}^T \cfrac{\partial L}{\partial \mathbf{H}_t} \cdot \cfrac{\partial \mathbf{H}_t}{\partial \mathbf{W}_{h h}}$以及$\displaystyle{}\cfrac{\partial L}{\partial \mathbf{b}_h}\text{=}\sum_{t=1}^T \cfrac{\partial L}{\partial \mathbf{H}_t} \cdot \cfrac{\partial \mathbf{H}_t}{\partial \mathbf{b}_h}$在时间维度累积
> >    - 参数更新：累加所有时间步的梯度后，(使用$\text{SND/Adam}$)更新权重参数
> >
> > :warning:$\text{BBTT}$存在==梯度消失/爆炸==的问题，即梯度因反复相乘而剧烈减小/增大，而$\text{LSTM}$则有效解决之
>
> ## $\textbf{2.2. LSTM}$结构: 门控记忆元
>
> > :one:各单元结构：对于输入$\mathbf{X}_t \text{∈} \mathbb{R}^{n \text{×} d}$和隐状态$\mathbf{H}_{t\text{-}1}\text{∈}\mathbb{R}^{n \text{×} h}$
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片sREATYJDKFUL,MHNFGBDFF10.png" alt="image-20241111205414014" width=450 /> 
> >
> > |                         单元类型                          |                           计算方法                           |                        输入权重                         |                        隐藏权重                         |                        偏置                         |
> > | :-------------------------------------------------------: | :----------------------------------------------------------: | :-----------------------------------------------------: | :-----------------------------------------------------: | :-------------------------------------------------: |
> > |     ${}\mathbf{F}_t\text{∈}\mathbb{R}^{n \text{×} h}$     | ${}\sigma\left(\mathbf{X}_t \mathbf{W}_{x f}+\mathbf{H}_{t\text{-}1} \mathbf{W}_{h f}+\mathbf{b}_f\right)$ | ${}\mathbf{W}_{x f} \text{∈} \mathbb{R}^{d \text{×} h}$ | ${}\mathbf{W}_{h f} \text{∈} \mathbb{R}^{h \text{×} h}$ | ${}\mathbf{b}_f \text{∈} \mathbb{R}^{1 \text{×} h}$ |
> > |     ${}\mathbf{I}_t\text{∈}\mathbb{R}^{n \text{×} h}$     | ${}\sigma\left(\mathbf{X}_t \mathbf{W}_{x i}+\mathbf{H}_{t\text{-}1} \mathbf{W}_{h i}+\mathbf{b}_i\right)$ | ${}\mathbf{W}_{xi} \text{∈} \mathbb{R}^{d \text{×} h}$  | ${}\mathbf{W}_{h i} \text{∈} \mathbb{R}^{h \text{×} h}$ | ${}\mathbf{b}_i \text{∈} \mathbb{R}^{1 \text{×} h}$ |
> > |     ${}\mathbf{O}_t\text{∈}\mathbb{R}^{n \text{×} h}$     | ${}\sigma\left(\mathbf{X}_t \mathbf{W}_{x o}+\mathbf{H}_{t\text{-}1} \mathbf{W}_{h o}+\mathbf{b}_o\right)$ | ${}\mathbf{W}_{x o} \text{∈} \mathbb{R}^{d \text{×} h}$ | ${}\mathbf{W}_{h o} \text{∈} \mathbb{R}^{h \text{×} h}$ | ${}\mathbf{b}_o \text{∈} \mathbb{R}^{1 \text{×} h}$ |
> > | ${}\tilde{\mathbf{C}}_t\text{∈}\mathbb{R}^{n \text{×} h}$ | ${}\text{tanh}\left(\mathbf{X}_t \mathbf{W}_{x c}+\mathbf{H}_{t\text{-}1} \mathbf{W}_{h c}+\mathbf{b}_c\right)$ | ${}\mathbf{W}_{x c} \text{∈} \mathbb{R}^{d \text{×} h}$ | ${}\mathbf{W}_{h o} \text{∈} \mathbb{R}^{h \text{×} c}$ | ${}\mathbf{b}_c \text{∈} \mathbb{R}^{1 \text{×} h}$ |
> >
> > :two:记忆元：$\mathbf{C}_t\text{=}\mathbf{F}_t\text{⊙}\mathbf{C}_{t\text{-}1}\text{+}\mathbf{I}_t \text{⊙}\tilde{\mathbf{C}}_t$ 
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图四点十分ad片1.png" alt="图四点十分ad片1" width=560 />  
> >
> > |                     操作                      | 含义                                                         |
> > | :-------------------------------------------: | ------------------------------------------------------------ |
> > | $\mathbf{F}_t\text{⊙}\mathbf{C}_{t\text{-}1}$ | ==忘掉==多少过去的记忆元$\mathbf{C}_{t\text{-}1}\text{∈}\mathbb{R}^{n \text{×} h}$ |
> > |  $\mathbf{I}_t \text{⊙}\tilde{\mathbf{C}}_t$  | ==采纳==多少当前的新数据${}\tilde{\mathbf{C}}_t\text{∈}\mathbb{R}^{n \text{×} h}$ |
> >
> > - 当遗忘门$\mathbf{F}_t\text{=}\textbf{1}_{n \text{×} h}$且输入门$\mathbf{I}_t\text{=}\textbf{0}_{n \text{×} h}$时，表示过去的记忆元$\mathbf{C}_{t\text{-}1}$完全传递到当前步
> >
> > :three:隐状态：$\mathbf{H}_t\text{=}\mathbf{O}_t\text{⊙}\tanh \left(\mathbf{C}_t\right)\text{→}\mathbf{H}_t\text{∈}(-1,1)$，即只要输出门保持$1\text{→}$效传递记以更新隐状态
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图说的是得瑟得瑟谔谔片2.png" alt="图说的是得瑟得瑟谔谔片2" width=560 /> 
>
> ## $\textbf{2.3. Encoder-Decoder}$架构
>
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241111220328001.png" alt="image-20241111220328001" width=600 /> 
> >
> > |  结构  | 功能                                           | 举例                            |
> > | :----: | ---------------------------------------------- | ------------------------------- |
> > | 编码器 | 变长序列$\xrightarrow{编码}$定长向量(隐含状态) | 原始文本$\xrightarrow{编码}$🤪🤪🤪 |
> > | 解码器 | 定长向量$\xrightarrow{解码}$变长序列           | 🤪🤪🤪$\xrightarrow{解码}$翻译文本 |

# $\textbf{3. }$注意力机制与$\textbf{Transformer}$ 

> ## $\textbf{3.1. }$注意力机制
>
> > :one:生物学中的注意力提示
> >
> > |              类型              | 含义                                 |      基础      |
> > | :----------------------------: | ------------------------------------ | :------------: |
> > | 非自主提示($\text{Bottom-up}$) | 自然而然地注意到环境中显眼的物体     |  物体的突出性  |
> > | 自主性提示($\text{Top-down}$)  | 由自我意识主关推动的对于某事物的注意 | ==自主性提示== |
> >
> > :two:注意力机制的要素
> >
> > 1. $\text{Query/Key/Value}$架构
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112001829030.png" alt="image-20241111223857435" width=380 /> 
> >
> >    | $\textbf{Item}$ |   相当于   | 含义                                                         |
> >    | :-------------: | :--------: | ------------------------------------------------------------ |
> >    | $\text{Value}$  |  感官输入  | 实际输入的信息(感官)                                         |
> >    |  $\text{Key}$   | 非自主提示 | ==每个输入感官都有的==非自住性提示$(\text{Value}\xleftrightarrow{一一对应}\text{Key})$ |
> >    | $\text{Query}$  | 自主性提示 | 引导注意力机制聚焦于<span style="color:red;">最相关的感官输入</span> |
> >
> > 2. 注意力汇聚函数：$\displaystyle{}f(x)\text{=}\sum_{i=1}^n \alpha(x,x_i) \text{×} y_i$ 
> >
> >    |  汇聚示例  | 输出                                                         | 注意力评分函数                                     |
> >    | :--------: | ------------------------------------------------------------ | :------------------------------------------------- |
> >    |  平均汇聚  | $f(x)\text{=}\displaystyle{} \sum_{i=1}^n \cfrac{1}{n}y_i$   | $\text{N/A}$                                       |
> >    | 非参数汇聚 | $f(x)\text{=}\displaystyle{}\sum_{i=1}^n \text{Softmax}\left(-\cfrac{1}{2}\left(x-x_i\right)^2 \right) y_i$ | $-\cfrac{1}{2}\left(x-x_i\right)^2 $               |
> >    |  参数汇聚  | $f(x)\text{=}\displaystyle{}\sum_{i=1}^n \text{Softmax}\left(-\cfrac{1}{2}\left(\left(x-x_i\right) w\right)^2\right) y_i$ | $-\cfrac{1}{2}\left(\left(x-x_i\right) w\right)^2$ |
> >
> >    :thinking:$x/k_i$接近$\text{→}\text{Softmax}$(权)值变大$\text{→}v_i$对结果贡献变大(==获得更多注意力==)
> >
> > :three:注意力评分函数：对于$\mathbf{q} \text{∈} \mathbb{R}^q/\mathbf{k}_i \text{∈} \mathbb{R}^k/\mathbf{v}_i \text{∈} \mathbb{R}^v$有
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112012024127.png" alt="image-20241112005116348" width=400 /> 
> >
> > 1. 数学模型：
> >    - 评分函数：$a(\mathbf{q}, \mathbf{k}_i)\text{∈} \mathbb{R}$ 
> >    - 权值函数：$\displaystyle{}\alpha\left(\mathbf{q}, \mathbf{k}_i\right)\text{=}\operatorname{Softmax}\left(a\left(\mathbf{q}, \mathbf{k}_i\right)\right)\text{=}\cfrac{e^{a(\mathbf{q}, \mathbf{k}_i)}}{\displaystyle{}\sum_{j\text{=}1}^n e^{a(\mathbf{q}, \mathbf{k}_i)}} \text{∈} \mathbb{R}$，其中$\displaystyle{}\sum_{i=1}^{n}\alpha\left(\mathbf{q}, \mathbf{k}_i\right)\text{=}1$
> >    - 汇聚函数：$\displaystyle{}f\left(\mathbf{q},\left(\mathbf{k}_1, \mathbf{v}_1\right), \ldots,\left(\mathbf{k}_n, \mathbf{v}_n\right)\right)\text{=}\sum_{i\text{=}1}^n \alpha\left(\mathbf{q}, \mathbf{k}_i\right) \mathbf{v}_i \text{∈} \mathbb{R}^v$ 
> > 2. 关于$\text{Softmax}$：
> >    - 本质：本质上得到的是概率分布$\xrightarrow{与\textbf{v}_i相乘}\textbf{v}_i$的加权平均
> >    - 掩码：由于$\text{Softmax(-∞)=0}$，故可$\text{Softmax}(\textbf{A})\xrightarrow{\large\textbf{A}\xrightarrow[部分位设为\text{-∞}]{忽略不必要元素(如无用词元)}\textbf{A}^{\prime}}\text{Softmax}(\textbf{A}^{\prime})$ 
> > 3. 缩放点积：可高效计算的评分函数  
> >    - 单查询：对$\mathbf{q}\text{∈}\mathbb{R}^d$和$\mathbf{k}\text{∈}\mathbb{R}^d$(注意二者维度要相同)，有$a(\mathbf{q}, \mathbf{k})\text{=}\cfrac{\mathbf{q}^{\top} \mathbf{k}  }{\sqrt{d}}\text{∈} \mathbb{R}$   
> >    - 批查询：对$n$个查询$\mathbf{Q}\text{∈}\mathbb{R}^{n\text{×}d}$和$m$个键值对$\begin{cases}\mathbf{K}\text{∈}\mathbb{R}^{m\text{×}d}\\\\\mathbf{V}\text{∈}\mathbb{R}^{m\text{×}v}\end{cases}\text{→}$有$\text{Softmax}\left(\cfrac{\mathbf{Q K}^{\top}}{\sqrt{d}}\right) \mathbf{V} \text{∈} \mathbb{R}^{n \text{×} v}$      
>
> ## $\textbf{3.2. Transformer}$注意力: 编码$\xrightarrow{输入}$自注意力$\xrightarrow{组合}$多头
>
> > :one:位置编码
> >
> > 1. 流程：对含$n$个词元的$d$维嵌入$\textbf{X}\text{∈}\mathbb{R}^{n\text{×}d}\xrightarrow[位置嵌入矩阵]{\textbf{P}\text{∈}\mathbb{R}^{n\text{×}d}}\textbf{X}\text{+}\textbf{P}$，且$\begin{cases}p_{i, 2 j}\text{=}\sin \left(\cfrac{i}{10000^{2 j / d}}\right)\\\\p_{i, 2 j+1}\text{=}\cos \left(\cfrac{i}{10000^{2 j / d}}\right)\end{cases}$
> > 2. 意义：捕获绝对位置信息$\text{+}$学习相对位置信息
> >
> > :two:自注意力机制
> >
> > 1. 自注意力结构：$\textbf{QKV}$皆由词嵌入得到
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112154934891.png" alt="image-20241112154934891" width=430 /> 
> >
> >    - 词嵌入：输入序列$\textbf{X}\text{∈}\mathbb{R}^{n\text{×}d_x}\xrightarrow{(位置)嵌入}\textbf{A}\text{∈}\mathbb{R}^{n\text{×}d_a}$ 
> >    - 线性变换：$\begin{bmatrix}
> >      \textbf{Q}\text{∈}\mathbb{R}^{n\text{×}d_q}\\\textbf{K}\text{∈}\mathbb{R}^{n\text{×}d_k}\\\textbf{V}\text{∈}\mathbb{R}^{n\text{×}d_v}
> >      \end{bmatrix}\text{=}\begin{bmatrix}
> >      \textbf{A}\textbf{W}^{q}\\\textbf{A}\textbf{W}^{k}\\\textbf{A}\textbf{W}^{v}
> >      \end{bmatrix}其中\begin{cases}\textbf{W}^{q}\text{∈}\mathbb{R}^{d_a\text{×}d_q}\\\textbf{W}^{k}\text{∈}\mathbb{R}^{d_a\text{×}d_k}\\\textbf{W}^{v}\text{∈}\mathbb{R}^{d_a\text{×}d_v}\end{cases}\text{→}皆为可学习参数$ 
> >    - 计算注意力：注意力分布$\mathcal{A}\text{=}\text{Softmax}\left(\cfrac{\mathbf{Q K}^{\top}}{\sqrt{d_k}}\right)\text{∈} \mathbb{R}^{n \text{×} n}\xrightarrow[加权求和]{与\textbf{V}线性变换}$输出$\hat{\mathcal{A}}\text{=}\mathcal{A}\mathbf{V} \text{∈} \mathbb{R}^{n \text{×} d_v}$       
> >
> > 2. 比较：$\text{CNN/RNN/}$自注意力机制
> >
> >    |     模型     |                             结构                             | 备注                                               |
> >    | :----------: | :----------------------------------------------------------: | -------------------------------------------------- |
> >    | $\text{CNN}$ | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112161734677.png" alt="image-20241112161734677" width=200 /> | 一次只能感知到一个卷积核大小范围(此处为$\text{3}$) |
> >    | $\text{RNN}$ | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112161742174.png" alt="image-20241112161742174" width=200 /> | 对于输入序列是顺序重复(串行)处理，无法并行         |
> >    |   自注意力   | <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112161749492.png" alt="image-20241112161749492" width=200 /> | ==每个词可感知到其它任何词==$\text{→}$可并行处理   |
> >
> > :three:多头注意力机制
> >
> > 1. 结构：多个注意力头连结然后线性变换
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112031935327.png" alt="image-20241112031935327" width=500 /> 
> >
> >    - 头汇聚：输出$\xleftarrow{全连接层}\mathbf{W}_o\left[\begin{array}{c}
> >      \mathbf{h}_1 \\
> >      \vdots \\
> >      \mathbf{h}_h
> >      \end{array}\right]\xleftarrow[线性变换(连结)]{可学习参数\mathbf{W}_o\text{∈}\mathbb{R}^{p_o}}\mathbf{h}_i\text{=}f\left(\mathbf{W}_i^{(q)} \mathbf{q}, \mathbf{W}_i^{(k)} \mathbf{k}, \mathbf{W}_i^{(v)} \mathbf{v}\right)\text{∈}\mathbb{R}^{p_v}$ 
> >    - 注意头：$\mathbf{h}_i\text{=}f\left(\mathbf{W}_i^{(q)} \mathbf{q}, \mathbf{W}_i^{(k)} \mathbf{k}, \mathbf{W}_i^{(v)} \mathbf{v}\right)\text{∈}\mathbb{R}^{p_v}\xleftarrow{汇聚函数f}\begin{cases}
> >      \mathbf{W}_i^{(q)}\mathbf{q}\xleftarrow[全连接层]{可学习参数\mathbf{W}_i^{(q)}\text{∈}\mathbb{R}^{p_q\text{×}d_q}}\mathbf{q}\\\\
> >      \mathbf{W}_i^{(k)}\mathbf{k}\xleftarrow[全连接层]{可学习参数\mathbf{W}_i^{(k)}\text{∈}\mathbb{R}^{p_k\text{×}d_k}}\mathbf{k}\\\\
> >      \mathbf{W}_i^{(v)}\mathbf{v}\xleftarrow[全连接层]{可学习参数\mathbf{W}_i^{(v)}\text{∈}\mathbb{R}^{p_v\text{×}d_v}}\mathbf{v}
> >      \end{cases}$
> >
> > 2. 意义：每个头关注输入的不同部分$\text{→}$表示比简单加权平均值更复杂的函数
>
> ## $\textbf{3.3. }\textbf{Transformer}$架构 
>
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/transformer.svg" alt="transformer" width=450 />  
> >
> > :one:编码器与解码器
> >
> > 1. 编码器：
> >
> >    - 层结构：$n\text{×}$(多头自注意力层$\text{+}$前馈网络层)
> >
> >    - 残差连接：$\text{Multihead(}\textbf{X})/\text{Forward(}\textbf{X})\text{+}\textbf{X}\text{→}$输入$\text{Add-Norm}$，目的在于防止梯度消失
> >
> > 2. 解码器：
> >
> >    - 层结构：$n\text{×}$(多头自注意力层$\text{+}$编码解码注意力中间层$\text{+}$前馈网络层)
> >    - 中间层：$\textbf{Q}$来自解码层，$\textbf{KV}$来自编码层
> >    - 掩码：使得解码器只能考虑所在位置之前的位置，从而实现了自回归
> >
> > :two:其它结构
> >
> > 1. 基于位置的前馈网络：就是一个两层的$\text{MLP}$，将词元位置的隐藏表示$\xrightarrow{转化}$同维度新向量
> > 2. 加法和规范化组件：完成残差连接与层归一化(均值$\text{=0}$/方差$\text{=1}$)