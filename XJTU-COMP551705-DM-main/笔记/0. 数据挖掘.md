<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112204045174.png" alt="image-20241112204045174" width=600 />  

# $\textbf{1. }$导论与数据预处理

> ## $\textbf{1.1 }$导论: 大数据$\textbf{\&}$数据挖掘
>
> > :one:大数据
> >
> > 1. 含义：数据量巨大的数据，以至于合理时间内人类无法整理出可用信息
> > 2. 特性：$\text{Volume}$(规模大)$\text{+Variety}$(多样)$\text{+Velocity}$(数据产生/处理极快)$\text{+Veracity }$(真实但低质)
> >
> > :two:数据挖掘
> >
> > 1. 含义：从大数据中挖掘有价值的知识/规律
> > 2. 任务：分析(关联性/聚类)$\text{+}$预测(分类/回归)$\text{+}$关联规则等
> >
> > :three:其它
> >
> > 1. 大数据的应用：进人工智能(算力驱动/神经符号协同/记忆启发)$\text{+}$促进教育
> > 2. 面临的挑战：相关性$\text{≠}$因果，可解释性，群智涌现(群体智力远超个体)，隐私，可视化
>
> ## $\textbf{1.2. }$数据预处理
>
> > ### $\textbf{1.2.1. }$数据及其描述
> >
> > > :one:数据对象及其属性
> > >
> > > 1. 对象：数据集的组成单元，代表一个实体
> > >
> > > 2. 属性：对实体(对象)的描述
> > >
> > >    | 属性类型 | 含义                                 |   举例   | 描述               |
> > >    | :------: | :----------------------------------- | :------: | :----------------- |
> > >    |   二元   | 属性值域只有$\text{True/False}$      | 诊断结果 | $\text{N/A}$       |
> > >    |   枚举   | 属性值域由无序/不定量符号组成        | 职业类型 | 众数               |
> > >    |   序数   | 属性值间的序有意义，但前后序是定性的 | 军衔级别 | 众数/中位数        |
> > >    |   数值   | 可用整数或实数度量                   |   好多   | 众数/中位数/平均数 |
> > >
> > > :two:数据基本统计描述  
> > >
> > > 1. 传统的：算术/加权平均，中位数，众数(模)，极差，标准差/方差
> > > 2. 百分位：第$k$个百分位数$x_k$表示$k\%$的数据低于$x_k$，如$Q_1$/中位数/$Q_3$(即$25/50/75$百分位数)
> > >
> > > :three:数据基本图形描述
> > >
> > > 1. 传统的：直方图，分位数图，散点图
> > >
> > > 2. $\text{Box Plot}$ 
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241112232606017.png" alt="image-20241112232606017" width=450 /> 
> > >
> > >    - 四分位极差：$\text{IQR=}Q_3-Q_1$ 
> > >    - 孤立点($\text{Outlier}$)：在$Q_1-1.5\text{IQR}$之下或者$Q_1\text{+}1.5\text{IQR}$之上
> > >    - 盒图要素：上下端在${Q_1/Q_3}$上，中位数处划线，胡须延伸到最大最小观测值  
> > >
> > > :four:数据相关性描述：$\text{Pearson}$相关系数$\displaystyle{}r=\cfrac{\displaystyle{}\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sqrt{\displaystyle{}\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \sqrt{\displaystyle{}\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}}$ 
> >
> > ### $\textbf{1.2.2. }$数据预处理
> >
> > > :one:概述
> > >
> > > 1. 为何要预处理：数据不完整/有噪声/不一致(比如年龄可用汉字或数字表示)
> > > 2. 数据预处理任务：数据清理/集成/变换/归约(压缩)/离散化.....
> > >
> > > :two:数据清洗
> > >
> > > 1. 填补空缺值：人工补全，全局(千篇一律)补全，平均值补全，基于概率(如$\text{Bayesian}$)补全
> > > 2. 噪声处理：用自适应回归来平滑，通过聚类检测并去除孤立点，排序后分箱
> > >
> > > :three:数据集成和变换
> > >
> > > 1. <span style="color:red;">数据</span>/<span style="color:green;">模式</span>集成：
> > >    - 含义：将多个数据源中的<span style="color:red;">数据</span>/<span style="color:green;">元数据</span>合并到一个一致的存储
> > >    - 难题：解决数值/属性的冲突(如去掉强相关属性中的一个)，实体识别，检测并去除冗余数据
> > > 2. 数据变换：将数据统一成适合挖掘的形式  
> > >    - 归一化：将数据缩放到特定区间，如最值归一$v^{\prime}\text{=}\cfrac{v-\min}{\max{}-\min{}}\text{/Z-Score}$归一 $v^{\prime}\text{=}\cfrac{v-\mu}{\sqrt{\sigma}}$
> > >    - 属性构造：通过现有属性构造新的属性  
> > >    - 数据泛化：沿概念分层向上汇总  
> > >
> > > :four:数据规约
> > >
> > > 1. 含义：大大压缩数据的存储空间，但是保证数据分析的质量
> > > 2. 策略：堆规约(移除不重要元素/属性)，数据压缩(有损/无损)，数值规约(用较小的数据表示替代)

# $\textbf{2. }$数据挖掘的基本任务

> ## $\textbf{2.1. }$关联规则挖掘  
>
> > ### $\textbf{2.1.1. }$基本概念
> > >
> > >:one:关联规则挖掘
> > >
> > >1. 含义：从事务数据库中发现项集之间有趣的关联  
> > >
> > >2. 定义：对于$Y\text{⊆}I/X\text{⊆}T_i$且$X\text{∩}Y\text{=}\varnothing$，$X\text{→}Y$表示既然$T_i$中含$X$则也(以一定支持/置信度)含$Y$ 
> > >
> > >    |      集合       | 含义                                                         | 示例                                 |
> > >    | :-------------: | ------------------------------------------------------------ | ------------------------------------ |
> > >    |     项集$I$     | $I\text{=}\left\{i_1, i_2, \cdots, i_m\right\}$              | 超市中的所有商品                     |
> > >    |    事务集$D$    | $D\text{=}\left\{T_1, T_2, \cdots, T_n\right\}$且$T_i\text{⊆}I$ | 所有交易($T_i$为第$i$次交易结算商品) |
> > >    | 事务包含项集$X$ | $X\text{⊆}I$且$X\text{⊆}T_i$                                 | 每次交易所结算的(部分)商品           |
> > >
> > > 3. 表示：
> > >
> > >     - 数学表示：$A\text{→}B\,\,[S,C]$ (当$SC$都超过阈值时认为该规则强相关)
> > >
> > >     - 可视化图：
> > >
> > >       <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241116205405334.png" width=360 /> 
> > >
> > >:two:规则度量
> > >
> > > 1. 客观度量：
> > >
> > >    | $\textbf{Item}$ | 公式                                                         | 含义                                                         |
> > >    | :-------------: | :----------------------------------------------------------- | ------------------------------------------------------------ |
> > >    |     支持度      | <span style="color:red;">$S(A\text{→}B)\text{=}P(A\text{∩}B)$</span>, <span style="color:green;">$S(A)\text{=}P(A)$</span> | 所有事务中，有多少事务<span style="color:red;">同时含$AB$</span>/<span style="color:green;">含$A$</span> |
> > >    |     置信度      | $C(A\text{→}B)\text{=}\cfrac{P(A\text{∩}B)}{P(A)}$           | 在包含$A$的事务中，有多少同时还含$B$                         |
> > >
> > >    - 示例：$\text{ \large }
> > >      \begin{array}{|c|c|}
> > >      \hline
> > >      \text{\small TID} & \text{\small Item} \\[-5pt]
> > >      \hline
> > >      \small 2000 & \small A, B, C \\[-5pt]
> > >      \hline
> > >      \small 1000 & \small A, C \\[-5pt]
> > >      \hline
> > >      \small 4000 & \small A, D \\[-5pt]
> > >      \hline
> > >      \small 5000 & \small B, E, F \\[-5pt]
> > >      \hline
> > >      \end{array}
> > >      \text{ ⇒ }
> > >      \begin{cases}
> > >      P(A) = \cfrac{3}{4} \\[5pt]
> > >      P(A \cap B) = \cfrac{1}{4}
> > >      \end{cases}
> > >      \text{ ⇒ }
> > >      A \to B \, \left[\cfrac{1}{4}, \cfrac{1}{3}\right]$ 
> > >
> > > 2. 主观度量：个人常识/实际情况........ 
> > >
> > >:three:关联规则/相关性/因果关系
> > >
> > > 1. 关联规则/相关性
> > >
> > >    - 相关性衡量：$\text{Lift}(A,B)\text{=}\cfrac{P(A\text{∪}A)}{P(A)P(B)}\text{→}\begin{cases}\text{＜}1\text{→}AB负相关\\\\\text{=}1\text{→}AB无相关\\\\\text{>}1\text{→}AB正相关\end{cases}$ 
> > >
> > >    - 二者关系：强关联规则$\text{≠} $正相关
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241117015923744.png" alt="image-20241117015923744" width=600 />  
> > >
> > > 2. 关联规则/因果关系：二者无关，但相关性有助于为发现因果关系提供线索  
> >
> > ### $\textbf{2.1.2. }$布尔关联规则
> >
> > >
> > >#### $\textbf{2.1.2.1. }$一些基本概念
> > >
> > > > :one:频繁项集
> > > >
> > > > 1. 定义：$X$频繁$\xLeftrightarrow{等价于}$事务集$D$中含$X$的$T_i$数量($X$支持度)超过阈值$\xLeftrightarrow{等价于}X$满足最小支持度
> > > > 2. 性质：
> > > >    - 如果$X$频繁$\text{→}X$的子集也一定频繁(向下封闭)
> > > >    - 如果$X$非频繁$\text{→}X$的超集(如$\{X,x_{n+1},x_{n+2},...\}$)也一定非频繁
> > > >
> > > > :two:闭合集$\&$最大集：为解决组合爆炸($规则数目\text{ ∝ }2^{数据集规模}$)问题
> > > >
> > > > 1. 定义：对于事务集$D$  
> > > >
> > > >    |     集合     | 含义                                                         |      意义       |
> > > >    | :----------: | ------------------------------------------------------------ | :-------------: |
> > > >    | 闭合集(模式) | $X$闭合$\xLeftrightarrow{等价于}X$频繁$\text{∩}X$所有超集的支持度小于$X$的 | $D$==无损==压缩 |
> > > >    | 最大集(模式) | $X$最大$\xLeftrightarrow{等价于}X$频繁$\text{∩}X$所有超集都非频繁 | $D$==有损==压缩 |
> > > >
> > > > 2. 示例：$D\text{=}\left\{A_1\text{=}\left\langle a_1, a_2, \ldots, a_{100}\right\rangle,A_2\text{=}\left\langle a_1, a_2, \ldots, a_{50}\right\rangle\right\}$，最小支持度$\text{=1}$ 
> > > >
> > > >    |   集合   | $\textbf{Item}$                                              |
> > > >    | :------: | ------------------------------------------------------------ |
> > > >    | 闭合模式 | $A_1$(支持度$\text{=1}/$无频繁超集)，$A_2$(支持度$\text{=2/}$频繁超集支持度$\text{=2}$) |
> > > >    | 最大模式 | $A_1$(支持度$\text{=1}/$无频繁超集)                          |
> > > >    | 所有模式 | 所有的频繁子集，比如$\left\langle a_1, \ldots, a_{49}\right\rangle$ |
> > > >
> > >
> > >#### $\textbf{2.1.2.2.}$ $\textbf{Apriori}$算法
> > >
> > > > :zero:总论
> > > >
> > > > 1. 原有方案：原始数据$\xrightarrow{(暴力)生成}$关联规则
> > > > 2. 现有方案：原始数据$\xrightarrow{\text{Apriori}算法}$频繁项集$\xrightarrow{生成}$关联规则
> > > >
> > > > :one:算法流程：原始数据$\xrightarrow{\text{Apriori}算法}$频繁项集
> > > >
> > > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241116154336636.png" alt="image-20241116154336636" width=650 /> 
> > > >
> > > > 1. 初始化：事务$D\xrightarrow{清洗}$单项$\small\{\{\mathrm{T_1}\}, \{\mathrm{T_2}\},..., \{\mathrm{T_n}\}\}\xrightarrow{满足最小支持度}$ $L_1\text{=}\small\{\{\mathrm{T_{i_1}}\}, \{\mathrm{T_{i_2}}\},..., \{\mathrm{T_{i_m}}\}\}$ 
> > > > 2. 主循环：候选集$L_1\xrightarrow{执行以下操作}$候选集$L_2$ (下一轮循环)
> > > >    - 组合：本轮频繁项集$L_{1}\xrightarrow[(具体见例子)]{两两组合}$候选项集$C_2$ 
> > > >    - 剪枝：候选项集$C_2\xrightarrow{去处有非频繁子集的项}$下一候频繁集$L_2$，以进行下轮以此循环
> > > > 4. 输出：当循环到$L_\alpha$为空集时停止循环，频繁项集$L\text{=}\{L_1\text{∪}L_2\text{∪}...\text{∪}L_\alpha\}$
> > > >
> > > > :two:频繁项集$\xrightarrow{生成}$关联规则
> > > >
> > > > 1. 基本流程：
> > > >    - 子集：频繁项集$L$所有非空子集$S\text{=}\{S_1,S_2,...,S_{2^{|L|}-2}\}$，任意$S_i\text{→}S_j$组合满足支持度
> > > >    - 规则：对每个子集计算$S_i\text{→}L\textbackslash{}S_i$的置信度，若小于阈值则视$S_i\text{→}L\textbackslash{}S_i$强相关
> > > > 1. 效率优化：
> > > >    - 原理：对$(X\text{→}L\text{-}X)$置信度不满足阈值$\xrightarrow{X^{\prime}\text{⊆}X}(X^{\prime}\text{→}L\text{-}X^{\prime})$也不满足
> > > >    - 优化：先验证$L\text{-}X$只有单一项的规则，若不满足则剪枝/满足则再去验证$L\text{-}X$多项的规则
> > >
> > >#### $\textbf{2.1.2.2.}$ $\textbf{Apriori}$算法示例
> > >
> > > > :zero:基本条件：事务及其项集如下表，设定最小支持度为$2$ 
> > > >
> > > > | $\small\textbf{T}$ | $\small\textbf{Beer}$ | $\small\textbf{Diap.}$ | $\small\textbf{Powd}$ | $\small\textbf{Bread}$ | $\small\textbf{Umbre.}$ | $\small\textbf{Milk}$ | $\small\textbf{Deter.}$ | $\small\textbf{Cola}$ |
> > > > | :------------: | :-------------: | :---------------: | :---------------: | :--------------: | :-----------------: | :-----------------: | :-----------------: | :-----------------: |
> > > > |      $1$       |        ✅        |         ✅         |         ✅         |        ✅         |          ✅          | ❌ | ❌ | ❌ |
> > > > |      $2$       | ❌ |         ✅         |         ✅         | ❌ | ❌ | ❌ | ❌ | ❌ |
> > > > |      $3$       |        ✅        |         ✅         | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ |
> > > > |      $4$       | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ |
> > > > |      $5$       | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ | ✅ |
> > > >
> > > > :one:$\text{Apriori}$算法：得到频繁项集
> > > >
> > > > 1. 初始化：$L_1\xLeftarrow{出现超过两次的商品}\{\small\text{Beer,Diap,Powd,Milk}\}$ 
> > > >
> > > > 2. 主循环：当前为$L_1$
> > > >
> > > >    - 组合：$L_1 \xrightarrow{\text{简单两两组合}} C_2 \text{=} \left\{ 
> > > >      \small 
> > > >      {\begin{bmatrix}
> > > >      \text{Beer} \\ \text{Diap}
> > > >      \end{bmatrix}}, 
> > > >      \cancel{\begin{bmatrix}
> > > >      \text{Beer} \\ \text{Powd}
> > > >      \end{bmatrix}}, 
> > > >      \begin{bmatrix}
> > > >      \text{Beer} \\ \text{Milk}
> > > >      \end{bmatrix}, 
> > > >      \begin{bmatrix}
> > > >      \text{Diap} \\ \text{Powd}
> > > >      \end{bmatrix}, 
> > > >      \cancel{
> > > >      \begin{bmatrix}
> > > >      \text{Diap} \\ \text{Milk}
> > > >      \end{bmatrix}}, 
> > > >      \cancel{
> > > >      \begin{bmatrix}
> > > >      \text{Powd} \\ \text{Milk}
> > > >      \end{bmatrix}}
> > > >      \right\}$ 
> > > >    - 剪枝：$C_2\xrightarrow{剪掉包含非频繁子集的}L_2\text{=}\left\{  \small  \begin{bmatrix} \text{Beer} \\ \text{Diap} \end{bmatrix},  \begin{bmatrix} \text{Beer} \\ \text{Milk} \end{bmatrix},  \begin{bmatrix} \text{Diap} \\ \text{Powd} \end{bmatrix} \right\}$ 
> > > >
> > > > 3. 主循环：当前为$L_2$
> > > >
> > > >    - 组合：$L_2 \xrightarrow[组合逻辑见下表]{\text{两两组合}} C_3\text{=}\left\{  \small  \cancel{ \begin{bmatrix} \text{Beer} \\  \text{Diap} \\  \text{Milk} \\   \end{bmatrix}},  \cancel{ \begin{bmatrix} \text{Beer} \\ \text{Diap}   \\  \text{Powd} \end{bmatrix}}\right\}$ 
> > > >
> > > >      |    合并的集     | 条件: 二者有$k\text{-1}$项(此处为$1$)相等 |    操作    |
> > > >      | :-------------: | :---------------------------------------: | :--------: |
> > > >      | $L_2[1]/L_2[2]$ |       共有$\text{Beer}\text{→}$满足       |  执行组合  |
> > > >      | $L_2[1]/L_2[3]$ |       共有$\text{Diap}\text{→}$满足       |  执行组合  |
> > > >      | $L_2[2]/L_2[3]$ |                  不满足                   | 不执行组合 |
> > > >
> > > >    - 剪枝：$C_2\xrightarrow{剪掉包含非频繁子集的}L_3\text{=}\varnothing$，故终止循环
> > > >
> > > > 4. 输出：$L\text{=}L_1\text{∪}L_2\text{=}\left\{     \small    \text{Beer},    \text{Diap},    \text{Powd},    \text{Milk},    \begin{bmatrix}    \text{Beer} \\ \text{Diap}    \end{bmatrix},    \begin{bmatrix}    \text{Beer} \\ \text{Milk}    \end{bmatrix},    \begin{bmatrix}    \text{Diap} \\ \text{Powd}    \end{bmatrix}   \right\}$ 
> > > >
> > > > :two:生成规则$\text{→ }
> > > > \small
> > > > \begin{array}{|c|c|c|c|}
> > > > \hline
> > > > \text{Item} & \text{Support(A,B)} & \text{Support A} & \text{Confidence} \\[-5pt]
> > > > \hline
> > > > \text{Beer} \to \text{Diaper} & 60 \% & 80 \% & 75 \% \\[-5pt]
> > > > \hline
> > > > \text{Beer} \to \text{Milk} & 40 \% & 80 \% & 50 \% \\[-5pt]
> > > > \hline
> > > > \text{Diaper} \to \text{Powd} & 40 \% & 80 \% & 50 \% \\[-5pt]
> > > > \hline
> > > > \text{Diaper} \to \text{Beer} & 60 \% & 80 \% & 75 \% \\[-5pt]
> > > > \hline
> > > > \text{Milk} \to \text{Beer} & 40 \% & 40 \% & 100 \% \\[-5pt]
> > > > \hline
> > > > \text{Powd} \to \text{Diaper} & 40 \% & 40 \% & 100 \% \\[-5pt]
> > > > \hline
> > > > \end{array}
> > > > \text{ etc.....}$ 
> >
> > ### $\textbf{2.1.3. }$多层关联规则挖掘
> >
> > > #### $\textbf{2.1.3.1. }$算法概述
> > >
> > > > :one:基本概念
> > > >
> > > > 1. 概念分层：按层次组织的数据(一般概念$\xrightarrow{\text{高层→低层}}$具体概念)，正符合事务数据库的存储
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241116225058693.png" alt="image-20241116225058693" width=350 /> 
> > > >
> > > > 2. 规则示例：高层规则 (计算机$\text{→}$电器)，低层规则 (台式机$\text{→}$打印机)
> > > >
> > > > :two:每层最小支持度的设定
> > > >
> > > > | 方法 | 描述                                               | 特点                         |
> > > > | :--: | -------------------------------------------------- | ---------------------------- |
> > > > | 一致 | 所有曾的最小支持度一致                             | 忽略底层(地层支持度一般较低) |
> > > > | 递减 | 层次越低$\text{→}$最小支持度越低                   | 保留底层罕见但有意义的规则   |
> > > > | 分组 | 专家指重点项集$\text{→}$由此设置每层不同最小支持度 | $\text{N/A}$                 |
> > > >
> > > > :three:分层挖掘：
> > > >
> > > > 1. 含义：在多个抽象层挖掘关联规则$\text{→}$在不同的抽象层进行转化
> > > > 2. 性质：(支持度的传播)父节点支持度$\text{≥}$其子节点支持度，且根结点的支持度为$100\%$
> > > > 3. 策略：(自顶向下)==具体步骤看例子==
> > > >    - 最高层：用$\text{Apriori}$寻找高层频繁项集$+$过滤事务集
> > > >    - 向下迭代：传递过滤后事务集到下一层，继续用$\text{Apriori}$寻找高层频繁项集$+$过滤事务集
> > > >    - 终止：迭代到层次(子节点)时停下
> > >
> > > #### $\textbf{2.1.3.2. }$算法实例
> > >
> > > > :zero:基本情况：
> > > >
> > > > 1. 概念分层：
> > > >
> > > >    |     分层     |               表示                | 最大支持度 |
> > > >    | :----------: | :-------------------------------: | :--------: |
> > > >    | 第一层(顶层) | $\text{\{1**, 2**, 3**, 4**\}}$等 |    $4$     |
> > > >    | 第二层(中层) | $\text{\{11*, 12*, 21*, 22*\}}$等 |    $3$     |
> > > >    | 第三层(低层) | $\text{\{111, 121, 211, 713\}}$等 |    $3$     |
> > > >
> > > > 2. 事务集：第$i$位表示在第$i$层的分类，如$524$表示在高/中/低层分属第$5$/第$2$/第$4$类
> > > >
> > > >    | $\textbf{TID}$ | 底层                             | 中层                             | 顶层                            |
> > > >    | :------------: | :------------------------------- | -------------------------------- | ------------------------------- |
> > > >    |  $\text{T1}$   | $\{\text{111,121,211,221}\}$     | $\{\text{11*,12*,21*,22*}\}$     | $\{\text{1**, 2**}\}$           |
> > > >    |  $\text{T2}$   | $\{\text{111,211,222,323}\}$     | $\{\text{11*,21*,22*,32*}\}$     | $\{\text{1**, 2**, 3**}\}$      |
> > > >    |  $\text{T3}$   | $\{\text{112,122,221,411}\}$     | $\{\text{11*,12*,22*,41*}\}$     | $\{\text{1**, 2**, 4**}\}$      |
> > > >    |  $\text{T4}$   | $\{\text{111,121}\}$             | $\{\text{11*,12*}\}$             | $\{\text{1**}\}$                |
> > > >    |  $\text{T5}$   | $\{\text{111,122,211,221,413}\}$ | $\{\text{11*,12*,21*,22*,41*}\}$ | $\{\text{1**, 2**, 4**}\}$      |
> > > >    |  $\text{T6}$   | $\{\text{211,323,524}\}$         | $\{\text{21*,32*,52*}\}$         | $\{\text{2**, 3**, 5**}\}$      |
> > > >    |  $\text{T7}$   | $\{\text{323,411,524,713}\}$     | $\{\text{32*,41*,52*,71*}\}$     | $\{\text{3**, 4**, 5**, 7**}\}$ |
> > > >
> > > > :one:顶层的$\text{Apriori}$算法：<span style="color:red;">最小支持度为$4$</span>
> > > >
> > > > 1. 初始化：
> > > >
> > > >    - 初始频繁集：$L(1,1)\text{=}\{\text{\{1**\},\{2**\}}\}\xLeftarrow{得到}$ 筛选出$\{i\text{**}\}$中支持度大于$5$的
> > > >
> > > >    - 表过滤：$L(1,1)$中项的子节点必定以$1/2$开头，故滤掉以其它开头的
> > > >
> > > >      | $\textbf{TID}$ | 当前事务表$\textbf{T[1]}$        | 滤后事务表$\textbf{T[2]}$    |
> > > >      | :------------: | :------------------------------- | ---------------------------- |
> > > >      |  $\text{T1}$   | $\{\text{111,121,211,221}\}$     | $\{\text{111,121,211,221}\}$ |
> > > >      |  $\text{T2}$   | $\{\text{111,211,222,323}\}$     | $\{\text{111,211,222}\}$     |
> > > >      |  $\text{T3}$   | $\{\text{112,122,221,411}\}$     | $\{\text{112,122,221}\}$     |
> > > >      |  $\text{T4}$   | $\{\text{111,121}\}$             | $\{\text{111,121}\}$         |
> > > >      |  $\text{T5}$   | $\{\text{111,122,211,221,413}\}$ | $\{\text{111,122,211,221}\}$ |
> > > >      |  $\text{T6}$   | $\{\text{211,323,524}\}$         | $\{\text{211}\}$             |
> > > >      |  $\text{T7}$   | $\{\text{323,411,524,713}\}$     | $\{\varnothing\}$            |
> > > >
> > > > 2. 主循环：当前为$L(1,1)$
> > > >
> > > >    - 组合：$L(1,1) \xrightarrow{\text{简单两两组合}} C(1,2) \text{=}\{\text{\{1**,2**}\}\}$
> > > >    - 剪枝：$C(1,2)\xrightarrow[滤无可滤]{依据\text{T[2]}减去包含非频繁子集的}L(1,2) \text{=}\{\text{\{1**,2**}\}\}$ (不可再组合故循环结束) 
> > > >
> > > > 3. 输出：第一层频繁集$L(1)\text{=}\{\text{\{1**\},\{2**\}},\text{\{1**,2**}\}\}$；将滤后事务表$T[2]$传给下一层
> > > >
> > > > :two:中层的$\text{Apriori}$算<span style="color:red;">最小支持度为$3$</span>法：
> > > >
> > > > 1. 初始化：
> > > >
> > > >    - 初始频繁集：$L(2,1)\text{=}\{\text{\{11*\},\{12*\},\{21*\},\{22*\}}\}\xLeftarrow{得到}$ 筛出$\{ij\text{*}\}$中支持度大于$3$的
> > > >
> > > >    - 事务表过滤：$L(2,1)$中项的子节点必定以$11/12/21/22$开头，滤掉以其他开头的
> > > >
> > > >      | $\textbf{TID}$ | 当前事务表$\textbf{T[2]}$    | 滤后事务表$\textbf{T[3]}$    |
> > > >      | :------------: | :--------------------------- | ---------------------------- |
> > > >      |  $\text{T1}$   | $\{\text{111,121,211,221}\}$ | $\{\text{111,121,211,221}\}$ |
> > > >      |  $\text{T2}$   | $\{\text{111,211,222}\}$     | $\{\text{111,211,222}\}$     |
> > > >      |  $\text{T3}$   | $\{\text{112,122,221}\}$     | $\{\text{112,122,221}\}$     |
> > > >      |  $\text{T4}$   | $\{\text{111,121}\}$         | $\{\text{111,121}\}$         |
> > > >      |  $\text{T5}$   | $\{\text{111,122,211,221}\}$ | $\{\text{111,122,211,221}\}$ |
> > > >      |  $\text{T6}$   | $\{\text{211}\}$             | $\{\text{211}\}$             |
> > > >      |  $\text{T7}$   | $\{\varnothing\}$            | $\{\varnothing\}$            |
> > > >
> > > > 2. 主循环：当前为$L(2,1)$
> > > >
> > > >    - 组合：$L(2,1) \xrightarrow[]{\text{两两组合}} C(2,2) \text{=}\{\text{\{11*,12*}\},\{\text{11*,21*}\}.....\}$ 
> > > >    - 剪枝：$C(2,2) \xrightarrow[]{\text{依据 T[3] 减去包含非频繁子集的}} L(2,2) \text{=}  \left\{ \begin{array}{l} \{\text{11*, 12*}\} \\ \{\text{11*, 21*}\} \\ \{\text{11*, 22*}\} \\ \{\text{12*, 22*}\} \\ \{\text{21*, 22*}\} \end{array} \right\}$
> > > >
> > > > 3. 主循环：当前为$L(2,2)$ 
> > > >
> > > >    - 组合：$L(2,2) \xrightarrow[]{\text{两两组合}} C(2,3) \text{=}\{\text{\{11*,12*,21*}\},\{11*,12*,22*\}.....\}$ 
> > > >    - 剪枝：$C(2,3) \xrightarrow[]{\text{依据 T[3] 减去包含非频繁子集的}} L(2,3) \text{=}  \left\{ \begin{array}{l} \{\text{11*,12*,22*}\} , \{\text{11*,21*,22*}\} \end{array} \right\}$
> > > >
> > > > 4. 主循环：当前为$L(2,3)$ 
> > > >
> > > >    - 组合：$L(2,3) \xrightarrow[]{\text{两两组合}} C(2,4) \text{=}\{\text{\{11*,12*,21*,22*}\}\}$ 
> > > >    - 剪枝：$C(2,4) \xrightarrow[]{\text{依据 T[3] 减去包含非频繁子集的}} L(2,4) \text{=}\varnothing\text{ → }$停止循环
> > > >
> > > > 5. 输出：第二层频繁集$L(2)\text{=}\{L(2,1)\text{∪}L(2,2)\text{∪}L(2,3)\}$；将滤后事务表$\text{T[3]}$传到下一层
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241117005904932.png" alt="image-20241117005904932" width=550 /> 
> > > >
> > > > :three:底层的$\text{Apriori}$算<span style="color:red;">最小支持度为$3$</span>法：
> > > >
> > > > 1. 初始化：
> > > >
> > > >    - 初始频繁集：$L(3,1)\text{=}\{\text{\{111\},\{211\},\{221\}}\}\xLeftarrow{得到}$ 筛选出$\{ijk\text{}\}$中支持度大于$3$的
> > > >
> > > >    - 事务表过滤：接下来只考虑$\{\text{\{111\},\{211\},\{221\}}\}$中的，故滤掉其他的
> > > >
> > > >      | $\textbf{TID}$ | 当前事务表$\textbf{T[3]}$    | 滤后事务表$\textbf{T[4]}$ |
> > > >      | :------------: | :--------------------------- | ------------------------- |
> > > >      |  $\text{T1}$   | $\{\text{111,121,211,221}\}$ | $\{\text{111,,211,221}\}$ |
> > > >      |  $\text{T2}$   | $\{\text{111,211,222}\}$     | $\{\text{111,211}\}$      |
> > > >      |  $\text{T3}$   | $\{\text{112,122,221}\}$     | $\{\text{221}\}$          |
> > > >      |  $\text{T4}$   | $\{\text{111,121}\}$         | $\{\text{111}\}$          |
> > > >      |  $\text{T5}$   | $\{\text{111,122,211,221}\}$ | $\{\text{111,211,221}\}$  |
> > > >      |  $\text{T6}$   | $\{\text{211}\}$             | $\{\text{211}\}$          |
> > > >      |  $\text{T7}$   | $\{\varnothing\}$            | $\{\varnothing\}$         |
> > > >
> > > > 2. 主循环：当前为$L(3,1)$ 
> > > >
> > > >    - 组合：$L(3,1) \xrightarrow[]{\text{两两组合}} C(3,2) \text{=}\{\text{\{111,211}\},\text{\{111,221}\},\text{\{211,221}\}\}$ 
> > > >    - 剪枝：$C(3,2) \xrightarrow[]{\text{依据 T[4] 减去包含非频繁子集的}} L(3,2) \text{=}\text{\{111,221}\}\text{ → }$无法再组合(停止循环)
> > > >
> > > > 3. 输出：第一层频繁集$L(1)\text{=}\{\text{\{111\},\{211\},\{221\}},\text{\{111,221}\}\}$ 
> >
> > ### $\textbf{2.1.4. }$多维关联规则挖掘
> >
> > > :one:多维关联规则
> > >
> > > 1. 含义：涉及两个或多个维或谓词的关联规则  
> > >
> > > 2. 示例：`<Age: 30..39> and <Married: Yes> -> <NumCars: 2>  `
> > >
> > >    $\small
> > >    \begin{array}{|c|c|c|c|}
> > >    \hline
> > >    \text{RecordID} & \text{Age} & \text{Married} & \text{NumCars} \\[-5pt]
> > >    \hline
> > >    100 & 23 & \text{No} & 1 \\[-5pt]
> > >    \hline
> > >    200 & 25 & \text{Yes} & 1 \\[-5pt]
> > >    \hline
> > >    300 & 29 & \text{No} & 0 \\[-5pt]
> > >    \hline
> > >    400 & 34 & \text{Yes} & 2 \\[-5pt]
> > >    \hline
> > >    500 & 38 & \text{Yes} & 2 \\[-5pt]
> > >    \hline
> > >    \end{array}$   
> > >
> > > :two:处理思路
> > >
> > > 1. 处理对象：搜索频繁项集$\xrightarrow{转为}$搜索频繁谓词集
> > >
> > > 2. 量化预处理：数值型关联规则挖掘$\xrightarrow{转化}$布尔型关联规则挖掘，如下例子
> > >
> > >    $\small
> > >    \begin{array}{|c|c|c|c|}
> > >    \hline
> > >    \text{RecordID} & \text{Age} & \text{Married} & \text{NumCars} \\[-5pt]
> > >    \hline
> > >    100 & 23 & \text{No} & 1 \\[-5pt]
> > >    \hline
> > >    200 & 25 & \text{Yes} & 1 \\[-5pt]
> > >    \hline
> > >    300 & 29 & \text{No} & 0 \\[-5pt]
> > >    \hline
> > >    400 & 34 & \text{Yes} & 2 \\[-5pt]
> > >    \hline
> > >    500 & 38 & \text{Yes} & 2 \\[-5pt]
> > >    \hline
> > >    600 & 45 & \text{No} & 3 \\[-5pt]
> > >    \hline
> > >    700 & 50 & \text{Yes} & 1 \\[-5pt]
> > >    \hline
> > >    800 & 60 & \text{No} & 0 \\[-5pt]
> > >    \hline
> > >    \end{array}$
> > >
> > >  :three:处理算法：$\text{ARCS}$大致流程 
> > >
> > > 1. 分箱：根据数据的分布，将数值属性离散化到箱(固定区间)
> > >
> > > 2. 搜索：找出频繁谓词集，得到一系列强关联规则(如下)
> > >    - $\text{RuleSet =}\begin{cases}
> > >      \text{age}(X, 35) \text{∧} \text{income}(X, 31K \ldots 40K) \text{⇒} \text{buys}(X, \text{TV}) \\\\
> > >      \text{age}(X, 34) \text{∧} \text{income}(X, 41K \ldots 50K) \text{⇒} \text{buys}(X, \text{TV}) \\\\
> > >      \text{age}(X, 35) \text{∧} \text{income}(X, 41K \ldots 50K) \text{⇒} \text{buys}(X, \text{TV})
> > >      \end{cases}$
> > >
> > > 3. 聚类：将所得强关联规则映射到$2D$栅格上，得到范围矩形
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241117015014348.png" alt="image-20241117015014348" width=340 /> 
> > >
> > >    - $\text{RuleSet}\text{→}\text{age}(X,34\text{-}35) \text{∧} \text{income}(X, 31K\text{-}40K) \text{⇒} \text{buys}(X, \text{TV})$ 
>
> ## $\textbf{2.2. }$序列模式分析: 事件在顺序上的相关性  
>
> > ### $\textbf{2.2.1. }$ 序列模式的概念
> >
> > > :one:序列结构：以$3\text{-}$序列$\langle\{i_1, i_2\}\{i_1\}\rangle$为例
> > >
> > > | $\textbf{Item}$ | 含义                          | 示例                                | 备注/补充                                |
> > > | :-------------: | ----------------------------- | ----------------------------------- | ---------------------------------------- |
> > > |      项目       | 序列的最基本单位              | $i_1$与$i_2$                        | $\text{N/A}$                             |
> > > |      元素       | 多个项目的集(内部无序/不重复) | $\{i_1\}$和$\{i_1, i_2\}$           | $\{i_1, i_2\}$和$\{i_2, i_1\}$是相同元素 |
> > > |      序列       | 多个序列元素组合按顺序的排列  | $\langle\{i_1, i_2\}\{i_1\}\rangle$ | 序列长度$\text{=}$序列包含的项目数       |
> > >
> > > :two:子序列
> > >
> > > 1. 形式化表示：$\begin{cases}\alpha{\text{=}\langle}a_1,a_2,\text{...},a_n\rangle\\\\\beta{\text{=}\langle}b_1,b_2,\text{...},b_n\rangle\end{cases}\xrightarrow[1\text{≤}j_1\text{<}j_2\text{<}\cdots{}\text{<}j_n\text{≤}m]{a_1\text{⊆}b_{j_1}/a_2\text{⊆}b_{j_2}/.../a_n\text{⊆}b_{j_n}}\begin{cases}\alpha{}为\beta子集\\\\\beta为\alpha{}超集\end{cases}\text{ → }\alpha{}\text{⊆}\beta$ 
> > >
> > > 2. 示例：序列中每个元素只抽一次，一次抽$0$个项目$\text{→}$所有项目；一个元素抽两次必不是子序列
> > >
> > >    |                     $\textbf{Sequence}$                      |           $\textbf{Subsequence}$            | 子集 | 备注                                                       |
> > >    | :----------------------------------------------------------: | :-----------------------------------------: | :--: | ---------------------------------------------------------- |
> > >    | $\langle\{\text{2,4}\}\{\text{3,5}, \text{6}\}\{\text{8}\}\rangle$ | $\langle\{\text{2}\}\{\text{3, 5}\}\rangle$ |  ✅   | $\text{N/A}$                                               |
> > >    | $\langle\{\text{1}, \text{2}\}\{\text{3}, \text{4}\}\rangle$ |  $\langle\{\text{1}\}\{\text{2}\}\rangle$   |  ❌   | $\{\text{1}, \text{2}\}$抽两次，两次分别为$\{1\}$和$\{2\}$ |
> > >    | $\langle\{\text{2}, \text{4}\}\{\text{2}, \text{4}\}\{\text{2}, \text{5}\}\rangle$ |  $\langle\{\text{2}\}\{\text{4}\}\rangle$   |  ✅   | $\text{N/A}$                                               |
> > >
> > > :three:序列模式的挖掘
> > >
> > > 1. 一些概念：
> > >    - 序列数据库：顾名思义，例如$\text{ → }
> > >      \begin{array}{|c|c|}
> > >      \hline
> > >      \text{SID} & \text{Sequence} \\[-5pt]
> > >      \hline
> > >      10 & \langle \mathrm{a} (\mathrm{abc}) (\mathrm{ac}) \mathrm{d} (\mathrm{cf}) \rangle \\[-5pt]
> > >      \hline
> > >      20 & \langle (\mathrm{ad}) \mathrm{c} (\mathrm{bc}) (\mathrm{ae}) \rangle \\[-5pt]
> > >      \hline
> > >      30 & \langle (\mathrm{ef}) (\mathrm{ab}) (\mathrm{df}) \mathrm{cb} \rangle \\[-5pt]
> > >      \hline
> > >      \end{array}$ 
> > >    - 支持度：$\text{Support}(\alpha)\text{=}$数据库$S$中包含序列$\alpha$的个数
> > >    - 序列模式：序列$\alpha$被称作$\alpha$序列模式$\xLeftrightarrow{等价于}\text{Support}(\alpha)\text{≥}\xi$(阈值)，长度为$I$的序称为$I\text{-}$模式
> > > 2. 模式挖掘：给定一个序列集$\text{→}$找出其所有频繁子序列(所有支持度大于阈值的子序列)
> >
> > ### $\textbf{2.2.2. GSP}$算法: 基于类$\textbf{Apriori}$方法
> >
> > > :one:算法流程：$L_1 \text{⇒} C_2 \text{⇒} L_2 \text{⇒} C_3 \text{⇒} L_3 \text{⇒} C_4 \text{⇒} L_4 \text{⇒} \ldots $
> > >
> > > 1. 初始化：扫描序列数据库$\text{→}$得到常为$1$的序列模式$L_1\text{→}$作为初始种子集
> > > 2. 主循环：当前种子集$L_{i}\xrightarrow{执行以下操作}$下轮种子集$L_{i+1}$(下轮循环)
> > >    - 生成：当前种子集$L_i$(长为$i$)$\xrightarrow{L_i自己两两连接+剪切}$候选序列模式$C_{i+1}$(长为$i\text{+}1$)
> > >    - 选择：序列模式$C_{i+1}$(长为$i\text{+}1$)$\xrightarrow[扫描序列数据库]{筛掉候选序列模式的支持数小于阈值的}$下轮种子集$L_{i\text{+}1}$(长为${i\text{+}1}$)
> > > 3. 终止：当循环到$L_\alpha$或者$C_{\beta}$为空时终止，输出频繁序列模式集$L=\displaystyle{}\bigcup_{i=1}^k L$ 
> > >
> > > :two:算法细节：连接$\&$剪切
> > >
> > > 1. 连接阶段：
> > >    - 操作：(去掉序列模式$s_1$第一项)$\xLeftrightarrow{相同序列}$(去掉序列模式$s_2$末尾项)，则$s_1\xLeftrightarrow{连接}s_2$ 
> > >    - 注意：当首/尾为多项(无序)元素时$\text{→}$为首/尾为元素种任意一项，如$\langle a(bc)\rangle$的首尾为$ab/ac$ 
> > > 2. 剪切阶段：
> > >    - 操作：对连接后的序列，去除其中不是序列模式的序列
> > >    - 注意：($\text{Apriori}$性质)$S$子集非频繁$\text{⇒}$$S$非频繁，例如$\langle hb\rangle$非频繁$\text{⇒}\langle hsb\rangle /\langle \{ah\}b\rangle$非频繁
> > >
> > > :three:算法示例：令最小支持度$\text{=}2$
> > >
> > > 0. 数据预处理：事务数据库$\text{→}$序列数据库
> > >
> > >    $
> > >    \small\begin{array}{|c|c|c|}
> > >    \hline
> > >    \text{TransDate} & \text{CustID} & \text{Item} \\[-10pt]
> > >    \hline
> > >    1 & 01 & \mathrm{A} \\[-10pt]
> > >    \hline
> > >    1 & 02 & \mathrm{B} \\[-10pt]
> > >    \hline
> > >    1 & 03 & \mathrm{B} \\[-10pt]
> > >    \hline
> > >    2 & 04 & \mathrm{F} \\[-10pt]
> > >    \hline
> > >    3 & 01 & \mathrm{B} \\[-10pt]
> > >    \hline
> > >    3 & 05 & \mathrm{A} \\[-10pt]
> > >    \hline
> > >    4 & 02 & \mathrm{G} \\[-10pt]
> > >    \hline
> > >    ... & ... & ... \\[-10pt]
> > >    \hline
> > >    \end{array}
> > >    \longrightarrow{}
> > >    \begin{array}{|c|c|}
> > >    \hline
> > >    \text{CustSeqID} & \text{Sequence} \\[-10pt]
> > >    \hline
> > >    01 & \langle\mathrm{AB} \mathrm{(FG)}(\mathrm{CD}\rangle \\[-10pt]
> > >    \hline
> > >    02 & \langle\mathrm{BGD}\rangle \\[-10pt]
> > >    \hline
> > >    03 & \langle\mathrm{BFG(AB)}\rangle \\[-10pt]
> > >    \hline
> > >    04 & \langle\text{A(AB)CD}\rangle \\[-10pt]
> > >    \hline
> > >    05 & \text{\langle A(BC)GF(DE)}\rangle \\[-10pt]
> > >    \hline
> > >    \end{array}$ 
> > >
> > > 1. 初始化：找出支持度$\text{>2}$的单项集，种子集$\small 
> > >    L_1 \text{ ← }
> > >    \begin{array}{|c|c|}
> > >    \hline
> > >    \text{Item} & \text{Support} \\[-5pt]
> > >    \hline
> > >    \text{A} & 4 \\[-5pt]
> > >    \hline
> > >    \text{B} & 5 \\[-5pt]
> > >    \hline
> > >    \text{C} & 3 \\[-5pt]
> > >    \hline
> > >    \text{D} & 4 \\[-5pt]
> > >    \hline
> > >    \text{F} & 4 \\[-5pt]
> > >    \hline
> > >    \text{G} & 4 \\[-5pt]
> > >    \hline
> > >    \end{array}
> > >    \text{ ← }
> > >    \begin{array}{|c|c|}
> > >    \hline
> > >    \text{Item} & \text{Support} \\[-5pt]
> > >    \hline
> > >    \text{A} & 4 \\[-5pt]
> > >    \hline
> > >    \text{B} & 5 \\[-5pt]
> > >    \hline
> > >    \text{C} & 3 \\[-5pt]
> > >    \hline
> > >    \text{D} & 4 \\[-5pt]
> > >    \hline
> > >    \text{E} & 1 \\[-5pt]
> > >    \hline
> > >    \text{F} & 4 \\[-5pt]
> > >    \hline
> > >    \text{G} & 4 \\[-5pt]
> > >    \hline
> > >    \end{array}$
> > >
> > > 2. 主循环：当前种子集为$L_1$
> > >
> > >    - 当前种子集$L_1$(长为$1$)$\xrightarrow{L_1自己两两连接}$候选序列模式$C_{2}$(长为$2$/如下表所示)
> > >
> > >      $\small\begin{cases}2项目\text{: }共36种\text{→}
> > >      \begin{array}{|c|c|c|c|c|c|}
> > >      \hline
> > >      \textcolor{red}{\langle (\mathrm{A})(\mathrm{A}) \rangle} & \textcolor{red}{\langle (\mathrm{A})(\mathrm{B}) \rangle} & \textcolor{red}{\langle (\mathrm{A})(\mathrm{C}) \rangle} & \textcolor{red}{\langle (\mathrm{A})(\mathrm{D}) \rangle} & \textcolor{red}{\langle (\mathrm{A})(\mathrm{F}) \rangle} & \textcolor{red}{\langle (\mathrm{A})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      \textcolor{red}{\langle (\mathrm{B})(\mathrm{A}) \rangle} & \textcolor{red}{\langle (\mathrm{B})(\mathrm{B}) \rangle} & \textcolor{red}{\langle (\mathrm{B})(\mathrm{C}) \rangle} & \textcolor{red}{\langle (\mathrm{B})(\mathrm{D}) \rangle} & \textcolor{red}{\langle (\mathrm{B})(\mathrm{F}) \rangle} & \textcolor{red}{\langle (\mathrm{B})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      \textcolor{red}{\langle (\mathrm{C})(\mathrm{A}) \rangle} & \textcolor{red}{\langle (\mathrm{C})(\mathrm{B}) \rangle} & \textcolor{red}{\langle (\mathrm{C})(\mathrm{C}) \rangle} & \textcolor{red}{\langle (\mathrm{C})(\mathrm{D}) \rangle} & \textcolor{red}{\langle (\mathrm{C})(\mathrm{F}) \rangle} & \textcolor{red}{\langle (\mathrm{C})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      \textcolor{red}{\langle (\mathrm{D})(\mathrm{A}) \rangle} & \textcolor{red}{\langle (\mathrm{D})(\mathrm{B}) \rangle} & \textcolor{red}{\langle (\mathrm{D})(\mathrm{C}) \rangle} & \textcolor{red}{\langle (\mathrm{D})(\mathrm{D}) \rangle} & \textcolor{red}{\langle (\mathrm{D})(\mathrm{F}) \rangle} & \textcolor{red}{\langle (\mathrm{D})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      \textcolor{red}{\langle (\mathrm{F})(\mathrm{A}) \rangle} & \textcolor{red}{\langle (\mathrm{F})(\mathrm{B}) \rangle} & \textcolor{red}{\langle (\mathrm{F})(\mathrm{C}) \rangle} & \textcolor{red}{\langle (\mathrm{F})(\mathrm{D}) \rangle} & \textcolor{red}{\langle (\mathrm{F})(\mathrm{F}) \rangle} & \textcolor{red}{\langle (\mathrm{F})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      \textcolor{red}{\langle (\mathrm{G})(\mathrm{A}) \rangle} & \textcolor{red}{\langle (\mathrm{G})(\mathrm{B}) \rangle} & \textcolor{red}{\langle (\mathrm{G})(\mathrm{C}) \rangle} & \textcolor{red}{\langle (\mathrm{G})(\mathrm{D}) \rangle} & \textcolor{red}{\langle (\mathrm{G})(\mathrm{F}) \rangle} & \textcolor{red}{\langle (\mathrm{G})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      \end{array}
> > >      \\\\1项目\text{: }共15种\text{→}
> > >      \begin{array}{|c|c|c|c|c|c|}
> > >      \hline
> > >      \langle \text{(XX)} \rangle & \textcolor{red}{\langle (\mathrm{AB}) \rangle} & \textcolor{red}{\langle (\mathrm{AC}) \rangle} & \textcolor{red}{\langle (\mathrm{AD}) \rangle} & \textcolor{red}{\langle (\mathrm{AF}) \rangle} & \textcolor{red}{\langle (\mathrm{AG}) \rangle} \\[-5pt]
> > >      \hline
> > >      \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \textcolor{red}{\langle (\mathrm{BC}) \rangle} & \textcolor{red}{\langle (\mathrm{BD}) \rangle} & \textcolor{red}{\langle (\mathrm{BF}) \rangle} & \textcolor{red}{\langle (\mathrm{BG}) \rangle} \\[-5pt]
> > >      \hline
> > >      \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \textcolor{red}{\langle (\mathrm{CD}) \rangle} & \textcolor{red}{\langle (\mathrm{CF}) \rangle} & \textcolor{red}{\langle (\mathrm{CG}) \rangle} \\[-5pt]
> > >      \hline
> > >      \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \textcolor{red}{\langle (\mathrm{DF}) \rangle} & \textcolor{red}{\langle (\mathrm{DG}) \rangle} \\[-5pt]
> > >      \hline
> > >      \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \textcolor{red}{\langle (\mathrm{FG}) \rangle} \\[-5pt]
> > >      \hline
> > >      \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle & \langle \text{(XX)} \rangle \\[-5pt]
> > >      \hline
> > >      \end{array}\end{cases}$  
> > >
> > >    - 候选序列模式$C_{2}$(长为$2$)$\xrightarrow{支持度筛选}$下轮种子集$L_{2}$(长为$2$/见下表红色部分)
> > >
> > >      $\small\textcolor{red}{\langle (\mathrm{AB}) \rangle} \text{ + }
> > >      \begin{array}{|c|c|c|c|c|c|}
> > >      \hline
> > >      {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\color{red} \langle (\mathrm{A})(\mathrm{B}) \rangle} & {\color{red} \langle (\mathrm{A})(\mathrm{C}) \rangle} & {\color{red} \langle (\mathrm{A})(\mathrm{D}) \rangle} & {\color{red} \langle (\mathrm{A})(\mathrm{F}) \rangle} & {\color{red} \langle (\mathrm{A})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\color{red} \langle (\mathrm{B})(\mathrm{C}) \rangle} & {\color{red} \langle (\mathrm{B})(\mathrm{D}) \rangle} & {\color{red} \langle (\mathrm{B})(\mathrm{F}) \rangle} & {\color{red} \langle (\mathrm{B})(\mathrm{G}) \rangle} \\[-5pt]
> > >      \hline
> > >      {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\color{red} \langle (\mathrm{C})(\mathrm{D}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} \\[-5pt]
> > >      \hline
> > >      {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & { \langle (\mathrm{X})(\mathrm{X}) \rangle} \\[-5pt]
> > >      \hline
> > >      {\color{red} \langle (\mathrm{F})(\mathrm{A}) \rangle} & {\color{red} \langle (\mathrm{F})(\mathrm{B}) \rangle} & {\color{red} \langle (\mathrm{F})(\mathrm{C}) \rangle} & {\color{red} \langle (\mathrm{F})(\mathrm{D}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} \\[-5pt]
> > >      \hline
> > >      {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\color{red} \langle (\mathrm{G})(\mathrm{D}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} & {\langle (\mathrm{X})(\mathrm{X}) \rangle} \\[-5pt]
> > >      \hline
> > >      \end{array}$  
> > >
> > > 3. 主循环：当前种子集为$L_2$
> > >
> > >    - 分析$L_2$种序列的($\text{2-1}$)首尾
> > >
> > >      $\small
> > >      \begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
> > >      \hline
> > >      \text{序} & \text{AB} & \text{AC} & \text{AD} & \text{AF} & \text{AG} & \text{BC} & \text{BD} & \text{BF} & \text{BG} & \text{CD} & \text{FA} & \text{FB} & \text{FC} & \text{FD} & \text{GD} & \text{(AB)} \\[-5pt]
> > >      \hline
> > >      \text{首} & \text{B} & \text{C} & \text{D} & \text{F} & \text{G} & \text{C} & \text{D} & \text{F} & \text{G} & \text{D} & \text{A} & \text{B} & \text{C} & \text{D} & \text{D} & \text{A/B} \\[-5pt]
> > >      \hline
> > >      \text{尾} & \text{A} & \text{A} & \text{A} & \text{A} & \text{A} & \text{B} & \text{B} & \text{B} & \text{B} & \text{C} & \text{F} & \text{F} & \text{F} & \text{F} & \text{G} & \text{B/A} \\[-5pt]
> > >      \hline
> > >      \end{array}$ 
> > >
> > >    - 连接与剪枝：$L_2$(长为$2$)$\xrightarrow[只连接第一项和最后一项相同的模式]{L_2自己两两连接+剪切}C_{3}$(长为$3$)$\xrightarrow{根据支持度过滤}L_3$(具体见下表)
> > >
> > >      $\small\begin{array}{|c|c|c|c|c|c|c|c|} \hline L_2(左) & L_2(左)尾 & L_2(右) & L_2(右)首B & 合并(未剪枝) & 合并(剪枝) & 支持度 & 合并得L_3(支持度过滤)\\ \hline \text{AB} & \text{B} & \text{BC} & \text{B} & \text{ABC} & \text{ABC} & 1 & \\[-5pt] \hline \text{AB} & \text{B} & \text{BD} & \text{B} & \text{ABD} & \text{ABD} & 2 & \text{ABD} \\[-5pt] \hline \text{AB} & \text{B} & \text{BF} & \text{B} & \text{ABF} & \text{ABF} & 2 & \text{ABF} \\[-5pt] \hline \text{AB} & \text{B} & \text{BG} & \text{B} & \text{ABG} & \text{ABG} & 2 & \text{ABG} \\[-5pt] \hline \text{AB} & \text{B} & \text{(AB)} & \text{B} & \text{A(AB)} & & & \\[-5pt] \hline \text{AC} & \text{C} & \text{CD} & \text{C} & \text{ACD} & \text{ACD} & 3 & \text{ACD} \\[-5pt] \hline \text{AF} & \text{F} & \text{FA} & \text{F} & \text{AFA} & & & \\[-5pt] \hline \text{AF} & \text{F} & \text{FB} & \text{F} & \text{AFB} & \text{AFB} & 0 & \\[-5pt] \hline \text{AF} & \text{F} & \text{FC} & \text{F} & \text{AFC} & \text{AFC} & 1 & \\[-5pt] \hline \text{AF} & \text{F} & \text{FD} & \text{F} & \text{AFD} & \text{AFD} & 2 & \text{AFD} \\[-5pt] \hline \text{AG} & \text{G} & \text{GD} & \text{G} & \text{AGD} & \text{AGD} & 2 & \text{AGD} \\[-5pt] \hline \text{BC} & \text{C} & \text{CD} & \text{C} & \text{BCD} & \text{BCD} & 2 & \text{BCD} \\[-5pt] \hline \text{BF} & \text{F} & \text{FA} & \text{F} & \text{BFA} & & & \\[-5pt] \hline \text{BF} & \text{F} & \text{FB} & \text{F} & \text{BFB} & & & \\[-5pt] \hline \text{BF} & \text{F} & \text{FC} & \text{F} & \text{BFC} & \text{BFC} & 1 & \\[-5pt] \hline \text{BF} & \text{F} & \text{FD} & \text{F} & \text{BFD} & \text{BFD} & 2 & \text{BFD} \\[-5pt] \hline \text{BG} & \text{G} & \text{GD} & \text{G} & \text{BGD} & \text{BGD} & 3 & \text{BGD} \\[-5pt] \hline \text{FA} & \text{A} & \text{AB} & \text{A} & \text{FAB} & \text{FAB} & 0 & \\[-5pt] \hline \text{FA} & \text{A} & \text{AC} & \text{A} & \text{FAC} & \text{FAC} & 1 & \\[-5pt] \hline \text{FA} & \text{A} & \text{AD} & \text{A} & \text{FAD} & \text{FAD} & 1 & \\[-5pt] \hline \text{FA} & \text{A} & \text{AF} & \text{A} & \text{FAF} & & & \\[-5pt] \hline \text{FA} & \text{A} & \text{AG} & \text{A} & \text{FAG} & & & \\[-5pt] \hline \text{FA} & \text{A} & \text{(AB)} & \text{A} & \text{F(AB)} & \text{F(AB)} & 2 & \text{F(AB)} \\[-5pt] \hline \text{FB} & \text{B} & \text{BC} & \text{B} & \text{FBC} & \text{FBC} & 1 & \\[-5pt] \hline \text{FB} & \text{B} & \text{BD} & \text{B} & \text{FBD} & \text{FBD} & 1 & \\[-5pt] \hline \text{FB} & \text{B} & \text{BF} & \text{B} & \text{FBF} & & & \\[-5pt] \hline \text{FB} & \text{B} & \text{BG} & \text{B} & \text{FBG} & & & \\[-5pt] \hline \text{FC} & \text{C} & \text{CD} & \text{C} & \text{FCD} & \text{FCD} & 2 & \text{FCD} \\[-5pt] \hline \text{(AB)} & \text{B} & \text{BC} & \text{B} & \text{(AB)C} & \text{(AB)C} & 1 & \\[-5pt] \hline \text{(AB)} & \text{B} & \text{BD} & \text{B} & \text{(AB)D} & \text{(AB)D} & 1 & \\[-5pt] \hline \text{(AB)} & \text{B} & \text{BF} & \text{B} & \text{(AB)F} & \text{(AB)F} & 0 & \\[-5pt] \hline \text{(AB)} & \text{B} & \text{BG} & \text{B} & \text{(AB)G} & \text{(AB)G} & 0 & \\[-5pt] \hline \text{(AB)} & \text{A} & \text{AB} & \text{A} & \text{(AB)B} & & \text{} & \text{} \\[-5pt] \hline \end{array}$ 
> > >
> > >    - 剪枝示例：对$\langle (\mathrm{A})(\mathrm{FB}) \rangle$有子模式$\langle (\mathrm{FB}) \rangle$不满足最低支持度，故直接将$\langle (\mathrm{A})(\mathrm{FB}) \rangle$剪去
> > >
> > > 4. 主循环：当前种子集为$L_3$
> > >
> > >    - 分析$L_3$中序列的($\text{3-1}$)首尾
> > >      
> > >      $\small
> > >      \begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
> > >      \hline
> > >      \text{序} & \text{ABD} & \text{ABF} & \text{ABG} & \text{ACD} & \text{AFD} & \text{AGD} & \text{BCD} & \text{BFD} & \text{BGD} & \text{F(AB)} & \text{F(AB)} & \text{FCD} \\[-5pt]
> > >      \hline
> > >      \text{首} & \text{BD} & \text{BF} & \text{BG} & \text{CD} & \text{FD} & \text{GD} & \text{CD} & \text{FD} & \text{GD} & \text{(AB)} & \text{(AB)} & \text{CD} \\[-5pt]
> > >      \hline
> > >      \text{尾} & \text{AB} & \text{AB} & \text{AB} & \text{AC} & \text{AF} & \text{AG} & \text{BC} & \text{BF} & \text{BG} & \text{FA} & \text{FB} & \text{FC} \\[-5pt]
> > >      \hline
> > >      \end{array}$
> > >      
> > >    - 连接与剪枝：$L_3$(长为$3$)$\xrightarrow[只连接前2项和最后2项相同的模式]{L_3自己两两连接+剪切}C_{4}$(长为$4$)$\xrightarrow{根据支持度过滤}L_4$(具体见下表)
> > >    
> > >      $\small\begin{array}{|c|c|c|c|c|c|c|c|} \hline L_3(左) & L_3(左)尾 & L_3(右) & L_3(右)首B & 合并(未剪枝) & 合并(剪枝) & 支持度 & 合并得L_4(过滤)\\ \hline \text{ABF} & \text{BF} & \text{BFD} & \text{BF} & \text{ABFD} & \text{ABFD} & 2 & \text{ABFD}\\[-5pt] \hline \text{ABG} & \text{BG} & \text{BGD} & \text{BG} & \text{ABDG} & \text{ABDG} & 2 & \text{ABDG} \\\hline\end{array}$
> > >    
> > >    - $L_4$无法再执行合并操作，故循环结束
> > >    
> > > 5. 输出：合并所有的$L_i$ 
> > 
> >### $\textbf{2.2.3. PrefixSpan}$算法: 基于模式增长
> > 
> >> :one:基本概念：以$\langle a(abc)(ac)d(cf)\rangle$为例
> > >
> > > 1. 前缀：对于$\alpha\text{=}\langle e_1 e_2 \cdots e_n\rangle$和$\beta\text{=}\langle e_1^{\prime} e_2^{\prime} \cdots e_m^{\prime}\rangle$
> > >    - 定义：$\beta{}$是$\alpha$前缀$\xLeftrightarrow{}\begin{cases}e_i\text{=}e_i^{\prime}\,\,(i\text{=}1,2,...,m\text{-}1)\\\\e_m^{\prime} \text{⊆} e_m\\\\e_m\textbackslash{}e_m^{\prime}项目排在e_m^{\prime}后面\end{cases}$ 
> > >    - 示例：$\langle a(abc)(ac)d(cf)\rangle\xrightarrow{前缀}\langle a(abc)a\rangle$✅$/\langle a(abc)c\rangle$❌ 
> > > 2. 投影：对于$\alpha$及其==子序列==$\beta$
> > >    - 定义：$\alpha\xrightarrow{关于\beta投影}\alpha^{\prime}\xLeftrightarrow{}\begin{cases}\beta{}是\alpha^{\prime}前缀(从\beta处截断并开始试图扩展)\\\\\alpha^{\prime}为满足上述条件的\alpha的最大子序列\end{cases}$ 
> > >    - 示例：$\alpha\text{=}\langle a(abc)(ac)d(cf)  \rangle\xrightarrow{关于\beta\text{=}\langle (bc)a   \rangle投影}\alpha^{\prime}\text{=}\langle (bc)(ac)d(cf) \rangle$ 
> > > 3. 后缀：对于$\alpha\text{}\xrightarrow{关于\beta\text{=}\langle e_1 e_2 \ldots e_{m\text{-}1} e_m^{\prime}   \rangle投影}\alpha^{\prime}\text{=}\langle e_1 e_2\ldots{e_{m\text{-}1} e_m^{\prime}} \ldots e_n \rangle$ 
> > >    - 定义：$\alpha\xrightarrow{关于\beta后缀}\langle {e_m\textbackslash{}e_m^{\prime},e_{m\text{+}1} } \ldots e_n \rangle$，可简单理解为$\alpha^{\prime}$去掉$\beta{}$
> > >    - 示例：$\alpha\text{=}\langle a(abc)(ac)d(cf)  \rangle\xrightarrow[投影]{关于\beta\text{=}\langle (bc)a   \rangle}\alpha^{\prime}\text{=}\langle (bc)(ac)d(cf) \rangle\xrightarrow[后缀]{关于\beta\text{=}\langle (bc)a   \rangle}\langle (c)d(cf) \rangle$  
> > >
> > > :two:$\text{PrefixSpan}$算法流程
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241117141229596.png" alt="image-20241117141229596" width=330 /> 
> > >
> > > 1. 对于原始序列数据库$D(1)$：
> > >    - 初始：扫描，生成所有长度为$1$的序列模式
> > >    - 迭代：根据生成的长度为$1$的序列模式，生成相应的投影(后缀)数据库$D(2)$
> > > 2. 对生成的投影数据库$D(2)$：
> > >    - 初始：扫描，生成所有长度为前缀$\text{1}$的序列模式
> > >    - 迭代：根据生成的长度为$1$的序列模式，生成相应的投影(后缀)数据库$D(3)$
> > > 3. 终止：不断迭代直到$D(n)$生成不出任何长度为$1$的序列模式
> > >
> > > :three:$\text{PrefixSpan}$算法流程示例：最小支持度为$2$  
> > >
> > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241117153646898.png" alt="image-20241117153646898" width=400 /> 
> > >
> > > 0. 初始化：对于原始数据库$D(0)$
> > >
> > >    $\begin{array}{|cc|}
> > >    \hline \text {SeqID} & \text {Sequence} \\[-5pt]
> > >    \hline 10 & \langle\mathrm{a}(\mathrm{abc})(\mathrm{ac}) \mathrm{d}(\mathrm{cf})\rangle\\[-5pt]
> > >    \hline 20 & \langle(\mathrm{ad}) \mathrm{c}(\mathrm{bc})(\mathrm{ae})\rangle \\[-5pt]
> > >    \hline 30 & \langle(\mathrm{ef})(\mathrm{ab})(\mathrm{df}) \mathrm{cb}\rangle \\[-5pt]
> > >    \hline 40 & \langle\mathrm{eg}(\mathrm{af}) \mathrm{cbc}\rangle \\[-5pt]
> > >    \hline
> > >    \end{array}$  
> > >
> > >    - 初始：所有长度为$1$的序列模式$\xrightarrow{已过滤}\{\langle a \rangle,\langle b \rangle,\langle c \rangle,\langle d \rangle,\langle e \rangle,\langle f \rangle,\langle g \rangle\}$  
> > >
> > >      $\small\begin{array}{|c|c|c|c|c|c|c|}
> > >      \hline \text { a } & \text { b } & \text { c } & \text { d } & \text { e } & \text { f } & \text { g } \\
> > >      \hline 4 & 4 & 4 & 3 & 3 & 3 & 1 \\
> > >      \hline
> > >      \end{array}$ 
> > >
> > >      :warning:一个元素在一个记录中出现多次时只认为是一次，如$\langle\mathrm{a}(\mathrm{abc})(\mathrm{ac}) \mathrm{d}(\mathrm{cf})\rangle$记$\text{a}$出现一次
> > >
> > >    - 过滤：依据$\text{Apriori}$原理滤掉小于最小支持度的结点(此处为$\text{g}$)，得到序列数据库$D(1)$
> > >
> > > 1. 对序列数据库$D(1)$：
> > >
> > >    $\begin{array}{|cc|}
> > >    \hline \text {SeqID} & \text {Sequence} \\[-5pt]
> > >    \hline 10 & \langle\mathrm{a}(\mathrm{abc})(\mathrm{ac}) \mathrm{d}(\mathrm{cf})\rangle\\[-5pt]
> > >    \hline 20 & \langle(\mathrm{ad}) \mathrm{c}(\mathrm{bc})(\mathrm{ae})\rangle \\[-5pt]
> > >    \hline 30 & \langle(\mathrm{ef})(\mathrm{ab})(\mathrm{df}) \mathrm{cb}\rangle \\[-5pt]
> > >    \hline 40 & \langle\mathrm{e}(\mathrm{af}) \mathrm{cbc}\rangle \\[-5pt]
> > >    \hline
> > >    \end{array}$   
> > >
> > >    - 初始：所有长度为$1$的序列模式$\text{→}\{\langle a \rangle,\langle b \rangle,\langle c \rangle,\langle d \rangle,\langle e \rangle,\langle f \rangle\}$ 
> > >
> > >      $\small\begin{array}{|c|c|c|c|c|c|c|}
> > >      \hline \text { a } & \text { b } & \text { c } & \text { d } & \text { e } & \text { f }  \\
> > >      \hline 4 & 4 & 4 & 3 & 3 & 3 \\
> > >      \hline
> > >      \end{array}$ 
> > >
> > >    - 迭代：将$D(1)$数据库分别在$\{\langle a \rangle,\langle b \rangle,\langle c \rangle,\langle d \rangle,\langle e \rangle,\langle f \rangle\}$上投影(==后缀==)，得到数据库$D(2)$
> > >
> > >      $\begin{array}{|c|l|} \hline \text{Prefix} & \text{Project Database} \\[-5pt] \hline \langle \text{a}\rangle  & \langle (\text{abc})(\text{ac})\text{d}(\text{cf})\rangle,\,  \langle (\text{\_d})\text{c}(\text{bc})(\text{ae})\rangle,\,  \langle (\text{\_b})(\text{df})\text{cb}\rangle,\,  \langle (\text{\_f})\text{cbc}\rangle\\[-5pt] \hline \langle \text{b}\rangle  & \langle  (\text{\_c})(\text{ac})\text{d}(\text{cf})\rangle,\,  \langle (\text{\_c})(\text{ae})\rangle,\,  \langle (\text{df})\text{cb}\rangle,\,  \langle \text{c}\rangle\\[-5pt] \hline \langle  \text{c}\rangle  & \langle (\text{ac})\text{d}(\text{cf})\rangle,\, \langle (\text{bc})(\text{ae})\rangle,\, \langle \text{b}\rangle,\,  \langle \text{bc}\rangle\\[-5pt] \hline \langle \text{d}\rangle  & \langle (\text{cf})\rangle,\, \langle \text{c} (\text{bc})(\text{ae})\rangle,\,  \langle (\text{\_f})\text{cb}\rangle\\[-5pt] \hline \langle \text{e}\rangle  & \langle  (\text{\_f})(\text{ab})(\text{df})\text{cb}\rangle,\,  \langle \text{f}(\text{af})\text{cbc}\rangle\\[-5pt] \hline \langle  \text{f} \rangle  & \langle (\text{ab})(\text{df}) \text{cb}\rangle,\, \langle \text{cbc}\rangle\\[-5pt] \hline \end{array}$  
> > >
> > > 2. 对序列数据库$D(2)$：以$D(2,a)$(即$\langle a \rangle\text{-}$投影数据库)为例
> > >
> > >    $\begin{array}{|cc|}
> > >    \hline \text {SeqID} & \text {Sequence} \\[-5pt]
> > >    \hline 101 & \langle\mathrm{(abc)}(\mathrm{ac}) \mathrm{d}(\mathrm{cf})\rangle\\[-5pt]
> > >    \hline 201 & \langle\mathrm{(\_d)} \mathrm{c}(\mathrm{bc})(\mathrm{ae})\rangle \\[-5pt]
> > >    \hline 301 & \langle\mathrm{(\_b)}(\mathrm{df}) \mathrm{cb}\rangle \\[-5pt]
> > >    \hline 401 & \langle\mathrm{(\_f}) \mathrm{cbc}\rangle \\[-5pt]
> > >    \hline
> > >    \end{array}$ 
> > >
> > >    - 初始：所有以$\langle a \rangle$为前缀长度为$2$的序列模式$\xrightarrow{过滤}\{\langle a \rangle,\langle b \rangle,\langle \_b \rangle,\langle c \rangle,\langle d \rangle,\langle f \rangle\}$  
> > >
> > >      $\small\begin{array}{|c|c|c|c|c|c|}
> > >      \hline \text { a } & \text { b } & \text { \_b } & \text { c } & \text { d } & \text { f } \\
> > >      \hline 2 & 4 & 2 & 4 & 2 & 2 \\
> > >      \hline
> > >      \end{array}$ (:warning:$\_\text{b}$出现两次是因为把$\text{ab}$也算了一次)
> > >
> > >    - 迭代：将$D(2,a)$数据库分别在$\{\langle a \rangle,\langle b \rangle,\langle \_b \rangle,\langle c \rangle,\langle d \rangle,\langle f \rangle\}$上投影(==后缀==)，得到数据库$D(3,a)$ 
> > >
> > >      $\begin{array}{|c|l|} \hline \text{Prefix} & \text{Project Database} \\[-5pt] \hline \langle \text{a}\rangle  & \langle (\text{\_bc})(\text{ac})\text{d}(\text{cf})\rangle,\,  \langle (\text{\_e})\rangle\\[-5pt] \hline \langle \text{b}\rangle  & \langle (\text{\_c})(\text{ac})\text{d}(\text{cf})\rangle,\,  \langle (\text{\_c})(\text{ae})\rangle,\,   \langle \text{c}\rangle\\[-5pt] \hline \langle  \text{\_b}\rangle  & \langle (\text{\_c})(\text{ac})\text{d}(\text{cf})\rangle,\, \langle (\text{df})\text{cb}\rangle\\[-5pt] \hline \langle \text{c}\rangle  & \langle (\text{ac})\text{d}\text{(cf)}\rangle,\, \langle  (\text{bc})(\text{ae})\rangle,\, \langle  \text{b}\rangle,\, \langle \text{bc}\rangle\\[-5pt] \hline \langle \text{d}\rangle  & \langle  (\text{cf})\rangle,\,  \langle \text{}(\text{\_f})\text{cb}\rangle\\[-5pt] \hline \langle  \text{f} \rangle  & \langle \text{cb}\rangle \\[-5pt] \hline \end{array}$ 
> > >
> > > 3. 对序列数据库$D(3,a)$：以$D(3,ad)$(即$\langle ad \rangle\text{-}$投影数据库)为例，无频繁元素停止迭代。==开始回溯==
> > >
> > > 4. 终止：回溯所有可能的结点，最终算法才算结束
>
> ## $\textbf{2.3. }$分类
>
> > ### $\textbf{2.3.1. }$概述: 分类定义$\textbf{\&}$评价指标
> >
> > > :one:分类概念：
> > >
> > > 1. 定义：将数据分为不同类别的过程
> > > 2. 过程：学习已有数据的特征和标签$\text{→}$构建模型$\text{→}$将新数据分配到预定义类别中
> > >
> > > :two:评估方法
> > >
> > > |            方法             | 描述                                                         |
> > > | :-------------------------: | ------------------------------------------------------------ |
> > > |   $\text{Holdout Method}$   | 将数据集(随机)分为<span style="color:red;">训练集</span>/<span style="color:green;">测试集</span>分别<span style="color:red;">训练</span>/<span style="color:green;">测试</span> |
> > > | $\text{Random Subsampling}$ | 进行多轮$\text{Holdout Method}$，取多轮评估的平均作为最终结果 |
> > > |  $\text{Cross-Validation}$  | 将数据分为$k$个小块，让每一小块轮流作测试集(其余$k\text{-1}$块为训练集 |
> > >
> > > :three:评估指标
> > >
> > > 1. $\text{Confusion Matrix}$：
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109152520072.png" alt="image-20241109152520072" width=199 /> 
> > >
> > >    |      **实际\预测**      |         **$\mathbf{C}_1$**         |      **$\neg \mathbf{C}_1$**       |
> > >    | :---------------------: | :--------------------------------: | :--------------------------------: |
> > >    |   **$\mathbf{C}_1$**    | $\text{True Positives(TP)}$真阳性  | $\text{False Negatives(FN)}$假阴性 |
> > >    | **$\neg \mathbf{C}_1$** | $\text{False Positives(FP)}$假阳性 | $\text{True Negatives(TN)}$真阴性  |
> > >
> > > 2. 准确率/误差率：
> > >
> > >    |       指标        |                计算                | 含义                 |
> > >    | :---------------: | :--------------------------------: | -------------------- |
> > >    | $\text{Accuracy}$ | $\cfrac{\text{TP+TN}}{\text{ALL}}$ | 预测==正确==的总占比 |
> > >    |  $\text{Error}$   |        $1-\text{Accuracy}$         | 预测==错误==的总占比 |
> > >
> > >
> > > 3. 敏感性/特异性：
> > >
> > >    - 敏感性/特异性含义：
> > >
> > >      |             指标             |               计算                | 含义                  |
> > >     | :--------------------------: | :-------------------------------: | --------------------- |
> > >      | $\text{Sensitivity(Recall)}$ | $\cfrac{\text{TP}}{\text{TP+FN}}$ | 检出$\text{TP}$的能力 |
> > >      |     $\text{Specificity}$     | $\cfrac{\text{TN}}{\text{TN+FP}}$ | 避免$\text{FP}$的能力 |
> > >    
> > >    - $\text{ROC}$曲线：即$\text{Specificity}-\text{(1-Specificity)}$曲线，$\text{AUC}$值为其横轴积分
> > >
> > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241113221128573.png" alt="image-20241113221128573" width=250 /> 
> > >
> > > 3. 召回/精度：   
> > >
> > >    |             指标             |               计算                | 含义                        |
> > >    | :--------------------------: | :-------------------------------: | --------------------------- |
> > >    | $\text{Recall(Sensitivity)}$ | $\cfrac{\text{TP}}{\text{TP+FN}}$ | 真实阳性中，$\text{TP}$比例 |
> > >    |      $\text{Precision}$      | $\cfrac{\text{TP}}{\text{TP+FP}}$ | 预测阳性中，$\text{TP}$比例 |
> > >    
> > > 3. $\displaystyle\text {F}_{\beta}\text{-score}$：  
> > >
> > >    |                     指标                      |                             计算                             | 含义                                                         |
> > > | :-------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
> > >    | $\displaystyle\text {F}_{\beta}\text{-score}$ | $(1\text{+}\beta^2) \text{×} \cfrac{\text{Precision} \text{×} \text {Recall }}{\beta{}\text{×}\text {Precision+} \text{Recall}}$ | <span style="color:red;">$\beta{}\text{<1}$</span>/<span style="color:green;">$\beta{}\text{>1}$</span>时<span style="color:red;">$\text{Preci.}$</span>/<span style="color:green;">$\text{Recall}$</span>影响更大 |
> > > |   $\displaystyle\text {F}_1\text{-score }$    | $2 \text{×} \cfrac{\text {Precision} \text{×} \text {Recall}}{\text {Precision+} \text {Recall}}$ | 二者均等重要的调和平均                                       |
> > >
> >
> > ### $\textbf{2.3.2. }$决策树
> >
> > > #### $\textbf{2.3.2.1. }$决策树概述
> > >
> > > > :one:决策树的结构：
> > > >
> > > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241113231755874.png" alt="image-20241113231755874" width=350 /> 
> > > >
> > > > 1. 对应关系：中间(根)结点$\xleftrightarrow{对应}$对某属性的测试，分支$\xleftrightarrow{对应}$属性值，叶结点$\xleftrightarrow{对应}$实例所属类别
> > > > 2. 分类流程：把实例从根结点一层层排列到叶子结点
> > > >
> > > > :two:决策树的分类策略
> > > >
> > > > 1. 属性的选择：基于启发式/统计测试，使得信息增益/增益率/$\text{Gini}$指标等度量值最好
> > > > 2. 终止划分条件：无样本剩下 $\text{or}$ 给定的某节点所有样本属于一类 $\text{or}$ 没有属性用于下一步划分
> > > >
> > > > :three:有关符号
> > > >
> > > > | 集合符号  | 集合样本数  | 含义                                                      |
> > > > | :-------: | :---------: | :-------------------------------------------------------- |
> > > > |   ${D}$   |    $|D|$    | 训练样本集，含$m$个类别$C_i\text{⊆}D(i\text{=}1\dots{}m)$ |
> > > > | $C_{i,D}$ | $|C_{i,D}|$ | $D$中$C_i$类样本的集合                                    |
> > >
> > > #### $\textbf{2.3.2.2. }$决策树的度量指标
> > >
> > > >:one:信息增益：
> > > >
> > > >1. 信息熵：划分后集合的混乱程度
> > > >
> > > >   - 集合：$D$包含了$n$个类别$C_i$(如下)
> > > >
> > > >      |   类别   |               $C_1$                |             **$C_2$**              |             **$C_3$**              | **......** |               $C_n$                |
> > > >      | :------: | :--------------------------------: | :--------------------------------: | :--------------------------------: | :--------: | :--------------------------------: |
> > > >      |  元素数  |             $N_{C_1}$              |             $N_{C_2}$              |             $N_{C_3}$               |   ......   |             $N_{C_n}$              |
> > > >      | 元素频率 | $P_1\text{=}\cfrac{N_{C_1}}{N_总}$ | $P_2\text{=}\cfrac{N_{C_2}}{N_总}$ | $P_3\text{=}\cfrac{N_{C_3}}{N_总}$ |   ......   | $P_n\text{=}\cfrac{N_{C_n}}{N_总}$ |
> > > >
> > > >    - 熵值：$\text{Info}(D)\text{=}\displaystyle{}-\sum_{i=1}^{n}P_i\log{P_i}$ (单位为$\text{bits}$)
> > > >
> > > >2. 条件熵：按属性划分的信息熵
> > > >
> > > >   - 划分：$D\xrightarrow[划分为v个子集]{有v个值的属性A}\{D_1,D_2,\dots,D_v\}$，注意$D_i$由于其它属性存在$\text{→}$内部依然分类
> > > >   - 熵值：$D$关于属性$A$划分的熵为$\text{Info}_A(D)\text{=}\displaystyle{}\sum_{j=1}^{v}\cfrac{|D_j|}{|D|}\text{×}\text{Info}(D_j)$
> > > >
> > > >3. 信息增益：
> > > >
> > > >   - 含义：==划分导致的不确定性降低程度==，即$\text{Gain=Info}(D)-\text{Info}_A(D)$ 
> > > >   
> > > >   - 意义：<span style="color:red;">选择有最大信息增益的属性来划分</span> 
> > > >   
> > > >     <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241114011949346.png" alt="image-20241114011949346" width=530 />  
> > > >   
> > > >
> > > >:two:信息增益率：
> > > >
> > > >1. 划分熵：$\displaystyle{}\text{SplitInfo}(A)\text{=}-\sum_{j=1}^m \cfrac{\left|D_j\right|}{|D|} \log _2\left(\frac{\left|D_j\right|}{|D|}\right)\text{→}$划分越均匀值越低$\xrightarrow{衡量}$划分有效$?$ 
> > > >
> > > >2. 信息增益率：$\text{GainRatio}(A)\text{=}\cfrac{\operatorname{Gain}(A)}{\operatorname{SplitInfo}(A)}\text{→}$避免信息增益的偏向性
> > > >3. 意义：<span style="color:red;">选择有最大信息增益率的属性来划分</span> 
> > > >
> > > >:three:$\text{Gini}$指数
> > > >
> > > >1. 数据集的$\text{Gini}$指数：
> > > >
> > > >   - 定义：$\displaystyle{}\text{Gini}(D)\text{=}1-\sum_{j=1}^n p_j^2$，其中$p_j$为类别$j$样本数占总样本数比例
> > > >
> > > >    - 含义：衡量样本分布的均匀程度$\text{→}\begin{cases}\text{Gini}值越大\text{→}样本越分散\text{→}纯度低\\\\
> > > >       \text{Gini}值越小\text{→}样本越集中\text{→}纯度高
> > > >       \end{cases}$
> > > >    
> > > >       <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241114023252163.png" alt="image-20241114023252163" width=330 />  
> > > >
> > > >2. 基于属性$A$分裂的$\text{Gini}$指数
> > > >
> > > >   - 定义：对于$D\xrightarrow{属性A(含有v个值)}\{D_1,D_2,...,D_v\}$有$\displaystyle{}\text{Gini}_A(D)\text{=}\sum_{i=1}^{v}\cfrac{\left|D_i\right|}{|D|} \text{Gini}\left(D_i\right)$ 
> > > >   - 含义：衡量$D$经过$A$分裂后的整体不纯度
> > > >
> > > >3. 基于属性$A$分裂的不纯度减少
> > > >
> > > >   - 定义：$\Delta{\text{Gini}}(A)\text{=}\text{Gini}(D)-\text{Gini}_A(D)$ 
> > > >   - 意义：<span style="color:red;">选择能够使$\Delta{\text{Gini}(A)}$最大$/\Delta{\text{Gini}_A}$最小最小的属性进行结点分裂</span> 
> > > >
> > > >:upside_down_face:度量的对比和总结
> > > >
> > > >|       指标        | 分裂属性选择                                                 | 分裂过程倾向于             |
> > > >| :---------------: | ------------------------------------------------------------ | -------------------------- |
> > > >|     信息增益      | 使信息增益$\text{Gain}(A)$最大的属性                         | 多值属性                   |
> > > >|    信息增益率     | 使信息增益率$\text{GainR.}(A)$最大的属性                     | 不平衡分裂(某些子集极小)   |
> > > >| $\text{Gini}$指数 | 使不纯度减少$\Delta{\text{Gini}}(A)$最大==(纯度增加)==的属性 | 多值属性$\&$分裂后纯度提高 |
> > >
> > > #### $\textbf{2.3.2.3. }$一些经典的决策树算法
> > >
> > > > :one:$\text{ID3}$(迭代二分$3$)
> > > >
> > > > 1. 思想：默认属性值离散$\text{→}$结点分裂时(遍历每个特征的信息增益)选择==信息增益最大==的特征
> > > > 2. 特点：倾向于选择多指特征(不合理)，对噪声敏感，方法简单应用广
> > > >
> > > > :two:$\text{C4.5}$算法：基于$\text{ID3}$的改进
> > > >
> > > > 1. 离散属性选择：直接选择结点分裂时==信息增益<span style="color:red;">率</span>==最大的特征$\text{→}$克服了对值属性的倾向性
> > > >
> > > > 2. 连续属性离散化：
> > > >
> > > >    - 排序：对连续属性$A$，其在$D$中取有$m$个离散属性值，排序后得到$\left\{a_1, a_2, \ldots, a_m\right\}$ 
> > > >
> > > >    - 分割：对$\left\{a_1, a_2, \ldots, a_m\right\}$有$m\text{-}1$种方式一分为二，选择==信息增益率最大的==分割以进行
> > > >
> > > >      <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241114163536502.png" alt="image-20241114163536502" width=300 /> 
> > > >
> > > >    - 递归：可对划分得子集$\left\{a_1, a_2, \ldots, a_k\right\}$或$\left\{a_{k+1}, a_{k+2}, \ldots, a_m\right\}$进一步按此划分
> > > >
> > > >    - 终止：直到达到某阈值，$\left\{a_1, a_2, \ldots, a_m\right\}$被分为互不交叉的$K$块(转化为$K$个离散值)
> > > >
> > > > 3. 对缺失数据的处理：
> > > >
> > > >    - 含义：数据的某个属性的值会缺失，需要根据已知值来估计(以填充)
> > > >    - 策略：($\text{Quinlan}$)计算每个属性$a_i$值出现的概率$P(a_i)\text{→}$为缺失值$e_i$赋予概率分布$P(a_i)$ 
> > > >
> > > > 4. 生成规则：
> > > >
> > > >    - 逻辑：将根$\text{→}$叶路径转化为如下$\text{IF-THEN}$规则，由此决策树也变为$\text{IF-THEN}$规则集合
> > > >
> > > >      ```txt
> > > >      IF [中间节点所有条件] THEN [根节点类别]
> > > >      ```
> > > >
> > > >    - 存储：规则(路径)会被存储在数组中，每行对应每个路径
> > > >
> > > > :three:$\text{CART}$算法：采用二元化分的二叉决策树
> > > >
> > > > 1. 划分方式：
> > > >    - 离散属性：计算每种二元分裂的$\text{Gini}$指数，选择$\text{Gini}$最小(纯度最高)的方式分裂
> > > >    - 连续属性：排序后考虑每个分割点，选择使得$\text{Gini}$最小的分割点进行分割
> > > > 2. 递归划分：在划分得到的子集上递归地进行下一次划分$\text{→}$直到达到终止条件
> > > > 3. 终止条件：
> > > >    - 叶结点：样本数为$1$，或者小于某给定值$N_{\min}$
> > > >    - 属性：结点中样本同属一类，即无更多属性可供分裂
> > > >    - 高度：决策树高度达到用户预设
> > >
> > > #### $\textbf{2.3.2.4. }$ 决策树的其它有关内容
> > >
> > > > :one:决策树的过拟合
> > > >
> > > > 1. 欠拟合与过拟合：
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241114180902178.png" alt="image-20241114180902178" width=450 /> 
> > > >
> > > >    |  类型  | 模型表现        | 成因                                                 |
> > > >    | :----: | --------------- | ---------------------------------------------------- |
> > > >    | 欠拟合 | 训练集❌/测试集❌ | 模型过于简单                                         |
> > > >    | 过拟合 | 训练集✅/测试集❌ | 决策树分支过多$\text{→}$对噪声的过分拟合，训练集太小 |
> > > >
> > > > 2. 欠拟合的解决：剪枝
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241114181928973.png" alt=" " width=300 /> 
> > > >
> > > >    |  方式  |   时机   | 操作                                 | 剪枝标准                            |
> > > >    | :----: | :------: | ------------------------------------ | ----------------------------------- |
> > > >    | 预剪枝 | 树构建时 | 使结点终止分裂                       | 结点实例少于某值/分裂不使纯度提高时 |
> > > >    | 后剪枝 | 树构建后 | 中间节点$\xrightarrow{替换为}$叶节点 | 剪枝后能否降低错误率                |
> > > >
> > > > :two:集成学习
> > > >
> > > > 1. 含义：单个机器学习(弱学习算法)$\xrightarrow[整合]{某种规则}$集成学习(强学习算法)，由此获得更好的效果  
> > > >
> > > > 2. $\text{Bagging}$：并行逻辑
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/werhgjhgdhtgerwr.png" alt="werhgjhgdhtgerwr" width=450 /> 
> > > >
> > > >    - 流程：产生$S$个训练集$\text{→}$训练$S$个分类器$\text{→}$预测时票选出$S$个预测结果中出现最多的
> > > >
> > > > 3. $\text{AdaBoost}$：串行逻辑
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241114205611790.png" alt="image-20241114204954384" width=550 />  
> > > >
> > > >    |   操作   | 含义                                                         |
> > > >    | :------: | ------------------------------------------------------------ |
> > > >    |  初始化  | 对于含$N$个样本的数据集，赋予每个样本相同的权值$\cfrac{1}{N}$ |
> > > >    |   训练   | 使用当前权重训练基学习器，并计算其在训练集上的误差           |
> > > >    |  更新权  | 降低被正确分类的样本的权值，提升被错误分类样本的权值         |
> > > >    | 线性组合 | 让误差大的基学习器比例大，误差小的比例小，进行线性组合       |
> > > >
> > > > :three:梯度与梯度提升树：样本对损失函数的负梯度$\xrightarrow{当作/估计}$该样本残差
> > > >
> > > > 1. 梯度提升：对样本$\begin{bmatrix}(x_1,y_1)\\(x_2,y_2)\\...\\(x_n,y_n)\end{bmatrix}$拟合(回归)，使损失函数$J\text{=}\displaystyle{}\sum_{i}L(F(x_i),y_i)$最小
> > > >
> > > >    - 拟合：对当下的拟合$\begin{bmatrix}F(x_1)\\F(x_2)\\...\\F(x_n)\end{bmatrix}$有残差$\begin{bmatrix}h(x_1)\text{=}y_1\text{-}F(x_1)\\h(x_2)\text{=}y_2\text{-}F(x_2)\\......\\h(x_n)\text{=}y_n\text{-}F(x_n)\end{bmatrix}$ 需调整$ch$来最小化$J$
> > > >    - 梯度：$\small\cfrac{\partial J}{\partial F(x_i)}\text{=}F(x_i)-y_i\text{ ←}
> > > >      \begin{cases}
> > > >      \displaystyle{}L(y_i, F(x_i))\text{=}\cfrac{(y-F(x))^\alpha}{\alpha}(以其为例, 可选其它)\\\\
> > > >      \displaystyle{}\cfrac{\partial J}{\partial F(x_i)}=\cfrac{\displaystyle{}\partial \sum_i L(y_i, F(x_i))}{\partial F(x_i)}=\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}
> > > >      \end{cases}$
> > > >    - 下降：$\small{}F(x_i)\text{:=}F(x_i)\text{+}h(x_i)\xrightarrow{\displaystyle{}h(x_i)=y_i-F(x_i)=\frac{\partial J}{\partial F(x_i)}}F(x_i)-\cfrac{\partial J}{\partial F(x_i)}$ 
> > > >
> > > > 2. 梯度提升树$\text{(GBDT)}$：
> > > >
> > > >    - 原理概述：由多个决策树串联的集成学习，可描述为$\displaystyle{} F(x) \text{=} F_0(x) + \sum_{m=1}^M \gamma_m h_m(x)$
> > > >
> > > >      |    参数    | 含义                                                         |
> > > >      | :--------: | ------------------------------------------------------------ |
> > > >      |   $F(x)$   | 用于预测$y$的预测模型                                        |
> > > >      |  $F_0(x)$  | 初始模型，一般初始化为$\cfrac{1}{n}\displaystyle{}\sum_{i=1}^n{}y_i$ |
> > > >      |  $h_m(x)$  | 第$m$个弱学习器(一般为决策树)                                |
> > > >      | $\gamma_m$ | 第$m$个决策树的步长稀疏/学习率，控制每棵树对总模型的贡献     |
> > > >
> > > >    - 核心步骤：
> > > >
> > > >      |  大步  |     小步     | 操作                                                         |
> > > >      | :----: | :----------: | ------------------------------------------------------------ |
> > > >      | 初始化 | $\text{N/A}$ | 设定$F_0(x)$为所有目标值的平均，例如$F_0(x)\text{=}\cfrac{1}{n}\displaystyle{}\sum_{i=1}^n{}y_i$ |
> > > >      | 迭代树 |   计算残差   | 用$\displaystyle{}r_{x_i,m} \text{=} \cfrac{-\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}$当作当前$F_{m-1}(x_i)$与$y_i$残差 |
> > > >      | 迭代树 |   拟合残差   | 让$h_m(x_i)$尽可能接近$r_{x_i,m}\text{→}$让当前$h_m(x_i)$学习上一步误差 |
> > > >      | 迭代树 |   更新模型   | $F_m(x) \text{=} F_{m-1}(x) \text{+} \gamma_m h_m(x)$        |
> > > >      |  终止  | $\text{N/A}$ | 达到最大迭代次数$M/$总体损失函数小于某值$\text{→}$停止迭代   |
> >
> > ### $\textbf{2.3.3. }$贝叶斯分类器
> >
> > > :one:$\text{Bayes}$定律
> > >
> > > 1. 推导：
> > >
> > >    - $P\left(B_i| A\right)\text{=}\cfrac{P\left(B_i\right) P\left(A|B_i\right)}{P(A)}\Leftarrow\begin{cases}P(A|B_i)\text{=}\cfrac{P(AB_i)}{P(B_i)}\\\\P(B_i|A)\text{=}\cfrac{P(AB_i)}{P(A)}\end{cases}$ 
> > >    - $P(B_i|A)\text{=}\cfrac{P(B_i)P(A|B_i)}{\displaystyle\sum_{j}P(A|B_j)P(B_j)}\Leftarrow{}{P(A)}\text{=}\displaystyle\sum_{j}P(AB_j)\text{=}\sum_{j}P(A|B_j)P(B_j)$ 
> > >
> > > 2. 解释：
> > >
> > >    |    参数    | 含义                                     | 示例                         |
> > >    | :--------: | ---------------------------------------- | ---------------------------- |
> > >    |  $P(B_i)$  | 先验概率，指对事件$B_i$发生的主观臆测    | 某种疾病在人群中的发病率     |
> > >    | $P(B_i|A)$ | 后验概率，指观察到$A$发生后$B_i$发生概率 | 对个体实施检测后个体患病概率 |
> > >
> > > :two:$\text{Bayes}$定律与分词(断句)：令$X$为待分词句子，$Y\text{=}\{W_1,W_2,...,W_n\}$为一种可能的分词结果
> > >
> > > 1. $\text{Bayes}$分解：$P(Y | X) \text{∝} P(Y)\text{×}P(X|Y)\xrightarrow[认为P(X|Y)\text{=}1]{基于该句子某种断句\text{→}必定生成该句子}P(Y | X) \text{∝} P(Y)$  
> > >
> > > 2. 联合概率展开：
> > >
> > >    - $P(Y)\text{=}P(W_1)$
> > >
> > >      ```txt
> > >      W1 →
> > >      ```
> > >
> > >    - $P(Y)\text{=}P(W_1)\text{×}P(W_2|W1)$
> > >
> > >      ```txt
> > >      W1 W2 →
> > >      ```
> > >
> > >    - $P(Y)\text{=}P(W_1)\text{×}P(W_2|W1)\text{×}P(W_3|W2,W_1)$
> > >
> > >      ```txt
> > >      W1 W2 W3 →
> > >      ```
> > >
> > >    - $P(Y)\text{=}P(W_1)\text{×}P(W_2|W1)\text{×}P(W_3|W2,W_1)\text{×}\cdots\text{×}P\left(W_n| W_1, W_2, \ldots, W_{n-1}\right)$ 
> > >
> > >      ```txt
> > >      W1 W2 W3 ... Wn.
> > >      ```
> > >
> > > 3. $k$阶马可夫假设：
> > >
> > >    - 含义：认为某个单词只由其前$k$个单词确定，以解决稀疏性问题
> > >    - 简化：$P(Y)\text{=}P(W_1)\text{×}P(W_2|W_1)\text{×}P(W_3|W_2)\text{×}\cdots\text{×}P\left(W_n|W_{n-1}\right)$  
> > >
> > > :three:朴素贝叶斯分类器
> > >
> > > 1. 模型描述：
> > >
> > >    - 假设：决定各分类的属性之间是相互独立(简单粗暴/但也损失了分类精度)
> > >    - 前提：样本$X(a_1,a_2,...,a_n)$有$n$个属性$\{A_1,A_2,...,A_n\}$且有$m$个类$\{C_1,C_2,...,C_m\}$
> > >
> > >    - 分类：将$X(a_1,a_2,...,a_n)$归类为$C_i\xLeftrightarrow{等价}P\left(C_i | X\right)\text{>}P\left(C_j|X\right)$，其中$C_j$为除$C_i$任一类
> > >
> > > 2. 模型分析：
> > >
> > >    - $P\left(C_i|X\right)\text{=}\cfrac{P\left(X|C_i\right) P\left(C_i\right)}{P(X)}\xRightarrow{P(X)为常数}$最大化$P\left(X|C_i\right) P\left(C_i\right)$以分类
> > >    - $P(X|C_i)P(C_i)\text{=}P(A_1\text{=}a_1,A_2\text{=}a_2,\cdots,A_n\text{=}a_n|C_i)P(C_i)\xRightarrow{各属性独立}\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)$ 
> > >
> > > 3. 模型流程：
> > >
> > >    | 阶段 | 操作                                                         |
> > >    | :--: | ------------------------------------------------------------ |
> > >    | 准备 | 确定样本的属性，获取相应的样本                               |
> > >    | 训练 | 对每个类别计算$P(C_i)\text{→}$对每个属性计算$P(a_k|C_i)$     |
> > >    | 应用 | 对新样本$\Lambda$计算其对每个类别的$P(\Lambda{}|C_i)P(C_i)\text{→}$找到使之最大的$C_i$以归类之 |
> > >
> > > 4. $\text{Underflow}$问题：
> > >
> > >    - 问题：多概率相乘$\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)$很可能会快速变为$0$
> > >    - 解决：取对数$\text{→}\log{\left(\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)\right)}$ 
> > >
> > > :four:示例：判断学历为大学，年薪$\text{30-40}$，薪水$\text{20000-30000}$的员工的性别
> > >
> > > ​    $\begin{array}{|cccccc|}
> > > \hline \text { 样本 } & \text { 性别 } & \text { 工作内容 } & \text { 学历 } & \text { 年龄 } & \text { 薪水 } \\
> > > \hline 1 & \text { 女 } & \text { 送货 } & \text { 大学 } & 20-30 & 20000-30000 \\
> > > \hline 2 & \text { 男 } & \text { 包装 } & \text { 大学 } & >40 & >40000 \\
> > > \hline 3 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & 30-40 & 20000-30000 \\
> > > \hline 4 & \text { 男 } & \text { 包装 } & \text { 高中 } & 30-40 & 20000-30000 \\
> > > \hline 5 & \text { 男 } & \text { 送华 } & \text { 大学 } & >40 & 30000-40000 \\
> > > \hline 6 & \text { 女 } & \text { 烘烤 } & \text { 高中 } & 20-30 & 20000-30000 \\
> > > \hline 7 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & 20-30 & <20000 \\
> > > \hline 8 & \text { 女 } & \text { 包装 } & \text { 大学 } & 30-40 & 20000-30000 \\
> > > \hline 9 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & >40 & 20000-30000 \\
> > > \hline 10 & \text { 男 } & \text { 包装 } & \text { 大学 } & 20-30 & <20000 \\
> > > \hline
> > > \end{array}$  
> > >
> > > 1. $\begin{cases}P(包装|女)\text{×}P(大学|女)\text{×}P(30\text{-}40|女)\text{×} P(20000\text{-}30000|女)\text{×}P(女)\text{=}0.0222\\\\P(包装|男)\text{×}P(大学|男)\text{×}P(30\text{-}40|男)\text{×} P(20000\text{-}30000|男)\text{×}P(男)\text{=}0.0315\end{cases}$
> > > 2. 故根据给定条件，应归类为男性
> >
> > ### $\textbf{2.3.4. KNN}$算法
> >
> > > :one:算法概念与思想
> > >
> > > 1. 基本思想：若与$x$最邻近的$k$个样本$\{x_1,x_2,...,x_n\}$大都属于$A$类别$\text{→}x_i$也属于$A$类别
> > > 2. 算法流程：选定$k$值，计算输入$x$与样本所有点的距离$\text{dist}(x,x_i)$
> > >    - 分类任务：对$k$个最邻近进行投票(如$x_i$属于$v$就为$v$类投$1$票)，选出数量最多的类作为$x$的类
> > >    - 回归任务：对$k$个最邻近取平均，作为$x$的相应值
> > > 3. 算法特点：为$\text{Lazy-Learning}$方法，即无需训练模型，待分类数据到达时马上开始分类
> > >
> > > :two:算法有关的问题
> > >
> > > 1. 关于$k$及其选择：
> > >    - 影响：$k$太小容易对噪声敏感(过拟合)，太大可能会包含太多其它类别的点
> > >    - 选择：$k$一定是奇数(避免平局)，另外可通过测试确定$k$(选择$k\text{=1,3,5...}$时使错误率最小的$k$)
> > > 2. $\text{Majority Voting}$问题
> > >    - 含义：某一类别占比太大，以至分类时总会选择到它
> > >    - 解决：加权投票，如$x_i$属于𝑣就为为$v$类投$\cfrac{1}{\text{dist}(x,x_i)^2}$票$\text{→}$使靠$x$更近的结点话语权更大
>
> ## $\textbf{2.4. }$聚类
>
> > ### $\textbf{2.4.1. }$聚类概念及概述
> >
> > > :one:聚类的概念
> > >
> > > 1. 目标：
> > >
> > >    - 把数据划为多个子集，相同子集内元素尽可能相似，不同子集尽可能相异\
> > >
> > >    - 因此聚类有赖于相似度计算$+$聚类算法(且前者更重要些)
> > >
> > > 2. 性质：无监督学习，源于分簇数目不定$+$没有分类标签
> > >
> > > :two:类别：硬聚类(一个点只能属于一个簇)，软聚类(一个点可属于多个簇)
> >
> > ### $\textbf{2.4.2. }$数据类型$\textbf{\&}$距离计算
> >
> > > :one:区间标度变量(数值变量)：变量标度为线性
> > >
> > > 1. 含义：线形标度的**连续**度量(如温度/焓/年份)，差值有意义但零值无意义
> > >
> > > 2. 标准化：$x_i\xrightarrow[\displaystyle{}平均值\text{: }m\text{=}\cfrac{1}{n} \sum_{i\text{=}1}^n x_{i}]{\displaystyle{}平均偏差\text{: }s\text{=}\cfrac{1}{n} \sum_{i\text{=}1}^n|x_{i}-m|}z_i\text{=}\cfrac{x_i-m}{s}$，注意平均偏差比标准差更鲁棒
> > >
> > > 3. 距离计算：$\text{Minkowski}$距离$\text{dist}(i,j)\text{=}\sqrt[\alpha]{\displaystyle{}\sum_{d=1}^{p}|z_{i_d}-z_{j_d}|^{\alpha}}\to\begin{cases}\alpha{}\text{=1}为\text{Manhatta}距离\\\\\alpha{}\text{=2}为\text{Euclidean}距离\end{cases}$ 
> > >
> > >    :heavy_plus_sign:比例标度型：变量的标度为非线性(如遵守$Ae^{Bt}$)，取对数后即变为线性，处理方式不变
> > >
> > > :two:布尔变量：只有$0/1$两种值
> > >
> > > 1. 布尔型列联表：对于对象$i$与对象$j$
> > >
> > >    | $i/j$ |                     $1$                     |                     $0$                     |
> > >    | :---: | :-----------------------------------------: | :-----------------------------------------: |
> > >    |  $1$  | $a$ (对象$i$值为$1/$对象$j$值为$1$的属性数) | $b$ (对象$i$值为$1/$对象$j$值为$0$的属性数) |
> > >    |  $0$  | $c$ (对象$i$值为$0/$对象$j$值为$1$的属性数) | $d$ (对象$i$值为$0/$对象$j$值为$0$的属性数) |
> > >
> > > 2. 距离计算：
> > >
> > >    |     系数类型     |                             计算                             | 含义                                  | 适用属性 |
> > >    | :--------------: | :----------------------------------------------------------: | :------------------------------------ | :------: |
> > >    |     简单匹配     | $d(i, j)\text{=}\cfrac{b\text{+}c}{a\text{+}b\text{+}c\text{+}d}$ | 所有属性中，两对象属性相异的比例      |   对称   |
> > >    | $\text{Jaccard}$ |   $d(i, j)\text{=}\cfrac{b\text{+}c}{a\text{+}b\text{+}c}$   | 排除全$0$属性后，两对象属性相异的比例 |  非对称  |
> > >
> > > 3. 距离计算示例：
> > >
> > >    | $\text{Name}$ | $\text{Gender}$ | $\text{$\text{F}$ever}$ |  $\text{Cough}$  | $\text{Test-1}$ | $\text{Test-2}$  | $\text{Test-3}$  | $\text{Test-4}$  |
> > >    | :-----------: | :-------------: | :---------------------: | :--------------: | :-------------: | :--------------: | :--------------: | :--------------: |
> > >    | $\text{Jack}$ | $\text{M(N/A)}$ |     $\text{Y(=1)}$      | $$\text{N(=0)}$$ | $\text{P(=1)}$  | $$\text{N(=0)}$$ | $$\text{N(=0)}$$ | $$\text{N(=0)}$$ |
> > >    | $\text{Mary}$ | $\text{F(N/A)}$ |     $\text{Y(=1)}$      | $$\text{N(=0)}$$ | $\text{P(=1)}$  | $$\text{N(=0)}$$ |  $\text{P(=1)}$  | $$\text{N(=0)}$$ |
> > >
> > >    - 属性：性别为对称属性将其忽略，其余为不对称属性(将有设为$1/$没有设为$2$)
> > >    - 系数：$a\text{=2}/b\text{=1/}c\text{=0}/d\text{=3}$故$\text{Jaccard}$系数为$\cfrac{1\text{+}0}{\text{2+1+0}}$ 
> > >
> > > :three:枚举(标称)变量：
> > >
> > > 1. 含义：二元变量推广为多元变量，如$\text{Color}$可取<span style="color:red;">$\text{R}$</span>/<span style="color:orange;">$\text{Y}$</span>/<span style="color:blue;">$\text{B}$</span>三种状态
> > >
> > > 2. 距离计算：
> > >    - 简单匹配：令$m$为对象$ij$匹配(相同)属性数，$p$为属性总数(维度)，则距离为$d(i, j)\text{=}\cfrac{p-m}{p}$ 
> > >    - 转化法：将枚举变成布尔，如$\text{Color}$(<span style="color:red;">$\text{R}$</span>/<span style="color:orange;">$\text{Y}$</span>/<span style="color:blue;">$\text{B}$</span>)$\xrightarrow{每个状态设一个二元变量}$是否为(<span style="color:red;">红</span>/<span style="color:orange;">黄</span>/<span style="color:blue;">蓝</span>)六种状态
> > >
> > > :four:序数型
> > >
> > > 1. 含义：
> > >
> > >    |    类型    | 含义                                         |   示例   |
> > >    | :--------: | -------------------------------------------- | :------: |
> > >    | 离散序数型 | 特殊的枚举型，只不过每个状态的排序是有意义的 |   军衔   |
> > >    | 连续序数型 | 类似于连续变量，但其无单位                   | 豆瓣评分 |
> > >
> > > 2. 距离计算：连续序数型与连续型基本一致，以下为离散序数型的
> > >
> > >    - 秩概念：对于$x_i$值域$\text{\{Value 1,Value 2,...,Value }M\}$，令其秩$r_i$值域为$\{1,2,...,M\}$ 
> > >    - 秩变换：认为$x_i\text{=Value }n\xLeftrightarrow{等价于}r_i\text{=}n$ 
> > >    - 归一化：$z_i\text{=}\cfrac{r_i-1}{M-1}$
> > >    - 距离计算：$\text{dist}(i,j)\text{=}\sqrt[\alpha]{\displaystyle{}\sum_{d=1}^{p}|z_{i_d}-z_{j_d}|^{\alpha}}$ 
> > >
> > > :five:混合计算：以下表为例
> > >
> > > | 对象 | 布尔型$1$ | 布尔型$2$ | 布尔型$3$ |   连续型$A$    |   连续型$B$    | 序数型$\Delta$(共$10$类) |
> > > | :--: | :-------: | :-------: | :-------: | :------------: | :------------: | :----------------------: |
> > > | $i$  |    $0$    |    $1$    |    $1$    | $0.12$(归一化) | $0.45$(归一化) |         第$2$类          |
> > > | $j$  |    $1$    |    $1$    |    $0$    | $0.54$(归一化) | $0.19$(归一化) |         第$5$类          |
> > >
> > > 1. 距离计算：
> > >    - 布尔型：按照规则$d_{ij}^{(f)} \text{=} \begin{cases} 
> > >      0,  \text{若 } x_{if} = x_{jf} \\ 
> > >      1,  \text{若 } x_{if} \neq x_{jf} 
> > >      \end{cases}$，令$d_{ij}^{(1)} \text{=} 1/d_{ij}^{(2)} \text{=} 0/d_{ij}^{(3)} \text{=} 1$ 
> > >    - 连续型：按照规则$d_{ij}^{(f)}\text{=} \cfrac{|x_{if} - x_{jf}|}{\max_h x_{hf} - \min_h x_{hf}}$，令$\begin{cases}d_{ij}^{(A)} \text{=} |0.12 - 0.54|\\\\d_{ij}^{(B)}\text{=}|0.45 - 0.19|\end{cases}$ 
> > >    - 序数型：获得归一化的秩$\begin{cases}z_{i, \text{序数型}} \text{=} \cfrac{2 - 1}{10 - 1}\\\\z_{j, \text{序数型}} \text{=} \cfrac{5 - 1}{10 - 1}\end{cases}$后按规则$d_{ij}^{(\Delta)} \text{=} |z_{i, \text{序数型}} - z_{j, \text{序数型}}|$得到距离
> > > 2. 合并距离：每个属性的子距离总和$/$属性数量
> >
> > ### $\textbf{2.4.3. }$聚类的方法
> >
> > > |   方法   | 概述                                                         | 特点                         |
> > > | :------: | ------------------------------------------------------------ | ---------------------------- |
> > > | 划分方法 | 将数据集划为$k$个子集，对应$k$个簇                           | 适合中小数据库的球状聚类     |
> > > | 层次方法 | <span style="color:red;">自上而下</span>/<span style="color:green;">自下而上</span>地<span style="color:red;">拆分</span>/<span style="color:green;">组合</span>数据集，来得到簇 | 适合发现嵌套关系             |
> > > | 基于密度 | 用数据点密度定义簇(高密度为簇/低密度为簇边界)                | 可过滤噪声与$\text{Outlier}$ |
> > >
> > > #### $\textbf{2.4.3.1.}$ 划分聚类方法
> > >
> > > > :one:$k\text{-Means}$算法
> > > >
> > > > 1. 算法流程：
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241115220949925.png" alt="image-20241115220949925" width=350 /> 
> > > >
> > > > 2. 终止条件：
> > > >
> > > >    - 指标：每个顶点到各自簇中心的距离平方的和，即$\displaystyle{}E\text{=}\sum_{i=1}^k \sum_{p \in C_i}\left\|p-m_i\right\|^2$
> > > >    - 标准：两次迭代之间，$E$的变化小于某个阈值
> > > >
> > > > 3. 算法分析：
> > > >
> > > >    - 复杂度：$O(nk*\text{iters})$
> > > >    - 缺点：常限于局部最优，$k$难以确定，簇的均值必须有定义，对噪声/离群点敏感
> > > >
> > > > :two:$k\text{-Medoids}$算法：以$\text{PAM}$算法为例
> > > >
> > > > 1. 算法流程：注意其中总代价$S$表示所有点到各自$\text{Medoids}$的距离之和
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/qwetarjshngbegrf.png" alt="绘图2EHAHATAR1" width=500 />  
> > > >
> > > > 2. 算法分析：复杂度为$O(k(n-k)^2)$，较$k\text{-Means}$更鲁棒
> > >
> > > #### $\textbf{2.4.3.2.}$ 层次聚类方法
> > >
> > > > :one:簇间距离的度量：令$\text{Dist = }$簇$C_i$中任一点$\xleftrightarrow{距离}$簇$C_j$中任一点
> > > >
> > > > |            方式             | 簇$C_i/C_j$距离                  | 特点                          |
> > > > | :-------------------------: | -------------------------------- | ----------------------------- |
> > > > |   $\text{Single Linkage}$   | 为$\text{Dist}$最小值(最近连接)  | ==能处理非球聚类==/对噪声敏感 |
> > > > |  $\text{Complete Linkage}$  | 为$\text{Dist}$最大值(最远连接)  | 倾向打破大的簇                |
> > > > | $\text{Centroids Distance}$ | 为二者质心间距离(中心连接)       | 适合处理均匀分布的数据        |
> > > > |   $\text{Group Average}$    | 为二者间所有点距的平均(平均连接) | 均衡了最近/最远点             |
> > > >
> > > > :two:聚类树($\text{Dendrogram}$)：$\text{AGNES}$(自下而上)/$\text{DIANA}$(自上而下)算法
> > > >
> > > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241116003914534.png" alt="image-20241116003914534" width=500 /> 
> > > >
> > > > 1. $\text{AGNES}$算法：自下而上
> > > >    - 初始化：视每点为一个单独簇
> > > >    - 合并簇：计算每两个簇间的距离$\text{→}$合并距离最近的两个簇
> > > >    - 终止：重复合并簇步骤，直到所有的样本最终被合并到一簇
> > > > 2. $\text{DIANA}$算法：自下而上
> > > >    - 初始化：视整个数据集为一个大簇
> > > >    - 簇划分：选定簇中与其他点距离最大的点(新簇种子)$\text{→}$将原簇其它点重新分配到原簇/新簇
> > > >    - 终止：重复簇划分步骤，直到每个点成为一个独立的簇
> > > >
> > > > :three:层次聚类的缺点与改进
> > > >
> > > > 1. 缺点：事件复杂度为$O(n^2)$，扩展性差，已做出的划分不可分割
> > > > 2. 改进：$\text{BIRCH/ROCK/CURE....}$ 
> > >
> > > #### $\textbf{2.4.3.3.}$ 密度聚类方法: 以$\textbf{DBScan}$为例
> > >
> > > > :one:基本要素
> > > >
> > > > 1. 密度定义有关参数：
> > > >
> > > >    |        参数         | 含义                                                         |
> > > >    | :-----------------: | ------------------------------------------------------------ |
> > > >    | $\Large\varepsilon$ | 邻域半径，用于确定某点的邻域范围(即$\varepsilon\text{-}$邻域) |
> > > >    |   $\text{MinPts}$   | 邻域内最小点数，用于判断邻域高密度与否                       |
> > > >
> > > > 2. 对点的划分：
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241116012041497.png" alt="image-20241116012041497" width=250 /> 
> > > >
> > > >    | 点类型 | 描述                                                      |
> > > >    | :----: | --------------------------------------------------------- |
> > > >    | 核心点 | 邻域内点数大于$\text{MinPts}$(包括了自己哦)               |
> > > >    | 边界点 | 邻域内点数小于$\text{MinPts}$，但其位于某个核心点的邻域内 |
> > > >    | 噪声点 | 完全不搭边的点                                            |
> > > >
> > > > 3. 一些定义：
> > > >
> > > >    |            定义            | 描述                                                         |  特性  |
> > > >    | :------------------------: | ------------------------------------------------------------ | :----: |
> > > >    | $p\xrightarrow{密度直达}q$ | $p$为核心点，且$p$的$\varepsilon\text{-}$邻域中有$q$         | 不对称 |
> > > >    | $p\xrightarrow{密度可达}q$ | 对核心点链$\{p_1\text{(=}p),p_2,p_3,...,p_n\text{(=}q)\}$有$p_i\xrightarrow{密度直达}p_{i+1}$ | 不对称 |
> > > >    | $p\xrightarrow{密度相连}q$ | 存在点$o$使得$p\xrightarrow{密度可达}o\xleftarrow{密度可达}q$ |  对称  |
> > > >
> > > > :two:算法流程
> > > >
> > > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241116021825058.png" alt="image-20241116021825058" width=500 /> 
> > > >
> > > > 1. 初始化：输入数据集$D$，超参数$\varepsilon/\text{MinPts}$ 
> > > >
> > > > 2. 主循环：遍历$D$中每个点，对未放问点$p\text{∈}D$执行以下操作
> > > >
> > > >    - 若$p$的邻域内稠密(点数$\text{≥MinPts}$)：标记被扩展的$p/q_j$为已访问
> > > >
> > > >      |   操作    | 描述                                                         |
> > > >      | :-------: | ------------------------------------------------------------ |
> > > >      |  扩展$p$  | 建立一个新簇$C\text{→}$将$p$及其邻居$\{q_1,q_2,...q_m\}$放入$C$簇 |
> > > >      | 扩展$q_j$ | 遍历邻居$\{q_1,q_2,...q_m\}\text{→}$若$q_j$为核心点$\text{→}$将$q_j$邻居中未访问点再加入$C$ |
> > > >
> > > >    - 若$p_i$的邻域内稀疏(点数$\text{<MinPts}$)：记$p$为噪声$\text{→}$标记$p$为已访问
> > > >
> > > > 3. 终止：运行主循环一直到所有样本被分簇/归类为噪声，最终输出若干簇$C_i$和噪声集$O$ 
> > > >
> > > > :three:算法示例：$\varepsilon\text{=2}/\text{MinPts=3}$ 
> > > >
> > > > $\small
> > > > \begin{array}{|c|c|c|c|c|c|c|c|c|}
> > > > \hline
> > > >           & (1, 2) & (1, 3) & (3, 1) & (2, 2) & (9, 8) & (8, 9) & (9, 9) & (18, 18) \\[-5pt]
> > > > \hline
> > > > (1, 2)     & \textcolor{red}{0.0} & \textcolor{red}{1.0} & 2.2 & \textcolor{red}{1.0} & 10.0 & 9.9 & 10.6 & 23.3 \\[-5pt]
> > > > \hline
> > > > (1, 3)     & \textcolor{red}{1.0} & \textcolor{red}{0.0} & 2.8 & \textcolor{red}{1.4} & 9.4 & 9.2 & 10.0 & 22.7 \\[-5pt]
> > > > \hline
> > > > (3, 1)     & 2.2 & 2.8 & \textcolor{red}{0.0} & \textcolor{red}{1.4} & 9.2 & 9.4 & 10.0 & 22.7 \\[-5pt]
> > > > \hline
> > > > (2, 2)     & \textcolor{red}{1.0} & \textcolor{red}{1.4} & \textcolor{red}{1.4} & \textcolor{red}{0.0} & 9.2 & 9.2 & 9.9 & 22.6 \\[-5pt]
> > > > \hline
> > > > (9, 8)     & 10.0 & 9.4 & 9.2 & 9.2 & \textcolor{red}{0.0} & \textcolor{red}{1.4} & \textcolor{red}{1.0} & 13.5 \\[-5pt]
> > > > \hline
> > > > (8, 9)     & 9.9 & 9.2 & 9.4 & 9.2 & \textcolor{red}{1.4} & \textcolor{red}{0.0} & \textcolor{red}{1.0} & 13.5 \\[-5pt]
> > > > \hline
> > > > (9, 9)     & 10.6 & 10.0 & 10.0 & 9.9 & \textcolor{red}{1.0} & \textcolor{red}{1.0} & \textcolor{red}{0.0} & 12.7 \\[-5pt]
> > > > \hline
> > > > (18, 18)   & 23.3 & 22.7 & 22.7 & 22.6 & 13.5 & 13.5 & 12.7 & \textcolor{red}{0.0} \\[-5pt]
> > > > \hline
> > > > \end{array}$  
> > > >
> > > > 1. 考察$(1,2)$：是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},(1,3),(2,2)\}$ 
> > > >    - 考察$(1,3)$：是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},\textcolor{orange}{(1,3)},(2,2)\}$ 
> > > >    - 考察$(2,2)$：是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},\textcolor{orange}{(1,3)},\textcolor{orange}{(2,2)},(3,1)\}$ 
> > > >      - 考察$(3,1)$：不是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},\textcolor{orange}{(1,3)},\textcolor{orange}{(2,2)},\textcolor{orange}{(3,1)}\}$ 
> > > > 2. 考察$(9,8)$：是核心点，簇$C_2\text{=}\{\textcolor{orange}{(9,8)},(8,9),(9,9)\}$ 
> > > >    - 考察$(8,9)$：是核心点，簇$C_2\text{=}\{\textcolor{orange}{(9,8)},\textcolor{orange}{(8,9)},(9,9)\}$ 
> > > >    - 考察$(9,9)$：是核心点，簇$C_2\text{=}\{\textcolor{orange}{(9,8)},\textcolor{orange}{(8,9)},\textcolor{orange}{(9,9)}\}$ 
> > > > 3. 考察$(18,18)$：不是核心点，直接丢到噪声集$O\text{=}\{(18,18)\}$ 
>
> ## $\textbf{2.5. }$番外: 网页数据挖掘之$\textbf{PageRank}$算法
>
> > :one:关于$\text{Web}$图
> >
> > 1. $\text{Web}$图结构：每个结点代表一个$\text{URL}$，$A\xrightarrow{权值}B$表示$A$有一定(权值)的概率访问$B$ 
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241117154518762.png" alt="image-20241117154518762" width=300 />  
> >
> > 2. $\text{Web}$图假设：
> >
> >    - 数量假设：一个结点入度越高，则该网页越重要
> >    - 质量假设：越是质量高的页面指向$A$，$A$越重要
> >
> > :two:算法描述：
> >
> > 1. 数学模型：$\displaystyle{}\text{PR}(a)_{i+1}\text{=}\cfrac{1{-}\alpha}{n}\text{+}\alpha\sum_{i=0}^n \cfrac{\text{PR}(\mathrm{Ti})_i}{\text{L(Ti)}}$，其中$\alpha$为阻尼因子
> >
> >    |            参数            | 含义                                 |
> >    | :------------------------: | :----------------------------------- |
> >    |    $\text{PR}(a)_{i+1}$    | 当前节点$a$的$\text{PR}$值           |
> >    | $\text{PR}(\mathrm{Ti})_i$ | 指向节点$a$的各个结点的$\text{PR}$值 |
> >    |       $\text{L(Ti)}$       | 指向节点$a$的各个结点的出度          |
> >
> > 2. 算法流程：
> >
> >    - 初始化：将所有结点$\text{PR}$值都设为某一值(通常为$\cfrac{1}{n}$)
> >    - 主循环：按照$\displaystyle{}\text{PR}(a)_{i+1}\text{=}\cfrac{1{-}\alpha}{n}\text{+}\alpha\sum_{i=0}^n \cfrac{\text{PR}(\mathrm{Ti})_i}{\text{L(Ti)}}$ 更新每个网页的$\text{PR}$值
> >    - 终止：一直迭代到$\text{PR}$值得不变
> >
> > 3. 矩阵化：
> >
> >    - 输入：有向图(结点个数为$n$)，初始向量$\textbf{R}_{0(n×1)}$，转移矩阵$\textbf{M}_{(n×n)}$，阻尼因子$\alpha$
> >    - 迭代：$\textbf{R}_{t+1}\text{=}\alpha\textbf{MR}_t\text{+}\cfrac{1-\alpha}{n}$，直到$\textbf{R}_t$变化不大
> >
> > :three:示例：令$\alpha\text{=}1$，很简单只展示第一轮迭代
> >
> >   <img src="https://i-blog.csdnimg.cn/blog_migrate/f74434af7fb73fdbdf710664899194b3.png" alt="在这里插入图片描述" width=200 /> $\xRightarrow{}\begin{array}{|c|c|c|c|}
> > \hline \text { Iter } & \text { PR(B) } & \text { PR(B) } & \text { PR(C) } \\
> > \hline \text { 初始化 } & \cfrac{1}{4} & \cfrac{1}{4} & \cfrac{1}{4} \\
> > \hline \text { 第一轮 } & \cfrac{3}{8} & \cfrac{1}{8} & \cfrac{3}{8} \\
> > \hline
> > \end{array}$ 

# $\textbf{3. }$自然语言理解

> ## $\textbf{3.1. NLU}$的概念与背景
>
> > :one:$\text{NLU}$与$\text{NLP}$
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241106213333767.png" alt="image-20241106213333767" width=400 /> 
> >
> > 1. 自然语言理解：
> >    - 含义：让计算机理解人类语言的结构$+$语义
> >    - 应用：信息检索/情感识别/机器翻译/拼写检查/知识图谱构建
> > 2. 自然语言处理：对自然语言的分析/理解/生成，即$\text{NLU+NLG(Generation)}$
> >
> > :two:$\text{AI-Hard}$问题
> >
> > 1. 含义：问题等同于$\text{AI}$核心的问题，即如何让计算机具有人类智能
> > 2. 典型：$\text{NLU/NLP}$，$\text{CV}$ 
> >
> > :three:$\text{NLU}$面临的挑战
> >
> > 1. 计算机的特性：善于处理明确/结构化/无歧义的语言(如编程语言)
> >
> > 2. 自然语言特性：具有复杂的上下文以及歧义性($\text{Ambiguity}$) 
> >
> >    | 歧义类型 | 含义                             | 示例                                                         |
> >    | :------: | -------------------------------- | ------------------------------------------------------------ |
> >    | 词汇歧义 | 词汇具有不同含义                 | $\text{Fuck}$可以是动词/语气词                               |
> >    | 句法歧义 | 一个句子被解析成不同的结构       | 南京市长江大桥                                               |
> >    | 语义歧义 | 句中包含了不明确的词             | <span style="color:green;">$\text{John kissed his wife, and so did Sam}$</span> |
> >    | 回指歧义 | 之前提到的词，在后面句子含义不同 | 小李告诉小王<span style="color:red;">**他**</span>生了       |
> >    | 语用歧义 | 短语/句子不同语境下含义不同      | 可以站起来吗 (询问能力$\text{or}$请求)                       |
>
> ## $\textbf{3.2. NLU}$的主要任务
>
> > ### $\textbf{3.2.1. NLU}$的==语法==任务
> >
> > > #### $\textbf{3.2.1.1. }$词汇层面的任务
> > >
> > > > :one:词干抽取($\text{Stemming}$)
> > > >
> > > > 1. 含义：抽取词的词干($\text{Stem}$)与词根($\text{root}$)，比如$\text{Niubilitiness}\to\begin{cases}\text{词干: Niubility}\\\\词根\text{: Niubi}\end{cases}$ 
> > > >
> > > > 2. 处理方法：
> > > >
> > > >    |     方法     | 含义                                      | 限制               |
> > > >    | :----------: | ----------------------------------------- | ------------------ |
> > > >    | 利用形态规则 | 机械地去处所有后缀，如$\text{-ing/-tion}$ | 不规则变形词不适用 |
> > > >    |   基于词典   | 按照词典中的映射还原词性                  | 受限于词典规模     |
> > > >    |   高级方法   | $\text{n-gram}$法/隐马可夫/机器学习       | 受限于语料库大小   |
> > > >
> > > > :two:词形还原($\text{Lemmatization}$) 
> > > >
> > > > 1. 含义：将不同形式词汇还原为词目($\text{Lemma}$)，如$\text{am, is, are, was, were→be}$ 
> > > > 2. 对比：词干抽取==完全不考虑上下文==，词形还原==考虑一定的上下文==
> > > >    - 示例：$\text{We are meeting in the zoom meeting}\xrightarrow[词形还原]{词干抽取}\begin{cases}\text{词干抽取: meet}\\\\\text{词形还原: meet+meeting}\text{}\end{cases}$ 
> > > > 3. 处理方法：
> > > >    - 基于规则：人工给予的语言学规则，或者机器学习训练出来的规则
> > > >    - 基于词典：受限于词典，只适用于简单语言
> > > >
> > > >   :bulb:词形还原/词干抽取并非$\text{100\%}$必要，比如细颗粒度情感分析就需要高精度文本(时态/复数等)
> > > >
> > > > :three:词性标注($\text{Part-of-speech tagging}$)
> > > >
> > > > 1. 含义：为文本中每个词标记词性(名词/动词/形容词)
> > > > 2. 方法：基于规则(人工)，基于隐马可夫模型($\text{HMM}$)，基于机器学习(SVM/神经网络)
> > > > 3. 挑战：分词$\text{\&}$词义多义性
> > > >
> > > > :four:术语抽取($\text{Terminology extraction}$)
> > > >
> > > > 1. 含义：信息抽取的子任务，识别文本中特定领域的专门术语
> > > > 2. 方法：机器学习，统计($\text{TF/IDF}$)，外部知识库
> > > > 3. 挑战：新术语/跨领域术语
> > >
> > > #### $\textbf{3.2.1.2. }$句法层面的任务
> > >
> > > > :one:句法分析($\text{Parsing}$)
> > > >
> > > > 1. 含义：分析文本中单词/短语之间的句法关系  
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107144233277.png" alt="image-20241107144233277" width=450 />  
> > > >
> > > > 2. 方法：统计学(概率上下文无关文法/最大化信息熵的原则)，机器学习方法($\text{RNN}$)
> > > >
> > > > :two:断句($\text{Sentence breaking}$)
> > > >
> > > > 1. 含义：句子边界消歧(例如$\text{ . }$可表示短句/缩写/小数点等)$+$句子分割
> > > > 2. 方法：基于最大熵，神经网络
> > > >
> > > > :three:分词($\text{Word segmentation}$)
> > > >
> > > > 1. 含义：==仅对词汇间没有明显边界的语言(中文)==而言，将连续字符分割为有意义单词
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107163154407.png" alt="image-20241107163154407" width=400 /> 
> > > >
> > > > 2. 方法：基于字典(正逆向匹配)，基于统计($\text{HMM/SVM}$)
> > > >
> > > > 3. 难处：未登录词/切分歧义
> >
> > ### $\textbf{3.2.2. NLU}$的==语义==任务
> >
> > > #### $\textbf{3.2.2.1. }$文本生成/转换
> > >
> > > > :one:机器翻译
> > > >
> > > > 1. 含义：将文本/语音从一种语言翻译到另一种语言  
> > > > 2. 方法：基于规则(源/目标语言形态语法等)，基于统计(用大型语料库构建概率模型)，神经网络
> > > > 3. 难题：词句歧义/对语料库大小强依赖/低频词句/长句子
> > > >
> > > > :two:问答与对话
> > > >
> > > > 1. 含义：实现自然语言形式的人机交互
> > > > 2. 分类：
> > > >    - $\text{5W1H}$类问题：即$\text{Who/What/When/Where/Why \& How}$ 
> > > >    - $\text{Open/Closed-domain}$类：回答可以没有限制 $\text{or}$ 专注于某一领域
> > > > 3. 方法：检索法(从库中抽取语料回答)，生成法(检索$+$推理)，$\text{Pipeline}$，$\text{Seq2Seq}$ 
> > > > 4. 难题：多知识约束/多轮对话/多模态/可解释性.....
> > > >
> > > > :three:自动文摘
> > > >
> > > > 1. 含义：为文档生成一段==包含原文档要点==的**压缩文档**，例如搜索引擎结果
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107174326216.png" alt="image-20241107174326216" width=600  /> 
> > > >
> > > > 2. 方法：对要点进行不修改的抽取，对要点概括(修改/复述)
> > > >
> > > > 3. 难题：难以评估，可理解性问题，需要背景知识
> > >
> > > #### $\textbf{3.2.2.2. }$文本信息提取
> > >
> > > > :one:命名实体识别($\text{NER, Named entity recognition}$) 
> > > >
> > > > 1. 命名实体：现实世界中的某个对象，如$\text{\textcolor{red}{Obama} is the \textcolor{red}{president} of \textcolor{red}{the United States}}$ 
> > > > 2. $\text{NER}$：信息提取的子任务，识别文本中所有实体$+$分配到特定类别(名字/时间/数量)
> > > > 3. 方法：语法规则(效果好但需要大量人工规则)，统计方法(需要标注大量数据)
> > > > 4. 难题：领域依赖性(如医学实体/术语)，实体类型多样
> > > >
> > > > :two:关系抽取
> > > >
> > > > 1. 含义：检测文本中实体的==语义关系==，并将各种关系分类
> > > > 2. 方法：结合了领域知识的机器学习
> > > > 3. 难点：训练集难以构建，自然语言的歧义
> > >
> > > #### $\textbf{3.2.2.3. }$文本内容分析
> > >
> > > > :one:文本分类
> > > >
> > > > 1. 含义：自动将文本划分到预定类中，比如垃圾邮件过滤/情感识别/黄色内容识别
> > > > 2. 方法：特征提取$\to$训练分类器(朴素贝叶斯/$\text{KNN}$/$\text{SVM}$)
> > > > 3. 难题：特征难提取(需要大量标注)，数据非平衡问题
> > > >
> > > > :two:情感分析
> > > >
> > > > 1. 含义：识别文本中的情感状态，主观评价等
> > > > 2. 方法：情感词库($\text{Happy/Fucking}$)，统计方法($\text{SVM/}$潜在语义分析)
> > > > 3. 难题：修辞的多样(反语/讽刺)，分面观点(即将复杂事物分解为不同方面)
> > > >
> > > > :three:主题分割
> > > >
> > > > 1. 含义：将单个长文本分为多个较短的，主题一致的片段
> > > > 2. 方法：
> > > >    - 基于内容变化：同一主题内容有高度相似性$\to$通过聚类
> > > >    - 基于边界特性：主题切换时会有边界(如过渡性/总结性文本)
> > > > 3. 难点：任务目标模糊(主题多样)，无关信息干扰，歧义性
> > >
> > > #### $\textbf{3.2.2.4. }$文本歧义消解
> > >
> > > > :one:词义消歧
> > > >
> > > > 1. 含义：确定一词多义词的含义
> > > > 2. 方法：基于词典(叙词表/词汇知识库)，基于机器学习(小语料库的半监督学习/标注后的监督学习)
> > > > 3. 难题：词义的离散型(一个词的不同含义可能完全不搭边)，需要常识
> > > >
> > > > :two:共指消解  
> > > >
> > > > 1. 含义：识别文本中表示同一个事物的不同代称
> > > > 2. 示例：<span style="color:orange;">甲队</span>打败了<span style="color:orange;">乙队</span>，<span style="color:red;">他们</span>更强$\xrightarrow{消解后}$虽然<span style="color:orange;">甲队</span>打败了<span style="color:orange;">乙队</span>，但<span style="color:red;">他们</span>更强 
> > > > 3. 方法：
> > > >    - 启发式：如最接近语法兼容词，即在代词前寻找==最近的$+$语法匹配的==词
> > > >    - 基于$\text{ML}$：如$\text{Mention-Pair Models / Mention-Ranking Models}$ 
> > > > 4. 难题：如何应用背景知识，歧义(哪哪都有它，考试的万金油解答嘿嘿)
>
> ## $\textbf{3.3. }$自然语言的统计特性
>
> > ### $\textbf{3.3.1. Zipf}$定律
> >
> > > :one:$\text{Zipf}$定律
> > >
> > > 1. 内容：令出现频率第$r$高的词汇出现频率为$f(r)$，则有$f(r)\text{=}\cfrac{\text{Const}}{r^{s}}$其中$s\text{≈}1$
> > >
> > > 2. 含义：对于词频分布，最常见词的分布极为普遍$+$大多数词出现频率极低
> > >
> > > 3. 解释：
> > >
> > >    |   解释模型   | 含义                                                         |
> > >    | :----------: | ------------------------------------------------------------ |
> > >    |  米勒猴实验  | 胡乱生成的带有字母$+$空格的序列，词频和排名也符合幂律关系    |
> > >    | 最小努力原则 | 通过词频差异最小化交流的成本                                 |
> > >    | 优先连接机制 | 网络结构中，新节点倾向于连接度数更大的点，与$\text{Zipf}$类似 |
> > >
> > > :two:$\text{Zipf}$定律的实验
> > >
> > > 1. 符合程度：$f(r)\text{=}\cfrac{\text{Const}}{r^{s}}\to{}\log{f(r)\text{=}}\log{C}-s\log{r}$故可通过检测后者线性程度
> > >
> > > 2. 实验结论：幂律分布很常见$+$排名靠中间的术语会更符合
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107220033751.png" alt="image-20241107220033751" width=570 />  
> > >
> > > :three:$\text{Zipf}$定律与索引
> > >
> > > 0. 倒排索引：用于快速全文检索的数据结构，示例如下
> > >
> > >    - 文档
> > >
> > >      ```txt
> > >      Doc1: fat cat rat rat 
> > >      Doc2: fat cat 
> > >      Doc3: fat
> > >      ```
> > >
> > >     - 构建的倒排索引
> > >
> > >       ```txt
> > >       fat: Doc1 Doc2 Doc3
> > >       cat: Doc1 Doc2
> > >       rat: Doc1
> > >       ```
> > >
> > >
> > > 1. 词频太高/太低的词都不适合索引，会导致返回太多/太少的文档，适中的才最有价值
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241107221536310.png" alt="image-20241107221536310" width=320 /> 
> > >
> > > 2. 基于$\text{Zipf}$定律，去处高频$\text{Stopword}$能优化倒排索引时空开销，如下为倒排索引的一个实例
> >
> > ### $\textbf{3.3.2. Heaps}$定律
> >
> > > :one:$\text{Heaps}$定律
> > >
> > > 1. 内容：词汇表大小$V$与文本词数$n$满足$V\text{=}Kn^{\beta}$ 
> > > 2. 参数：$10\text{≤}K\text{≤}100$且$0.4\text{≤}\beta\text{≤}0.6$，当$K\text{=}44$与$\beta\text{=}0.49$最匹配
> > >
> > > :two:用途：预测随文本增长词汇表$\&$倒排索引大小的变化
> >
> > ### $\textbf{3.3.3. Benford}$定律(第一数字法则)
> >
> > > :one:$\text{Benford}$定律
> > >
> > > 1. 背景：在许多社会现象中，数据首位数往往分布不均(为$1$概率最大$\xrightarrow{依次递减}$为$9$概率最小)
> > > 2. 定律：令数据集中$d$作为首字母的概率$P(d)=\lg{\left(1+\cfrac{1}{d}\right)}$，$d\text{＞}9$及非十进制时依旧适用
> > >
> > > :two:对$\text{Benford}$定律的一些思考
> > >
> > > 1. 适用：跨数量级变化的数据集，如财务数据和自然现象
> > > 2. 应用：检测数据造假、异常值、验证财务报告真实性
> > > 3. 成因：还不具备完全的可解释性，大概是因为数据==在对数尺的分布==  
>
> ## $\textbf{3.4. }$词袋语言模型
>
> > ### $\textbf{3.4.1. BoW}$模型
> >
> > > :one:基本步骤：以句$\text{I love machine learning}$以及$\text{Machine learning is fun}$为例
> > >
> > > |  步骤  | 示例                                                         |
> > > | :----: | ------------------------------------------------------------ |
> > > |  分词  | $\text{I \\ love \\ machine \\ learning \\ }\text{Machine \\ learning \\ is \\ fun}$ |
> > > |  建表  | $V\text{=[I, love, machine,lerning, is, fun]}$               |
> > > | 向量化 | 第一句变为$A_1\text{=[1,1,1,1,0,0]}$第二局变为$A_2\text{=[0,0,1,1,1,1]}$ |
> > >
> > > :two:特点
> > >
> > > 1. 原理上：完全忽略了语法/词序，默认词与词间的概率分布独立
> > > 2. 效果上：
> > >    - 优点：实现极其简单，但高效且应用广泛
> > >    - 缺点：无法区分$\text{\&}$一义多词，如同义词替换后的两文档相似度低于实际值
> >
> > ### $\textbf{3.4.2. TF-IDF}$模型
> >
> > > :one:$\text{TF-IDF}$值
> > >
> > > 1. 计算: $\text{TF-IDF}(t,d)\text{=TF}(t,d)\text{×IDF(}t)\text{→}\begin{cases}词频\text{TF}(t,d)=\cfrac{词t在文档d出现次数}{文档d总词数}\\\\逆文档频\text{IDF(t)=}\log\cfrac{文档总数}{\text{DF}(t)(包含t的文档数)\text{+}1}\end{cases}$  
> > > 2. 含义: $\text{TF-IDF}(t,d)$越高，代表词$t$对文档$d$越重要
> > >
> > > :two:$\text{TF-IDF}$值改进：原始词频值往往不是所需的
> > >
> > > 1. 对原始词频$\text{TF}(t,d)$的改进
> > >
> > >    | 词频类型 |                             公式                             | 意义                                         |
> > >    | :------: | :----------------------------------------------------------: | -------------------------------------------- |
> > >    |   对数   |               $1\text{+}\log (\text{TF}(t,d))$               | 压缩较高词频，减少其对相关性影响的夸大       |
> > >    |   增强   | $\displaystyle{}0.5\text{+}\cfrac{0.5 \text{×}\text{TF}(t,d)}{\max _{\mathrm{t}}\text{TF}(t,d)}$ | 映射词频到$0.5\text{→1}$，防止高频词权重过大 |
> > >    |   布尔   | $\begin{cases}1 \,\text{ if  } \text{TF}(t,d)>0 \\0  \,\text{  otherwise }\end{cases}$ | 不关注具体的词频值，仅表示是否出现           |
> > >    | 平均对数 | $\cfrac{1+\log \left(\text{TF}(t,d)\right)}{1+\log \left(\mathrm{ave}_{\mathrm{t∈d}}, \left(\text{TF}(t,d)\right)\right)}$ | 使词频高的词与低的词之间的差距不会过大       |
> > >
> > > 2. 对文档频率$\text{DF}(t)$的改进：$N$是文档总数
> > >
> > >    |     文档频率$\text{DF}(t)$     | 公式                                                         | 意义                       |
> > >    | :----------------------------: | :----------------------------------------------------------- | -------------------------- |
> > >    |   逆文档频率$\text{IDF}(t)$    | 即$\log{}\cfrac{N}{\text{DF}(t)}$者$\log{}\cfrac{N}{\text{DF}(t)\text{+1}}$ | 衡量词在文档集合中的稀有性 |
> > >    | 概率文档频率$\text{ProbDF(}t)$ | $\max\left\{0,\log\cfrac{N-\text{DF}(t)}{\text{DF}(t)}\right\}$ | 通过概率角度评估词的稀有性 |
> > >
> > > 
> > >
> > > 3. 归一化：对于$\textbf{TF-IDF}\text{=}\begin{bmatrix}
> > >    \text{TF-IDF}(t_1,d_1) & \text{TF-IDF}(t_1,d_2) & \cdots & \text{TF-IDF}(t_1,d_n) \\
> > >    \text{TF-IDF}(t_2,d_1) & \text{TF-IDF}(t_2,d_2) & \cdots & \text{TF-IDF}(t_2,d_n) \\
> > >    \vdots  & \vdots  & \ddots & \vdots  \\
> > >    \text{TF-IDF}(t_m,d_1) & \text{TF-IDF}(t_m,d_2) & \cdots & \text{TF-IDF}(t_m,d_n) \\
> > >    \end{bmatrix}$ 
> > >
> > >    | 归一类型 | 公式                                                         | 意义                                      |
> > >    | :------: | ------------------------------------------------------------ | ----------------------------------------- |
> > >    | 余弦归一 | $\textbf{TF-IDF}\text{×}\cfrac{1}{\displaystyle{}\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}[\text{TF-IDF}(t_i,d_j)]^{2}}}$ | 用于计算文档间的余弦相似度                |
> > >    | 基准归一 | $\textbf{TF-IDF}\text{×}\cfrac{1}{mn}$                       | 消除文档集合大小对权重的影响              |
> > >    | 字长归一 | $\textbf{TF-IDF}\text{×}\cfrac{1}{\text{(CharLen)}^{\alpha}}$ | 适用于不同长度的文档，且$\alpha\text{<}1$ |
> > >
> > > :three:基于$\text{TF-IDF}$的余弦相似度
> > >
> > > 1. $\text{TF-IDF}$值：对于文档$d_1$和$d_2$，词汇表长为$m$
> > >    - $\textbf{TF-IDF}\text{=}\begin{bmatrix}
> > >      \text{TF-IDF}(t_1,d_1)&\text{TF-IDF}(t_1,d_2)\\
> > >      \text{TF-IDF}(t_2,d_1)&\text{TF-IDF}(t_2,d_2)\\
> > >      \vdots  & \vdots  \\
> > >      \text{TF-IDF}(t_m,d_1) & \text{TF-IDF}(t_m,d_2)\\
> > >      \end{bmatrix}\xrightarrow{余弦归一化}\begin{bmatrix}
> > >        \text{tf-idf}(t_1,d_1)&\text{tf-idf}(t_1,d_2)\\
> > >        \text{tf-idf}(t_2,d_1)&\text{tf-idf}(t_2,d_2)\\
> > >        \vdots  & \vdots  \\
> > >        \text{tf-idf}(t_m,d_1) & \text{tf-idf}(t_m,d_2)\\
> > >      \end{bmatrix}$  
> > > 2. 两文档余弦值：
> > >    - 未归一化表示：$\text{sim}(d_1, d_2) = \cfrac{\displaystyle{}\sum_{j=1}^m \text{TF-IDF}(t_j, d_1) \cdot \text{TF-IDF}(t_j, d_2)}{\displaystyle{}\sqrt{\sum_{j=1}^m (\text{TF-IDF}(t_j, d_1))^2} \cdot \sqrt{\sum_{j=1}^m (\text{TF-IDF}(t_j, d_2))^2}}$
> > >    - 归一化表示 ：$\displaystyle{}\text{sim}(d_1, d_2) = \sum_{j=1}^m \text{tf-idf}(t_j, d_1) \cdot \text{tf-idf}(t_j, d_2)$ 
>
> ## $\textbf{3.5 }$主题语言模型
>
> > ### $\textbf{3.5.0. }$概述
> >
> > > :one:分布模型：$\text{Doc.}\xrightarrow[非监督学习(聚类)]{主题分布}
> > > \begin{cases}
> > > \textbf{Topic 1}(P_{T_1})\xrightarrow[非监督学习(聚类)]{词语分布}
> > > \begin{cases}
> > > \text{Word}_{11}(P_{W_{11}})\\
> > > \text{Word}_{12}(P_{W_{11}})\\
> > > \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots    \\
> > > \text{Word}_{1m}(P_{W_{1m}})\\
> > > \end{cases}
> > > \\
> > > \textbf{Topic 2}(P_{T_2})\xrightarrow[非监督学习(聚类)]{词语分布}
> > > \begin{cases}
> > > \text{Word}_{21}(P_{W_{21}})\\
> > > \text{Word}_{22}(P_{W_{21}})\\
> > > \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots    \\
> > > \text{Word}_{2m}(P_{W_{2m}})\\
> > > \end{cases}
> > > \\
> > > \,\,\,\,\,\,\,\,\,\,\,\,\,\vdots
> > > \\
> > > \textbf{Topic n}(P_{T_n})\xrightarrow[非监督学习(聚类)]{词语分布}
> > > \begin{cases}
> > > \text{Word}_{n1}(P_{W_{n1}})\\
> > > \text{Word}_{n2}(P_{W_{n1}})\\
> > > \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots    \\
> > > \text{Word}_{nm}(P_{W_{nm}})\\
> > > \end{cases}
> > > \end{cases}$ 
> > >
> > > 1. 主题分布：每篇文档由若干主题按一定比例构成
> > > 2. 词语分布：每个主题包含一组特定的词语，每个词具有不同的出现概率
> > >
> > > :two:概率模型
> > >
> > > 1. 公式：$\displaystyle{}p(w | \mathrm{Doc})=\sum_{i=1}^n p\left(w | T_i\right) \cdot p\left(T_i | \mathrm{Doc}\right)$ 
> > >
> > > 2. 含义：将文档的内容视为不同主题的组合$\to$由每主题的词语概率预测文档中词语的分布
> >
> > ### $\textbf{3.5.1. }$基于矩阵分解的模型
> >
> > > #### $\textbf{3.5.1.1. LSA(SVD)}$模型
> > >
> > > > :one:奇异值分解
> > > >
> > > > 1. 含义：对任意$A_{m\text{×}n}$可将其分解为三个矩阵$A_{m\text{×}n}\text{=}U_{m\text{×}m}\Sigma_{m\text{×}n}V_{n\text{×}n}^{T}\text{}$
> > > >
> > > >    |            矩阵类型             | 描述                                                         |
> > > >    | :-----------------------------: | ------------------------------------------------------------ |
> > > >    |   左奇异矩阵$U_{m\text{×}m}$    | 为正交矩阵即$U_{m\text{×}m}U_{m\text{×}m}^{T}\text{=}I_{m\text{×}m}$ |
> > > >    | 奇异值矩阵$\Sigma_{m\text{×}n}$ | 为对角矩阵(对角为是奇异值)，如$\small\begin{bmatrix}\alpha_1 & 0 & 0 & \cdots & 0& \cdots & 0 \\0 & \alpha_2 & 0 & \cdots & 0& \cdots & 0 \\0 & 0 & \alpha_3 & \cdots & 0& \cdots & 0 \\\vdots & \vdots & \vdots & \ddots & \vdots & &\vdots\\0 & 0 & 0 & \cdots & \alpha_m& \cdots & 0 \\\end{bmatrix}_{m \text{×} n}$ |
> > > >    |   右奇异矩阵$V_{n\text{×}n}$    | 为正交矩阵即$V_{n\text{×}n}V_{n\text{×}n}^{T}\text{=}I_{n\text{×}n}$ |
> > > >
> > > > 2. $\text{Eckart–Young–Mirsky}$定理：$A_k=U_k \Sigma_k V_k^T$奇异值的截断
> > > >
> > > >    - $U_k$ 和 $V_k$ 分别是 $U$ 和 $V$ 的前 $k$ 列
> > > >    - $\Sigma_k$ 是奇异值矩阵 $\Sigma$ 中前 $k$ 个最大的奇异值组成的 $k\text{×}k$ 子矩阵
> > > >
> > > > :two:$\text{LSA}$模型步骤：原始$\text{Word-Doc}$矩阵$\xrightarrow[近似]{奇异分解}$其近似的低阶矩阵
> > > >
> > > > 1. $\text{Word-Doc}$矩阵：
> > > >
> > > >    - $A_{t \text{×} d} = \begin{bmatrix}
> > > >      \text{Doc}_1 \text{→} \text{Word}_{11} & \text{Doc}_2 \text{→} \text{Word}_{12} & \cdots & \text{Doc}_n \text{→} \text{Word}_{1d} \\
> > > >      \text{Doc}_1 \text{→} \text{Word}_{21} & \text{Doc}_2 \text{→} \text{Word}_{22} & \cdots & \text{Doc}_n \text{→} \text{Word}_{2d} \\
> > > >      \vdots & \vdots & \ddots & \vdots \\
> > > >      \text{Doc}_1 \text{→} \text{Word}_{t1} & \text{Doc}_2 \text{→} \text{Word}_{t2} & \cdots & \text{Doc}_n \text{→} \text{Word}_{td} \\
> > > >      \end{bmatrix}$
> > > >
> > > >    - $\text{Doc}_i\text{→Word}_{ij}$可为词$\text{Word}_{ij}$的词频或者$\text{TF-IDF}$值
> > > >
> > > > 2. $A_{t\text{×}d}$奇异分解：$A_{t\text{×}d}\text{=}T_{t\text{×}n}S_{n\text{×}n}D_{d\text{×}n}^{T}\text{}$ 
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109022707515.png" alt="image-20241109022707515" width=580 /> 
> > > >
> > > >    |     矩阵类型     | 描述                                           |
> > > >    | :--------------: | ---------------------------------------------- |
> > > >    | $S_{n\text{×}n}$ | 奇异值按降序排列，代表重要的==潜在语义的强度== |
> > > >    | $T_{t\text{×}n}$ | 词汇矩阵，每列蕴含一个隐含概念(主题)           |
> > > >    | $D_{d\text{×}n}$ | 文档矩阵，每列蕴含一个隐含概念(主题)           |
> > > >
> > > > 3. 低秩近似： $A\text{→}A_k$ 
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片tsrhy4.png" alt="图片dq121" width=580 />   
> > > >
> > > >    - 降维： $S_{n\text{×}n}\xrightarrow{只保留前k个最大的奇异值}S_{k\text{×}k}$，其中$k$又称为==预期主题数==
> > > >
> > > >    - 降噪：$A_{t\text{×}d}\text{=}T_{t\text{×}n}S_{n\text{×}n}D_{d\text{×}n}^{T}\text{}\xrightarrow{S_{n\text{×}n}降维}A_{t\text{×}d}\text{=}T_{t\text{×}k}S_{k\text{×}k}D_{d\text{×}k}^{T}\text{}$，滤掉不重要的主题
> > > >
> > > > :three:文档与词汇的表示
> > > >
> > > > 1. 词汇：$T_{t\text{×}k}S_{k\text{×}k}$的行向量，且$\hat{w}_n=u_n\text{×}\textbf{S}$
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片ssddddddddd8.png" alt="图dsdff片5"  width=500 />  
> > > >
> > > > 2. 文档：$D_{d\text{×}k}S_{k\text{×}k}$的行向量($S_{k\text{×}k}D_{d\text{×}k}^T$的列向量)，且$\hat{d}_m\text{=}\textbf{S}\text{×}v_{m}^{T}$ 
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片dfdfdfdfdw7.png" alt="图片sdsdsdsdsdsdsdssd6" width=500 />  
> > >
> > > #### $\textbf{3.5.1.2. MNF}$建模
> > >
> > > > :one:建模过程
> > > >
> > > > 1. 对$\textbf{V}$寻找非负矩阵$\textbf{HW}$使$\textbf{V}\text{≈}\textbf{WH}$
> > > >
> > > > 2. 使得代价函数$\displaystyle{}\|V-W H\|\text{=}\sqrt{\sum_{i, j}\left(V_{i, j}-(W H)_{i, j}\right)^2}$ 尽可能小
> > > >
> > > > :two:建模的意义
> > > >
> > > > 1. 非负：使分解结果更有意义
> > > > 2. 示例：**文档-单词**$\xrightarrow{\text{NMF}}$**文档-主题**$\text{×}$**主题-单词**
> >
> > ### $\textbf{3.5.2. }$基于概率的模型
> >
> > > #### $\textbf{3.5.2.0. }$概率模型概述
> > >
> > > > :one:符号：其中$K$为话题数，$K\text{≪}M$且为预先定义的超参数
> > > >
> > > > |                集合                 | 含义                                      |   随机变量    |
> > > > | :---------------------------------: | ----------------------------------------- | :-----------: |
> > > > | 文本集$D\text{=}\{d_1,d_2,...d_N\}$ | 包含所有文本，$N$为文本总数               | $d$(观测变量) |
> > > > | 话题集$Z\text{=}\{z_1,z_2,...z_K\}$ | 包含所有可能的话题，$K$为==预设==话题总数 | $z$(隐藏变量) |
> > > > | 词汇集$W\text{=}\{w_1,w_2,...w_M\}$ | 所有可能的单词，$M$为单词总数             | $w$(观测变量) |
> > > >
> > > > :two:三类分布：$P(d)$为可观测参数，如何估计$P(z|d)$和$P(w|z)$两参数派生了$\text{pLAS}$和$\text{LDA}$方法
> > > >
> > > > |   分布   | 表示                   | 含义                                             |
> > > > | :------: | :--------------------- | ------------------------------------------------ |
> > > > | 文档分布 | $P(d)\sim{}$多项分布   | 生成文本$d$的概率                                |
> > > > | 主题分布 | $P(z|d)\sim{}$多项分布 | 文本$d$生成话题$z$的概率，每个文本都有其主题分布 |
> > > > | 单词分布 | $P(w|z)\sim{}$多项分布 | 话题$z$生成单词$w$的概率，每个主题都有其单词分布 |
> > > >
> > > > :three:观测表征
> > > >
> > > > 1. 观测数据：文本-单词共现矩阵，其中$n($单词$i,$ 文本$j)$表示单词$i$在文本$j$中出现的次数
> > > >
> > > >    | 共现矩阵$T$  |        文$d_1$        |        文$d_2$        | $\text{...}$ |        文$d_N$        |
> > > >    | :----------: | :-------------------: | :-------------------: | :----------: | :-------------------: |
> > > >    |   词$w_1$    | $n($词$w_1,$ 文$d_1)$ | $n($词$w_1,$ 文$d_2)$ | $\text{...}$ | $n($词$w_1,$ 文$d_N)$ |
> > > >    |   词$w_2$    | $n($词$w_2,$ 文$d_1)$ | $n($词$w_2,$ 文$d_2)$ | $\text{...}$ | $n($词$w_2,$ 文$d_N)$ |
> > > >    | $\text{...}$ |     $\text{...}$      |     $\text{...}$      | $\text{...}$ |                       |
> > > >    |   词$w_M$    | $n($词$w_M,$ 文$d_1)$ | $n($词$w_M,$ 文$d_2)$ | $\text{...}$ | $n($词$w_M,$ 文$d_N)$ |
> > > >
> > > > 2. 生成概率：假设每个单词分布独立，则有$\displaystyle{}P(T)\text{=}\prod_{(w, d)} P(w, d)^{n(w, d)}$ 
> > > >
> > > > :four:$\text{LDA}$与$\text{pLSA}$  
> > > >
> > > > |     模型      |  思想  | 对于两$P(z\mid{}d)$和$P(w\mid{}z)$待估参数                   |
> > > > | :-----------: | :----: | ------------------------------------------------------------ |
> > > > | $\text{pLSA}$ | 频率学 | 视作固定值(即均匀分布)，用最大似然估计解出来                 |
> > > > | $\text{LDA}$  | 贝叶斯 | 视作服从$\text{Dirichlet}$分布的随机变量，先验分布$\xrightarrow{修正}$最终分布 |
> > >
> > > #### $\textbf{3.5.2.1 pLSA}$模型
> > >
> > > > :one:生成模型：
> > > >
> > > > 1. 定义：对生成概率$\displaystyle{}P(w, d)\text{=}P(d) \sum_z P(z | d) P(w | z)$形式的拆解
> > > >
> > > > 2. 概率依赖：文本$\text{→}$话题$\text{→}$单词
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109184747859.png" alt="image-20241109170227089" width=300 />    
> > > >
> > > >    | 选择 | 描述                                                         | 备注               |
> > > >    | :--: | ------------------------------------------------------------ | ------------------ |
> > > >    | 文本 | 从$D$中，按$P(d)$选择文本$d$$\xrightarrow{重复N次}$生成$N$个文本 | $N$为文本数量      |
> > > >    | 话题 | 对每个文本，按$P(z|d)$选择话题$z$$\xrightarrow{重复L次}$生成$L$个话题 | $L$为文本(定/变)长 |
> > > >    | 单词 | 对每个话题，按$P(w|z)$选择一单词                             | $\text{N/A}$       |
> > > >
> > > > :two:共现模型：
> > > >
> > > > 1. 定义：对生成概率$\displaystyle{}P(w, d)\text{=}\sum_{z \text{∈} Z} P(z) P(w | z) P(d | z)$形式的拆解
> > > >
> > > > 2. 概率依赖：话题$\text{→}$单词，话题$\text{→}$文本
> > > >
> > > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109184707676.png" alt="image-20241109184707676" width=270 /> 
> > > >
> > > >    | 选择 | 描述                                                         | 备注                |
> > > >    | :--: | ------------------------------------------------------------ | ------------------- |
> > > >    | 话题 | 从$Z$中，按$P(z)$选择话题$z$$\xrightarrow{重复L次}$生成$L$个话题 | $L$为文本(定/变)长  |
> > > >    | 单词 | 对每个话题，按$P(w|z)$选择一单词                             | 单词/文本的选择独立 |
> > > >    | 文本 | 从$D$中，按$P(d|z)$选择文本$d$$\xrightarrow{重复N次}$生成$N$个文本 | $N$为文本数量       |
> > >
> > > #### $\textbf{3.5.2.2. LDA}$模型简述
> > >
> > > > :anger:别看$\text{PPT}$了那就是一坨屎，以下内容来自维基百科
> > > >
> > > > :one:$\text{LDA}$模型要素 
> > > >
> > > > 1. 三种分布：
> > > >
> > > >    |        分布        |          维度          |                   元素                   | 隐藏/观测 |
> > > >    | :----------------: | :--------------------: | :--------------------------------------: | :-------: |
> > > >    |  主题分布$\Theta$  | 文档数$\text{×}$主题数 |  $\theta_{i,j}$为文档$i$中主题$j$的比例  |   隐藏    |
> > > >    |  词汇分布$\beta$   | 主题数$\text{×}$词汇数 |  $\beta_{i,j}$为主题$i$中词汇$j$的频次   |   隐藏    |
> > > >    | 主题分布$\text{w}$ | 文档数$\text{×}$词汇数 | $\text{w}_{i,j}$为文档$i$中词汇$j$的频次 |   观测    |
> > > >
> > > > 2. 两种超参数：
> > > >
> > > >    |  超参数  | 描述                                     | 功能                   |
> > > >    | :------: | ---------------------------------------- | ---------------------- |
> > > >    | $\alpha$ | 文档集级参数，$\text{Dirichlet}$分布参数 | 生成文档的主题$\Theta$ |
> > > >    |  $\eta$  | 文档集级参数，$\text{Dirichlet}$分布参数 | 生成每个主题的$\beta$  |
> > > >
> > > > :two:$\text{LDA}$的生成：分布$\displaystyle{}p\left(w_i, z_i, \theta_i, \Phi \mid \alpha, \beta\right)=\prod_{j=1}^N p\left(\theta_i \mid \alpha\right) p\left(z_{i, j} \mid \theta_i\right) p(\Phi \mid \beta) p\left(w_{i, j} \mid \phi_{z_{i, j}}\right)$ 
> > > >
> > > > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109234613737.png" alt="image-20241109234613737" width=400 /> 
> > > >
> > > > 1. 第一部分：
> > > >    - 从先验$\text{Dirichlet}$分布$\alpha$中抽样$\text{→}$生成某一文档$i$的主题(多项式)分布$\theta_i$ 
> > > >    - 从$\theta_i$分布中抽样$\text{→}$生成某一文档$i$的某一主题$z_{i,j}$
> > > > 2. 第二部分：
> > > >    - 从先验$\text{Dirichlet}$分布$\eta$中抽样$\text{→}$生成主题$z_{i,j}$的词语分布$\beta_{z_{i,j}}$
> > > >    - 从$\beta_{z_{i,j}}$分布中抽样$\text{→}$生成词语$w_{i,j}$
> > > >
> > > > :four:$\text{LDA}$的求解(训练)：<span style="color:red;">我也不信考试会考这$\text{B}$玩意儿</span>
> > > >
> > > > 1. $\text{EM}$算法($\text{Old-Fashioned}$)
> > > > 2. $\text{Gibbs}$采，$\text{MCMC(Markov Chain Monte Carlo)}$算法
> > >
> > > #### $\textbf{3.5.2.3. }$番外: $\textbf{pLSA}$的$\textbf{EM}$求解
> > >
> > > > :zero:总体思路
> > > >
> > > > 1. 极大似然估计：找到时$P(T)$最大的参数
> > > > 2. $\text{EM}$算法：直接最大化对数似然函数非常困难，从而通过$\text{EM}$迭代的方式实现
> > > >
> > > > :one:极大似然函数
> > > >
> > > > 1. 似然函数推导
> > > >
> > > >    - 给定共现数据$\textbf{T}=\{n(w_i,d_j)\}\text{→}$要让$\displaystyle{}P(T)\text{=}\prod_{i,j}P(w_i,d_j)^{n(w_i,d_j)}$最大
> > > >
> > > >    - 取对数$+$引入隐含变量：
> > > >
> > > >      $\begin{flalign*}
> > > >      &\displaystyle{}\log{P(T)}\text{=}\sum_{i=1}^{M}\sum_{j=1}^{N}n(w_i,d_j)\text{×}\log{P(w_i,d_j)}&\\
> > > >      & \Bigg\Downarrow {\\引入隐含变量\text{: }\small\displaystyle{}p\left(d_j\right) \sum_z p\left(z_k | d_j\right) p\left(w_i | z_k\right)\\} &\\
> > > >      &\displaystyle{}\log{P(T)}\text{=}\sum_{i=1}^{M}\sum_{j=1}^{N}\left(n(w_i,d_j)\text{×}\left( \log{p(d_j)} \text{+} \log\left(\sum_z p(z_k | d_j) p(w_i | z_k)\right) \right)\right) &
> > > >      \end{flalign*}$ 
> > > >
> > > > 2. 似然函数分析：
> > > >
> > > >    - 已知值：$n(w_i,d_j)$在$\textbf{T}$向量中就有之，$p(d_j)$可由真实大量文本集得到
> > > >    - 参数值：$\displaystyle{}\log\left(\sum_z p(z_k | d_j) p(w_i | z_k)\right)$，其中$\displaystyle{}\log\sum$形式适合用$\text{EM}$算法解决
> > > >
> > > > :two:极大似然函数的下界
> > > >
> > > > 1. $\text{Jensen}$不等式
> > > >
> > > >    |      情况      | $E(f(x))$与$f(E(x))$   |
> > > >    | :------------: | ---------------------- |
> > > >    | $f(x)$为凸函数 | $E(f(x))\geq{}f(E(x))$ |
> > > >    | $f(x)$为凹函数 | $E(f(x))\leq{}f(E(x))$ |
> > > >    |  $x\text{=}C$  | $E(f(x))=f(E(x))$      |
> > > >
> > > > 2. $\displaystyle{}\log\left(\sum_z p(z_k | d_j) p(w_i | z_k)\right)$的处理：构建方差$+$应用$\text{Jensen}$不等式
> > > >
> > > >    - $\displaystyle{}\sum_z p(z_k | d_j) p(w_i | z_k)\xrightarrow{z的分布Q(z)}\sum_z {Q(z)}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\xrightarrow{\small{}X\text{=}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}}E(X)$ 
> > > >    - 原式$\text{=}\log(E(X))\xrightarrow[\text{Jensen不等式}]{\log(x)为凹函数}\text{≥}E(\log(X))\text{=}\displaystyle{}\sum_z\left(\log{}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\right)Q(z)$  
> > > > 
> > > > 3. 下界与极大似然：提升下界$\displaystyle{}\sum_z\left(\log{}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\right)Q(z)$的最大值$\text{→}$最大化似然函数
> > > > 
> > > >:three:$\text{EM}$算法：详细步骤就不写了，<span style="color:red;">我不信考试会考这$\text{B}$玩意儿</span>
> > > > 
> > > > 1. $\text{E}$步：确定$Q$函数$\text{→}$表示当前参数估计下==完全数据(观测数据$+$隐含变量)==的对数似然的期望
> > > >    - 此处$Q\text{=}Q(z)\text{=}p(z_k|w_i,d_j)$ 
> > > > 2. $\text{M}$步：迭代$Q$函数，不断更新参数$\to$使当前参数估计靠近最优值
> > > >    - 此处需要更新的参数为文档-主题分布$P(z|d)$，主题-词汇分布$P(w|z)$
> > > >    - 最终使$\displaystyle{}\sum_z\left(\log{}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\right)Q(z)$最大，从而使$P(T)$最大

