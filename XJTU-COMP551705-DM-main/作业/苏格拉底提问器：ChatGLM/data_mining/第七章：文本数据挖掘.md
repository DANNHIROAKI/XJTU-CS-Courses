<!-- 西安交通大學 XI'AN JIAOTONG UNIVERSITY  -->
![](https://web-api.textin.com/ocr_image/external/1cb7bab72f570c03.jpg)


![](https://web-api.textin.com/ocr_image/external/ca9620bfb2a89f45.jpg)

1696

ANIIA0TO UNIVERSITY

## 数据挖掘

## 第七章：自然语言理解

### 刘均（liukeen＠xjtu.edu.cn）

陕西省大数据知识工程重点实验室

大数据算法与分析技术国家工程实验室


![](https://web-api.textin.com/ocr_image/external/be2cf52075390555.jpg)

NLU面临的挑战

自然语言特性

BOW模型

主题模型

基本要求：了解文本数据的特点、挖掘任务及其挑战，重点介绍BOW、主题模型中的基本原理与方法。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/9c1fa0850c9f35da.jpg)

XIAN JIAOTONG UNIVERSITY

### NLU概念与研究背景


![](https://web-api.textin.com/ocr_image/external/5ef91f23f83bcd6f.jpg)

### 自然语言理解（Natural language understanding，NLU）

NLU是NLP／AI的子领域，旨在研究如何实现让计算机理解人类语言的结构与语义

自然语言理解（Natural language processing，NLP）旨在研究如何对自然语言进行分析、理解、生成。

<!-- Natural Language Processing NL input Computer NL output Understanding Generation  -->
![](https://web-api.textin.com/ocr_image/external/ecb4416990ebef34.jpg)

NLP=NLU+NLG


![](https://web-api.textin.com/ocr_image/external/d243f7abc83e48c4.jpg)

### ✓NLU是一个AI-hard问题。问题难度等同于AI的核心问题“如何使计算机具有与人类等同的智能”

·典型AI-hard问题：NLU、CV

西安交通大學


![](https://web-api.textin.com/ocr_image/external/62b56804d485a179.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/65bf01f8655cce5d.jpg)

## 为什么要NLU


![](https://web-api.textin.com/ocr_image/external/5507875b3eb45852.jpg)

将文本／语言转化为机器理解的形式，解决人类语言的可计算性


![](https://web-api.textin.com/ocr_image/external/7b918860dc3cd8b0.jpg)

典型应用

·信息检索、搜索引擎

·信息抽取

·情感识别

·机器（辅助）翻译

·拼写和语法检查

·知识图谱构建

西安交通大學


![](https://web-api.textin.com/ocr_image/external/799f19fc25f0f3b8.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d7d304473d1d1804.jpg)

## 为什么要NLU

# NLU用于智慧教育（我们目前的工作）

教育知识图谱构建

## 智能答疑

<!-- 语法 解析 与检 查 算术表达式 靖確治 链栈 制x 顺序栈 二叉树 人数 应用 紫 存储结构 特点 阵列 线性表 堆排序 串行 队列 堆栈 知识森林 分面树  -->
![](https://web-api.textin.com/ocr_image/external/b0d94b29aa3ed7aa.jpg)

<!-- 问题：该堆栈经过一次pop操作之后， 它的栈顶元素是什么？ push&gt; 1 2 4 6 9  -->
![](https://web-api.textin.com/ocr_image/external/cc0bbd0c0ded5365.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/39711b9a940f0782.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

## 面临的挑战


![](https://web-api.textin.com/ocr_image/external/ac974c82c04c78e7.jpg)

计算机善于处理精确的、明确的和高度结构化的语言，如编程语言；而自然语言通常是歧义的且与许多复杂因素相关，包括俚语、方言和上下文。


![](https://web-api.textin.com/ocr_image/external/9837cafe84c87ec7.jpg)

歧义性（Ambiguity）

·词汇（词级）歧义-词汇具有不同含义

·句法歧义-句子有不同解析方式

·部分信息的-如代词

·上下文信息-上下文可能会影响句子的含义

西安交通大學

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/80d83058f2f66d01.jpg)

## 词汇歧义（Lexical Ambiguity）

silver as a noun, an adjective, or a verb

# 句法歧义：一个句子被解析成不同的结构

The man saw the girl with the telescope

其它解析结果？

<!-- ROOT S NP VP DT NN VBD NP | I The man Saw NP PP DT NN IN NP | the girl | with DT NN NNP | 1 the telescope *CR*  -->
![](https://web-api.textin.com/ocr_image/external/9d46c8e73ea69bf6.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/2075aa8219f9890d.jpg)

<!-- Text to parse Model e The man saw the girl with the telescope English-en_cc Merge Punctuation Merge Phrases prep nsubj dobj pobj The man saw the girl with the telescope NOUN VERB NOUN ADP NOUN  -->
![](https://web-api.textin.com/ocr_image/external/e719c69f51d23b63.jpg)

https://explosion.ai/demos/displacy?text=The%20man%20sa w%20the%20girl%20with%20the%20telescope&model=en_co re_web_sm&cpu=0&cph=1

西安交通大學

XIAN JIAOTONG UNIVERSITY

# 语义歧义（Semantic Ambiguity）：句子中包含了不明确的单词或短语

· "John and Mary are married." (To each other? or separately?)

① John and Mary got engaged last month.Now, John and Mary are married.

② Which of the men at this party are single? John and Jim are married;the rest are all available.

## · John kissed his wife, and so did Sam

https://cs.nyu.edu/faculty/davise/ai/ambiguity.html

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/b407c562d5f703a3.jpg)

回指歧义（Anaphoric Ambiguity）：之前提到的短语或单词在后面句子中有不同的含义

The horse ran up the hill. It was very steep.It soon got tired.

#  语用歧义（Pragmatic ambiguity）：短语或句子在不同的环境或上下文具有不同的含义

Do you serve crabs here

https://cs.nyu.edu/faculty/davise/ai/ambiguity.html

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JIAOTONG UNIVERSITY

# NLU面临的挑战


![](https://web-api.textin.com/ocr_image/external/bc9927f7f6f211e6.jpg)

# Some interpretations of : I made her duck.

1. I cooked duck for her.

2. I cooked duck belonging to her.

3. I created a toy duck which she owns.

4. I caused her to quickly lower her head or body.

5. I used magic and turned her into a duck.

duck-morphologically and syntactically ambiguous: noun or verb.

her-syntactically ambiguous：与格（dative）、所有格。

make-semantically ambiguous: cook or create.

make -syntactically ambiguous:

Transitive-takes a direct object.⇒2

Di-transitive（双及物）-takes two objects.⇒5

Takes a direct object and a verb. =&gt; 4

http://web.cs.hacettepe.edu.tr/\~ilyas/Courses/BIL711/lec01-overview.PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0bbdf3831affdc86.jpg)

XIAN JIAOTONG UNIVIRSITY

# 挑战


![](https://web-api.textin.com/ocr_image/external/c6178f6a18966515.jpg)


![](https://web-api.textin.com/ocr_image/external/1bf7ab402ec6900c.jpg)

高维数据：All possible word and phrase types in the language！！

与数据挖掘的区别：

records (= docs) are not structurally identical

· records are not statistically independent

文本概念之间存在复杂与隐含的关系

"AOL merges with Time-Warner”

“Time-Warner is bought by AOL”

自然语言包含不同层次上的歧义性（Lexical，syntactic，semantic）'Apple',“Table'

# AI-hard问题。问题难度等同于AI的核心问题“如何使计算机具有与人类等同的智能”

· 典型AI-hard问题：NLU、CV

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVERSITY

## 原理1：词汇的语义由上下文决定


![](https://web-api.textin.com/ocr_image/external/115faa271e1f21cf.jpg)

上个世纪五十年代美国语言学家Zellig Sabbettai Harris提出的分布式假设：词汇的分布相似性和语义相似性存在相关性。


![](https://web-api.textin.com/ocr_image/external/1bfedc2d0dadd1f0.jpg)

语言学家John Rupert Firth对分进行推广：可以通过一个词周围的词汇了解这个词的含义；

在相同语境中出现的词语往往具有相似的含义。

...government debt problems turning into banking crises as happened in 2009...

...sayig that Europe needs unified banking regulation to replace the hodgepodge...

...India has just given its banking system a shot in the arm...

These context words will represent banking

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f93e7979fca0b0b2.jpg)

XIAN JIAOTONG UNIVIRSITY

## 原理2：映射到统一空间中进行处理


![](https://web-api.textin.com/ocr_image/external/7c155732f249b5cd.jpg)

<!-- 爱情 读者 图书 8 8 科幻 军事 主题空间  -->
![](https://web-api.textin.com/ocr_image/external/3fa6034b750a6e81.jpg)


![](https://web-api.textin.com/ocr_image/external/277b2ae6925e6a28.jpg)

# 主要任务-语法


![](https://web-api.textin.com/ocr_image/external/8f0264a5b79e2def.jpg)

1．词干抽取（Stemming）

2．词形还原（Lemmatization）

3．词性标注（Part-of-speech tagging）

4．句法分析（Parsing）

5．断句（Sentence breaking）

6．术语抽取（Terminology extraction）

7．分词（Word segmentation）

https://en.wikipedia.org/wiki/Natural-language_processing 西安交通大學


![](https://web-api.textin.com/ocr_image/external/2ff2a4ffe539d76f.jpg)

XIAN JIAOTONG UNIVIRSITY

## 词干抽取（Stemming）


![](https://web-api.textin.com/ocr_image/external/1cfef6f32bcc1db6.jpg)

# 词干抽取：抽取词的词干（stem）或词根（root）形式

closed、closely、closing→close

unhappyness→unhappy（词干）、happy（词根）

## 主要方法：

利用形态规则：如if the word ends in 'ing', remove the 'ing';

局限性：不规则变形词

基于词典：受限于词典规模

n-gram方法、隐马尔可夫模型、机器学习；局限性：语料数据

应用：信息检索、领域术语集

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ab1f5e1f8901d749.jpg)

XIAN JIAOTONG UNIVIRSITY

# 词形还原（Lemmatization）


![](https://web-api.textin.com/ocr_image/external/6e1e2174dbf7ab96.jpg)

# 词形还原：把不同形式的词汇还原为词目（lemma）形式

good、better、best→good

walk, walked, walks, walking→alk

am,is, are,was,were→be

与词干抽取（Stemming）的区别：词形还原取决于词性以及词汇在句子中的实际含义。词干抽取不需要单词的实际语义与上下文环境

in our last meeting

<!-- Lemmatization meeting,meet Stemming meet  -->
![](https://web-api.textin.com/ocr_image/external/6b4b32af8a830347.jpg)

We are meeting again tomorrow

规则方法：人工规则（专家语言知识）与机器学习（需要大量的训练数据）出的规则

基于词典：适用于简单的语言，受限于词典

https://en.wikipedia.org/wiki/Lemmatisation 西安交通大學


![](https://web-api.textin.com/ocr_image/external/d53d1493c47c550b.jpg)

XIAN JIAOTONG UNIVIRSITY

## 词性标注（Part-of-speech tagging）

词性标注：为文本中每个词标记一个词性的过程，如名词、动词、形容词或其他词性的过程

<table border="1" ><tr>
<td colspan="1" rowspan="1">A dog</td>
<td colspan="1" rowspan="1"> is a</td>
<td colspan="1" rowspan="1">very c</td>
<td colspan="1" rowspan="1">ommon</td>
<td colspan="1" rowspan="1">four-</td>
<td colspan="1" rowspan="1">legged</td>
<td colspan="1" rowspan="1">anima</td>
<td colspan="1" rowspan="1">l that</td>
<td colspan="1" rowspan="1"> is oft</td>
<td colspan="1" rowspan="1">en ke</td>
<td colspan="1" rowspan="1">pt by </td>
<td colspan="1" rowspan="1">people</td>
</tr><tr>
<td colspan="1" rowspan="1">asa</td>
<td colspan="1" rowspan="1">pet or</td>
<td colspan="1" rowspan="1"> to gu</td>
<td colspan="1" rowspan="1">ard or</td>
<td colspan="1" rowspan="1">hunt</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

Adjective Adverb Conjunction Determiner Noun

Number Preposition Pronoun Verb

基于规则：依据标注的语料库人工制定判断规则

基于HMM：可观测序列是分词，隐藏状态是词性，在标注的语料库上训练HMM

机器学习：SVM、NN

挑战：词汇多义性；分词

The can can be used to startle the dog when it misbehaves

红黑树是一种二叉树

西安交通大學


![](https://web-api.textin.com/ocr_image/external/7bc566ddae9dcdb3.jpg)

XIAN JIAOTONG UNIVERSITY

## 句法分析（Parsing）


![](https://web-api.textin.com/ocr_image/external/3a25ee559a1d3329.jpg)

# 句法分析：分析文本中单词／短语之间的句法关系，形成句法结构

<!-- 句子 动词短语 名词短语 名词短语 量词短语 名词短语 名词 动词 数词 量词 形容词 名词 决策树 是 一 种 树形 模型  -->
![](https://web-api.textin.com/ocr_image/external/e9a6a296b809175c.jpg)

统计方法：probabilistic context-free grammars， maximum entropy...

机器学习方法：NN、RNN（Stanford）

应用：命名实体识别、关系抽取、翻译、问答

西安交通大學


![](https://web-api.textin.com/ocr_image/external/93154cbd9c606e99.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/515e5e3d6c15d46f.jpg)

## 断句：用于确定句子的起始位置

Sentence boundary disambiguation, sentence segmentation

标点符号等存在歧义。如“.”可能是句号，也可能是缩写、小数点、省略号（47％的“．”在WSJ语料中是缩写）


![](https://web-api.textin.com/ocr_image/external/68c107b26627b62e.jpg)

中文不存在此问题

1．如果是“”

2．如果前面标记位于手工编辑的缩写列表中，则不会结束句子 正确率95％

3．如果后面标记时大写，则结束一个句子

## 基于最大熵、神经网络的方法取得更高的正确率

https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5f2a1c0e146054d1.jpg)

XIAN JIAOTONG UNIVERSITY

## 术语抽取（Terminology extraction）


![](https://web-api.textin.com/ocr_image/external/7c41b53b8b45397f.jpg)

术语抽取：是信息抽取（Information Extraction）的子任务，目标是从文本中抽取特定领域的专门用语


![](https://web-api.textin.com/ocr_image/external/5f24bfb9a10e90f7.jpg)

二叉树是数据结构领域的术语

小世界特性是复杂网络分析领域的术语，但不是数据结构领域的术语

## 一般方法：


![](https://web-api.textin.com/ocr_image/external/ba6547674bef45b0.jpg)

机器学习：聚类法、分类法

统计度量：从语料中统计用词规律，如TF／IDF

✓外部知识库：如Wikipedia

# 难点：新术语、跨领域术语

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a7d59152d76d256f.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/4de26dd718234164.jpg)

分词：对于词汇间没有边界标记的语言（如中文），将连续的字符串（句子）划分成有意义的单词

<!-- 南京市长江大桥 南京市／长江大桥 南京市长／江大桥  -->
![](https://web-api.textin.com/ocr_image/external/959b1a19159922b9.jpg)

## 一般方法：

词典方法：正向最大匹配、逆向最大匹配和双向匹配

基于统计：HMM、CRF、SVM

# 难题：未登录词，切分歧义

西安交通大學


![](https://web-api.textin.com/ocr_image/external/90f5ee128cd1c202.jpg)

XIAN JIAOTONG UNIVERSITY

## 主要任务-语义


![](https://web-api.textin.com/ocr_image/external/5ef91f23f83bcd6f.jpg)

1．机器翻译（Machine translation）

2．命名实体识别（Named entity recognition）

3．文本分类（Text classification）

4．问答（Question answering）

5．关系抽取（Relationship extraction）

6．情感分析（Sentiment analysis）

7．主题分割（Topic segmentation）

8．词义消歧（Word sense disambiguation）

9．自动文摘（Automatic summarization）

10．共指消解（Coreference resolution） https://en.wikipedia.org/wiki/N

atural-language_processing

XIAN JIAOTONG UNIVERSITY

# 机器翻译 (Machine translation)


![](https://web-api.textin.com/ocr_image/external/6730b9094b9ce393.jpg)

## 机器翻译：将文本／语音从一种语言翻译到另一种语言。不同于CAT。

基本方法：

基于规则的机器翻译：翻译过程通常依据利用源语言与目标语言的形态规则、语法规、专业词典新闻分类。

统计机器翻译：通过大量平行语料的统计分析构建概率模型，生成源语言可能性最大的翻译结果

神经网络机器翻译：利用神经网络对翻译过程进行建模，实现端到端的翻译

## 难题：

<!-- 请在一米线外等候 PLEASE WAIT OUTSIDE A NOODLE  -->
![](https://web-api.textin.com/ocr_image/external/8e8e330fc2d41566.jpg)

词语歧义

强烈依赖语料的数据量

低频词汇

<!-- 禁止打手机 No beat the cellular phone  -->
![](https://web-api.textin.com/ocr_image/external/dc09613637489659.jpg)

长句子

西安交通大學


![](https://web-api.textin.com/ocr_image/external/7bc566ddae9dcdb3.jpg)

XIAN JIAOTONG UNIVERSITY

# 命名实体识别（Named entity recognition）


![](https://web-api.textin.com/ocr_image/external/bd501b43bb15ea0a.jpg)

命名实体识别：信息抽取的子任务，旨在识别文本中的命名实体，并划分到预定义的类别

命名实体：现实世界中的某个对象

<table border="1" ><tr>
<td colspan="1" rowspan="1">Name</td>
<td colspan="1" rowspan="1">sOraniza</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">gPerson</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Location</td>
</tr><tr>
<td colspan="1" rowspan="1">Times</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Date</td>
</tr><tr>
<td colspan="1" rowspan="1">Nb</td>
<td colspan="1" rowspan="1">Time</td>
</tr><tr>
<td colspan="1" rowspan="1">um</td>
<td colspan="1" rowspan="1">ersMoney</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Percent</td>
</tr></table>

Obama is the president of the United States

Jim bought 300 shares of Acme Corp. in 2006

## 主要方法：

语法规则：准确率高，召回率低，需要语言学专家制定规则

统计方法：需要大量标注数据

## 难题：

领域依赖性，如医学领域

实体类型多样

西安交通大學


![](https://web-api.textin.com/ocr_image/external/24e5bfd996fff5c5.jpg)

XIAN JIAOTONG UNIVERSITY

# 文本分类(Text classification)


![](https://web-api.textin.com/ocr_image/external/bb292ac6610186eb.jpg)

## 文本分类：自动将文本划分到预先定义的类别中

垃圾邮件过滤

Yahoo!: 200 people for manual 情感识别 labeling of Web pages; using a hierarchy of 500,000 categories

新闻分类

MEDLINE (National Library of

色情文档识别

Medicine): &#36;2 million/year for

manual indexing of journal articles

基本方法：①提取特征；②训练分类器分类：Naïve Bayes、KNN、SVM和NN

难题：

特征选择：根据分类问题选择合适的特征，并非TF／IDF都适用

数据标注问题：需要大量、专业的人工进行数据标注

数据非平衡问题

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0378af6d5826628f.jpg)

XIAN JIAOTONG UNIVIRSITY

## 问答（Question answering）


![](https://web-api.textin.com/ocr_image/external/496e119d5540e88d.jpg)

问答：旨在研究如何自动回答自然语言形式的问题

分类：5W1H、Open／Closed-domain

方法

检索方法：给定特定数据集，通过检索和答案抽取从中找到能够回答问题的答案

自然语言生成：根据知识库／知识图谱自动推理／计算问题的答案

## 面临的难题 问题：该堆栈经过一次pop操作之后，它的栈顶元素是什么？

多知识约束：答疑

多轮对话

背景知识的表示与应用

<!-- pushy pop 1 2 4 6 9  -->
![](https://web-api.textin.com/ocr_image/external/d57ae454cfe0ed6c.jpg)

问答中的用户画像

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ab1f5e1f8901d749.jpg)

XIAN JIAOTONG UNiVrRSiTY

# 关系抽取（Relationship extraction）


![](https://web-api.textin.com/ocr_image/external/eab6c0e9355dd416.jpg)

# 关系抽取：对文档中实体间的语义关系进行检测与分类

<!-- 内角和 的定义 三角形内 角和定理 三角形 的定义 三角形外 角和定理 外角和 的定义 学习依赖关系  -->
![](https://web-api.textin.com/ocr_image/external/f7812c49f69bff2e.jpg)

<!-- 血小板聚集 钙通道阻滞剂 偏头痛 传播皮层抑郁 镁 精神紧张  -->
![](https://web-api.textin.com/ocr_image/external/b13aa63a5feae76d.jpg)

<!-- Alice Leonardo Da Vinci hetp:/ Http://agle.org/cae pdhpedi.orgounce/ Leonardo.da_Mind foaf:knows dcterms:creator foaf:topic_interest dcterms:title "Mona Lisa" BOB http://exampis.org/bobtme schema birthDate Mtp:/ The Mona Lisa www.miidata.org/entny/Q12418 rdf:type dcterms subject La Joconde àWashington http://dsts.nurpeann.nu/tem/04 MFA Person foat.Person "1990-07-04***xsd:date 8618938F4117025F17A88813C5P9AA4D619  -->
![](https://web-api.textin.com/ocr_image/external/8cd96c2e36006d71.jpg)

## 难点

训练数据集构建

<!-- * 解析 確汉 今 查 算术表达式 链栈 丑常 x 顺序栈 ＜業 应用 存储结构 特点 阵列 串行  -->
![](https://web-api.textin.com/ocr_image/external/a3e626de373e5fd8.jpg)

自然语言的歧义、非规范性

<!-- 数据 结构 线性 表 图 数据 类型 一般 线性 受限 表 线性 有向 图 无向 表 图 双向 循环 堆栈 队列 上下位关系 链表 链表 兄弟关系  -->
![](https://web-api.textin.com/ocr_image/external/3752375c53ea2937.jpg)

✓数据间冲突与不确定性

西安交通大學


![](https://web-api.textin.com/ocr_image/external/531bc987de475de1.jpg)

XIAN JIAOTONG UNIVIRSITY

## 情感分析（Sentiment analysis）


![](https://web-api.textin.com/ocr_image/external/e4fee80eac6f9963.jpg)

情感分析：识别文本（商品评论、微博发帖等）中的情感状态（悲伤、忧郁、快乐...）、主观评价（积极、消极、中立）等信息

倾向性分析、观点挖掘（Opinion Mining）

✓《复联3》是一部史无前例的电影

让人有些绝望的电影结局

✓MIUI8系统还算流畅，功能多，人性化，但是广告不能完全关闭

## 基本方法：

✓情感词库（happy，sad，bored...）

✓统计方法：LSA、SVM、...

## 难题：

多样性的修辞手段，如反语，讽刺；

分面观点。商品：数码相机；新闻事件：雷公太极被徐晓冬KO

西安交通大學

XIAN JIAOTONG UNIVIRSITY

## 主题分割（Topic segmentation）


![](https://web-api.textin.com/ocr_image/external/bc9927f7f6f211e6.jpg)

# 主题分割：将单个长文本分割成较短的、主题一致（topically coherent）的多个片段

To construct the FalDroid-I dataset, approximately 15,000 malware samples are first downloaded from VirusShare and uploaded to VirusTotal, which is a system with 53 anti-virus scanners.The following two issues are found from the anti-virus scanners.... Finally, 8,407 malware samples in 36 families are labeled. The samples in the FalDroid-Il dataset are provided by contagion and MassVet and labeled in the same manner as those in the FalDroid-I dataset. Finally, 643 malware samples in 43 families are labeled.

## 基本方法：

✓基于内容变化：同一文本主题的部分往往具有较高相似度，通过聚类以及主题模型估等；

✓基于边界特征：在主题间进行切换时的边界特征，如过渡性、总结性文本难题：

任务目标模糊，主题粒度多样性

自然语言歧义性，无关信息的干扰

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0bbdf3831affdc86.jpg)

XIAN JIAOTONG UNIVIRSITY

## 词义消歧（Word sense disambiguation）


![](https://web-api.textin.com/ocr_image/external/3a25ee559a1d3329.jpg)

## 词义消歧：当一个单词具有多重含义时，确定该单词在句子中的具体含义

# 实例：

Little John was looking for his toy box. Finally he found it. The box was in the pen. John was very happy.

## 方法

基于词典、叙词表（Thesauri）以及词汇知识库（如Wordnet）

半监督方法：利用小型语料库作为种子数据

监督方法：使用标注语料进行训练

## 难题

词义的离散性（discreteness of senses）

语用学问题，需要常识（common sense）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/10b1ad61a99faf81.jpg)

XIAN JIAOTONG UNIVIRSITY

# 自动文摘（Automatic summarization）


![](https://web-api.textin.com/ocr_image/external/1d8f2d2343b43718.jpg)

自动文摘：为文档自动生成一个包含原文档要点的压缩版本

实例：搜索引擎为每个返回结果生成一段摘要


![](https://web-api.textin.com/ocr_image/external/1c57d4017c1dbf25.jpg)

<!-- Baidu百度 数据结构 百度一下 网页 新闻 贴吧 知道 音乐 图片 视频 地图 文库 更多》 百度为您找到相关结果约27,200,000个 了搜索工具 为您推荐：数据结构c语言版 数据结构严蔚敏pdf下载 严蔚敏 数据结构 pdf 数据结构＿百度百科 数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存 在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的 数据结构可以带来更高的运行或者存储效率。数据... 定义 研究对象 重要意义 研究内容 结构分类 更多＞ baike.baidu.com/  -->
![](https://web-api.textin.com/ocr_image/external/40039ae515c793ea.jpg)

## 方法

抽取方法：抽取包含重要信息的部分，而不对抽取内容进行修改

概括方法：利用自然语言生成技术进行更精炼的修改、复述。

## 难题


![](https://web-api.textin.com/ocr_image/external/58021e646e3a626f.jpg)

可读性、可理解性、压缩要求、背景知识

评估问题：人工、自动；ROUGE： Recall Oriented Understudy for Gisting Evaluation

西安交通大學


![](https://web-api.textin.com/ocr_image/external/12fc61d5fc899430.jpg)

XIAN JIAOTONG UNIVIRSITY

# 共指消解（Coreference resolution）


![](https://web-api.textin.com/ocr_image/external/bb292ac6610186eb.jpg)

共指消解：自动识别出文本中表示同一个事物的不同指称（mention）

实例

甲队打败了乙队，他们更强

虽然甲队打败了乙队，但他们都很强

## 方法


![](https://web-api.textin.com/ocr_image/external/e723554872f03442.jpg)

启发式规则：最接近语法兼容词（closest grammatically compatible word)

基于ML：Mention-Pair Models、Mention-Ranking Models、Graph-Based Approaches

## 面临的难题

如何应用背景知识

自然语言表达多样与歧义性（甲队打败了乙队，他们实力还是强／弱一些）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0378af6d5826628f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a2535c8e36a47083.jpg)

文本建模：将非（半）结构化文本转化为结构化形式便于后续分析处理。

# 理论基础：Zipf＇s Law＆ Heaps＇Law

✓Zipf＇s Law：描述文档中的词频分布

Heaps＇Law：描述文档集的增长与文档中词汇量之间的量化关系

✓对于索引机制、术语权重的设计具有指导作用

西安交通大學


![](https://web-api.textin.com/ocr_image/external/7faa9ce6fe1392ce.jpg)

XIAN JIAOTONG UNIVIRSITY

# 文本建模 -Zipf＇s Law


![](https://web-api.textin.com/ocr_image/external/222e0013c72030a9.jpg)

## 词频分布

少数词非常普遍：如两个频度最高的词（the与of）在文本中占10％的比例

大多数出现的频次非常低：一半左右的词在语料库中只出现一次

<table border="1" ><tr>
<td colspan="1" rowspan="1">FrequentWord</td>
<td colspan="1" rowspan="1">Number ofOccurrences</td>
<td colspan="1" rowspan="1">Percentageof Total</td>
</tr><tr>
<td colspan="1" rowspan="1">the</td>
<td colspan="1" rowspan="1">7,398,934</td>
<td colspan="1" rowspan="1">5.9</td>
</tr><tr>
<td colspan="1" rowspan="1">of</td>
<td colspan="1" rowspan="1">3,893,790</td>
<td colspan="1" rowspan="1">3.1</td>
</tr><tr>
<td colspan="1" rowspan="1">to</td>
<td colspan="1" rowspan="1">3,364,653</td>
<td colspan="1" rowspan="1">2.7</td>
</tr><tr>
<td colspan="1" rowspan="1">and</td>
<td colspan="1" rowspan="1">3,320,687</td>
<td colspan="1" rowspan="1">2.6</td>
</tr><tr>
<td colspan="1" rowspan="1">in</td>
<td colspan="1" rowspan="1">2,311,785</td>
<td colspan="1" rowspan="1">1.8</td>
</tr><tr>
<td colspan="1" rowspan="1">is</td>
<td colspan="1" rowspan="1">1,559,147</td>
<td colspan="1" rowspan="1">1.2</td>
</tr><tr>
<td colspan="1" rowspan="1">for</td>
<td colspan="1" rowspan="1">1,313,561</td>
<td colspan="1" rowspan="1">1.0</td>
</tr><tr>
<td colspan="1" rowspan="1">The</td>
<td colspan="1" rowspan="1">1,144,860</td>
<td colspan="1" rowspan="1">0.9</td>
</tr><tr>
<td colspan="1" rowspan="1">that</td>
<td colspan="1" rowspan="1">1,066,503</td>
<td colspan="1" rowspan="1">0.8</td>
</tr><tr>
<td colspan="1" rowspan="1">said</td>
<td colspan="1" rowspan="1">1,027,713</td>
<td colspan="1" rowspan="1">0.8</td>
</tr></table>

词频的分布是长尾

（long tailed）或重尾

（heavy tailed）分布 Frequencies from 336,310 documents in the 1GB TREC Volume 3 Corpus 125,720,891 total word occurrences; 508,209 unique words

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d53d1493c47c550b.jpg)

XIAN JIAOTONG UNIVERSITY

# 文本建模-Zipf＇s Law


![](https://web-api.textin.com/ocr_image/external/222e0013c72030a9.jpg)


![](https://web-api.textin.com/ocr_image/external/7821bc813d0bef92.jpg)

# Zipf's Law


![](https://web-api.textin.com/ocr_image/external/5deef610a3832022.jpg)

Rank（r）：一个词按照词频（f）从大到小的排列次序。

George Kingsley Zipf（1902-1950）发现：

f⋅r=c(for constant c)

·The i th most frequent term has frequency proportional to 1/i

·Let this frequency be c/i.

Then$\sum _ { i = 1 } ^ { 5 0 0 , 0 0 } c / i = 1 .$

The k th Harmonic number is $H _ { k } = \sum _ { i = 1 } ^ { k } 1 / i .$

$c = 1 / H _ { m } = 1 / \ln n ( 5 0 0 k ) \sim 1 / 1 3 .$

· So the i th most frequent term has frequency roughly 1/13i.

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d53d1493c47c550b.jpg)

XIAN JIAOTONG UNIVERSITY

# 文本建模 - Zipf＇s Law


![](https://web-api.textin.com/ocr_image/external/09214c18e1ee620b.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">the</td>
<td colspan="1" rowspan="1">1130021</td>
<td colspan="1" rowspan="1">from</td>
<td colspan="1" rowspan="1">96900</td>
<td colspan="1" rowspan="1">or</td>
<td colspan="1" rowspan="1">54958</td>
</tr><tr>
<td colspan="1" rowspan="1">of</td>
<td colspan="1" rowspan="1">547311</td>
<td colspan="1" rowspan="1">he</td>
<td colspan="1" rowspan="1">94585</td>
<td colspan="1" rowspan="1">about</td>
<td colspan="1" rowspan="1">53713</td>
</tr><tr>
<td colspan="1" rowspan="1">to</td>
<td colspan="1" rowspan="1">516635</td>
<td colspan="1" rowspan="1">million</td>
<td colspan="1" rowspan="1">93515</td>
<td colspan="1" rowspan="1">market</td>
<td colspan="1" rowspan="1">52110</td>
</tr><tr>
<td colspan="1" rowspan="1">a</td>
<td colspan="1" rowspan="1">464736</td>
<td colspan="1" rowspan="1">year</td>
<td colspan="1" rowspan="1">90104</td>
<td colspan="1" rowspan="1">they</td>
<td colspan="1" rowspan="1">51359</td>
</tr><tr>
<td colspan="1" rowspan="1">in</td>
<td colspan="1" rowspan="1">390819</td>
<td colspan="1" rowspan="1">its</td>
<td colspan="1" rowspan="1">86774</td>
<td colspan="1" rowspan="1">this</td>
<td colspan="1" rowspan="1">50933</td>
</tr><tr>
<td colspan="1" rowspan="1">and</td>
<td colspan="1" rowspan="1">387703</td>
<td colspan="1" rowspan="1">be</td>
<td colspan="1" rowspan="1">85588</td>
<td colspan="1" rowspan="1">would</td>
<td colspan="1" rowspan="1">50828</td>
</tr><tr>
<td colspan="1" rowspan="1">that</td>
<td colspan="1" rowspan="1">204351</td>
<td colspan="1" rowspan="1">was</td>
<td colspan="1" rowspan="1">83398</td>
<td colspan="1" rowspan="1">you</td>
<td colspan="1" rowspan="1">49281</td>
</tr><tr>
<td colspan="1" rowspan="1">for</td>
<td colspan="1" rowspan="1">199340</td>
<td colspan="1" rowspan="1">company</td>
<td colspan="1" rowspan="1">83070</td>
<td colspan="1" rowspan="1">which</td>
<td colspan="1" rowspan="1">48273</td>
</tr><tr>
<td colspan="1" rowspan="1">is</td>
<td colspan="1" rowspan="1">152483</td>
<td colspan="1" rowspan="1">an</td>
<td colspan="1" rowspan="1">76974</td>
<td colspan="1" rowspan="1">bank</td>
<td colspan="1" rowspan="1">47940</td>
</tr><tr>
<td colspan="1" rowspan="1">said</td>
<td colspan="1" rowspan="1">148302</td>
<td colspan="1" rowspan="1">has</td>
<td colspan="1" rowspan="1">74405</td>
<td colspan="1" rowspan="1">stock</td>
<td colspan="1" rowspan="1">47401</td>
</tr><tr>
<td colspan="1" rowspan="1">it</td>
<td colspan="1" rowspan="1">134323</td>
<td colspan="1" rowspan="1">are</td>
<td colspan="1" rowspan="1">74097</td>
<td colspan="1" rowspan="1">trade</td>
<td colspan="1" rowspan="1">47310</td>
</tr><tr>
<td colspan="1" rowspan="1">on</td>
<td colspan="1" rowspan="1">121173</td>
<td colspan="1" rowspan="1">have</td>
<td colspan="1" rowspan="1">73132</td>
<td colspan="1" rowspan="1">his</td>
<td colspan="1" rowspan="1">47116</td>
</tr><tr>
<td colspan="1" rowspan="1">by</td>
<td colspan="1" rowspan="1">118863</td>
<td colspan="1" rowspan="1">but</td>
<td colspan="1" rowspan="1">71887</td>
<td colspan="1" rowspan="1">more</td>
<td colspan="1" rowspan="1">46244</td>
</tr><tr>
<td colspan="1" rowspan="1">as</td>
<td colspan="1" rowspan="1">109135</td>
<td colspan="1" rowspan="1">will</td>
<td colspan="1" rowspan="1">71494</td>
<td colspan="1" rowspan="1">who</td>
<td colspan="1" rowspan="1">42142</td>
</tr><tr>
<td colspan="1" rowspan="1">at</td>
<td colspan="1" rowspan="1">101779</td>
<td colspan="1" rowspan="1">say</td>
<td colspan="1" rowspan="1">66807</td>
<td colspan="1" rowspan="1">one</td>
<td colspan="1" rowspan="1">41635</td>
</tr><tr>
<td colspan="1" rowspan="1">mr</td>
<td colspan="1" rowspan="1">101679</td>
<td colspan="1" rowspan="1">new</td>
<td colspan="1" rowspan="1">64456</td>
<td colspan="1" rowspan="1">their</td>
<td colspan="1" rowspan="1">40910</td>
</tr><tr>
<td colspan="1" rowspan="1">with</td>
<td colspan="1" rowspan="1">101210</td>
<td colspan="1" rowspan="1">share</td>
<td colspan="1" rowspan="1">63925</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

Frequency of 50 most common words in English

(sample of 19 million words)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/54ef8328e5f62ce5.jpg)

XIAN JIAOTONG UNIVrRSiTY


![](https://web-api.textin.com/ocr_image/external/11fa5b3a10959468.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="5" rowspan="1">rf*1000/n</td>
</tr><tr>
<td colspan="1" rowspan="1">the</td>
<td colspan="1" rowspan="1">59</td>
<td colspan="1" rowspan="1">from</td>
<td colspan="1" rowspan="1">92</td>
<td colspan="1" rowspan="1">or</td>
<td colspan="1" rowspan="1">101</td>
</tr><tr>
<td colspan="1" rowspan="1">of</td>
<td colspan="1" rowspan="1">58</td>
<td colspan="1" rowspan="1">he</td>
<td colspan="1" rowspan="1">95</td>
<td colspan="1" rowspan="1">about</td>
<td colspan="1" rowspan="1">102</td>
</tr><tr>
<td colspan="1" rowspan="1">to</td>
<td colspan="1" rowspan="1">82</td>
<td colspan="1" rowspan="1">million</td>
<td colspan="1" rowspan="1">98</td>
<td colspan="1" rowspan="1">market</td>
<td colspan="1" rowspan="1">101</td>
</tr><tr>
<td colspan="1" rowspan="1">a</td>
<td colspan="1" rowspan="1">98</td>
<td colspan="1" rowspan="1">year</td>
<td colspan="1" rowspan="1">100</td>
<td colspan="1" rowspan="1">they</td>
<td colspan="1" rowspan="1">103</td>
</tr><tr>
<td colspan="1" rowspan="1">in</td>
<td colspan="1" rowspan="1">103</td>
<td colspan="1" rowspan="1">its</td>
<td colspan="1" rowspan="1">100</td>
<td colspan="1" rowspan="1">this</td>
<td colspan="1" rowspan="1">105</td>
</tr><tr>
<td colspan="1" rowspan="1">and</td>
<td colspan="1" rowspan="1">122</td>
<td colspan="1" rowspan="1">be</td>
<td colspan="1" rowspan="1">104</td>
<td colspan="1" rowspan="1">would</td>
<td colspan="1" rowspan="1">107</td>
</tr><tr>
<td colspan="1" rowspan="1">that</td>
<td colspan="1" rowspan="1">75</td>
<td colspan="1" rowspan="1">was</td>
<td colspan="1" rowspan="1">105</td>
<td colspan="1" rowspan="1">you</td>
<td colspan="1" rowspan="1">106</td>
</tr><tr>
<td colspan="1" rowspan="1">for</td>
<td colspan="1" rowspan="1">84</td>
<td colspan="1" rowspan="1">company</td>
<td colspan="1" rowspan="1">109</td>
<td colspan="1" rowspan="1">which</td>
<td colspan="1" rowspan="1">107</td>
</tr><tr>
<td colspan="1" rowspan="1">is</td>
<td colspan="1" rowspan="1">72</td>
<td colspan="1" rowspan="1">an</td>
<td colspan="1" rowspan="1">105</td>
<td colspan="1" rowspan="1">bank</td>
<td colspan="1" rowspan="1">109</td>
</tr><tr>
<td colspan="1" rowspan="1">said</td>
<td colspan="1" rowspan="1">78</td>
<td colspan="1" rowspan="1">has</td>
<td colspan="1" rowspan="1">106</td>
<td colspan="1" rowspan="1">stock</td>
<td colspan="1" rowspan="1">110</td>
</tr><tr>
<td colspan="1" rowspan="1">it</td>
<td colspan="1" rowspan="1">78</td>
<td colspan="1" rowspan="1">are</td>
<td colspan="1" rowspan="1">109</td>
<td colspan="1" rowspan="1">trade</td>
<td colspan="1" rowspan="1">112</td>
</tr><tr>
<td colspan="1" rowspan="1">on</td>
<td colspan="1" rowspan="1">77</td>
<td colspan="1" rowspan="1">have</td>
<td colspan="1" rowspan="1">112</td>
<td colspan="1" rowspan="1">his</td>
<td colspan="1" rowspan="1">114</td>
</tr><tr>
<td colspan="1" rowspan="1">by</td>
<td colspan="1" rowspan="1">81</td>
<td colspan="1" rowspan="1">but</td>
<td colspan="1" rowspan="1">114</td>
<td colspan="1" rowspan="1">more</td>
<td colspan="1" rowspan="1">114</td>
</tr><tr>
<td colspan="1" rowspan="1">as</td>
<td colspan="1" rowspan="1">80</td>
<td colspan="1" rowspan="1">will</td>
<td colspan="1" rowspan="1">117</td>
<td colspan="1" rowspan="1">who</td>
<td colspan="1" rowspan="1">106</td>
</tr><tr>
<td colspan="1" rowspan="1">at</td>
<td colspan="1" rowspan="1">80</td>
<td colspan="1" rowspan="1">say</td>
<td colspan="1" rowspan="1">113</td>
<td colspan="1" rowspan="1">one</td>
<td colspan="1" rowspan="1">107</td>
</tr><tr>
<td colspan="1" rowspan="1">mr</td>
<td colspan="1" rowspan="1">86</td>
<td colspan="1" rowspan="1">new</td>
<td colspan="1" rowspan="1">112</td>
<td colspan="1" rowspan="1">their</td>
<td colspan="1" rowspan="1">108</td>
</tr><tr>
<td colspan="1" rowspan="1">with</td>
<td colspan="1" rowspan="1">91</td>
<td colspan="1" rowspan="1">share</td>
<td colspan="1" rowspan="1">114</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/6ab7cdbb1abd8038.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/8906e83aaf4c0e60.jpg)

# Zipf＇s Law的符合程度

符合$y = k x ^ { c }$的分布为幂律分布（power law）.

✓Zipf＇s law 也是幂律分布

✓在双对数坐标系（log-log plot），幂律分布为斜率为c的直线

$\log ( y ) = \log ( k x ^ { c } ) = \log k + c \log ( x )$

✓除了rank很高与很低的术语，Zipf分布与实际符合较好

Fit to Zipf for

<!-- DONoL C0o1 AerbeA 001 00 0 10 100 1000 10000 100000 rark  -->
![](https://web-api.textin.com/ocr_image/external/a04c34b78d534791.jpg)

Brown Corpus

西安交通大學


![](https://web-api.textin.com/ocr_image/external/bc4683797f658bca.jpg)

XIAN JIAOTONG UNIVIRSITY

# 文本建模 -Zipf＇s Law


![](https://web-api.textin.com/ocr_image/external/169604f8b59d1ce4.jpg)

## Zipf 定律的解释

Miller's Monkey

·长度为i的词的概率： $P ( i ) = ( 1 / 2 7 ) ^ { i } ( 1 / 2 7 ) = ( 1 / 2 7 ) ^ { i + 1 }$

·长度为i的词的rank值 $\sum _ { j = 1 } ^ { i - 1 } 2 6 ^ { j } < r _ { i } \leq \sum _ { j = 1 } ^ { i } 2 6 ^ { j }$


![](https://web-api.textin.com/ocr_image/external/8fa308c8684940f4.jpg)

$r = \sum _ { j = 1 } ^ { i } 2 6 ^ { j } = \frac { 2 6 } { 2 5 } ( 2 6 ^ { i } - 1 ) ,$ $p ( i ^ { \prime } ) = ( 1 / 2 7 ) ^ { i ^ { \prime } + 1 }$

$= ( 1 / 2 7 ) \frac { \log ( \frac { 2 5 } { 2 0 } r + 1 ) } { \log 2 6 } + 1$

$i ^ { \prime } = \frac { \log ( \frac { 2 5 } { 2 6 } r + 1 ) } { \log 2 6 }$ $= ( 1 / 2 7 ) ( \frac { 2 5 } { 2 6 } r + 1 ) ^ { - \frac { 1 0 8 2 7 } { 1 0 8 2 6 } }$ using the fact$a ^ { \log b } = b ^ { \log a }$


![](https://web-api.textin.com/ocr_image/external/f36079d382931469.jpg)

$\approx 0 . 0 4 ( r + 1 . 0 4 ) ^ { - 1 . 0 1 } ,$


![](https://web-api.textin.com/ocr_image/external/df9b26b06869d68c.jpg)


![](https://web-api.textin.com/ocr_image/external/1e1666ac05788abc.jpg)

## Zipf 定律的解释

Principle of least effort：词频的差异有助于使用较少的词汇表达尽可能多的语义

# 语言使用的影响机制：Preferential attachment

· Start with a limited number of initial nodes $m _ { 0 }$

At each time step, add a new node that has m edges that link to m existing nodes in the system $m \leq m _ { 0 }$

When choosing the nodes to which to attach,assume a probability II for a node i proportional to $\prod ( k _ { i } ) = \frac { k _ { i } } { \sum k _ { j } }$the number ki of links already attached to it j

$n = t + m _ { 0 }$

After t time steps, the network will have$n = M + m _ { 0 }$nodes and M=mt edges M=mt

It can be shown that this leads to a power law network!

<!-- (a) New node  -->
![](https://web-api.textin.com/ocr_image/external/7df0671b43586e17.jpg)

<!-- (b) Time t1 t _ { 2 } XIXN JIA  -->
![](https://web-api.textin.com/ocr_image/external/202cf8a582b7698f.jpg)


![](https://web-api.textin.com/ocr_image/external/979a0e961846590b.jpg)

<!-- z(r) F _ { f } The most useful words F _ { r } r  -->
![](https://web-api.textin.com/ocr_image/external/f68714b614aa8b6d.jpg)

Luhn (1958) suggested that both extremely common and extremely uncommon words were not very useful for indexing.

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5e156666d215dc90.jpg)

XIAN JIAOTONG UNIVIRSITY

# 文本建模-Zipf＇s Law

# Zipf＇s Law对索引的作用

## 好的索引词汇

·词频太高：可能返回所有文档

·词频太低：仅能返回很少的文档


![](https://web-api.textin.com/ocr_image/external/ea7468d67ee9d199.jpg)

利用Zipf＇s Law可以去除频次高的Stopword，优化的倒排索引的时空开销

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d53d1493c47c550b.jpg)

XIAN JIAOTONG UNIVERSITY

# 文本建模-Zipf＇s Law


![](https://web-api.textin.com/ocr_image/external/6e1e2174dbf7ab96.jpg)

Doc 1 Doc 2 Doc 3 Doc 4

one fish, two fish red fish, blue fish cat in the hat green eggs and ham


![](https://web-api.textin.com/ocr_image/external/33494f10d6b9d4ca.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">4</td>
</tr><tr>
<td colspan="1" rowspan="1">blue</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">cat</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">egg</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">fish</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">green</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">ham</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">hat</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">one</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">red</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">two</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

<!-- blue 2 cat 3 egg 4 fish 1 2 green 4 ham 4 hat 3 one 1 red 2 two 1  -->
![](https://web-api.textin.com/ocr_image/external/8ba869c76e577e12.jpg)

## 倒排索引

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d53d1493c47c550b.jpg)

XIAN JIAOTONG UNIVIRSITY

# 文本建模-Heaps＇s Law


![](https://web-api.textin.com/ocr_image/external/bc9927f7f6f211e6.jpg)

Heaps在1978年提出Heaps＇sLaw。他观察到在语言系统中，词汇表的大小与文本篇幅（所有出现的单词累积数目）之间存在幂函数关系，其幂指数小于1

## Heaps＇s Law的作用

可以估测跟定文本集的词汇表的大小

可以预测随着文本集增长倒排索引规模的变化

## Heaps's Law:

V是词汇表的大小 with constants K, 0&lt;β&lt;1$V = K n ^ { \beta }$


![](https://web-api.textin.com/ocr_image/external/b70a989e41a722fc.jpg)

n是文本集词的个数

<!-- V Text size  -->
![](https://web-api.textin.com/ocr_image/external/196adfc75406f233.jpg)


![](https://web-api.textin.com/ocr_image/external/a3d496a58cadbbc6.jpg)

10≤K≤100,β≈0.4∼0.6

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d53d1493c47c550b.jpg)

XIAN JIAOTONG UNIVIRSITY

# 文本建模-Heaps＇s Law


![](https://web-api.textin.com/ocr_image/external/3792f651dd518d5d.jpg)

<!-- 12 Alice Reuters Tale Bible (SpuBsnouL)sedAL Po 10 8 6 4 2 0 0 10 20 30 40 50 60 70 80 90 100 Number of Words (Thousands)  -->
![](https://web-api.textin.com/ocr_image/external/8a6d72b23d0a5af1.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f3f3f7085a38bf35.jpg)

XIAN JIAOTONG UNIVIRSITY

文本建模-Heaps＇s Law


![](https://web-api.textin.com/ocr_image/external/15654d95ccee5af3.jpg)

We want to estimate the size of the vocabulary for a corpus of 1,000,000 words. However, we only know statistics computed on smaller corpora sizes:

For 100,000 words, there are 50,000 unique words

For 500,000 words, there are 150,000 unique words

Estimate the vocabulary size for the 1,000,000 words corpus

How about for a corpus of 1,000,000,000 words?

7.73

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a5a48c7e25f4190a.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/c32181eefdd0d4a4.jpg)

现实世界的数据集中，首位数字为1至9的样本数量并非均匀分布，而是从1至9随着数值增加，频率逐渐减少

1881年，天文学家Simon Newcomb发现对数表中以1起首的页较为破旧。

1938年，物理学家Frank Benford 做了20,229组观测（人口数量、出生率、物理化学常数等），给出Benford定律

1938年，WVU的Mark Nigrini将Benford定律作为审计工具，检测公司数据的异常

西安交通大學


![](https://web-api.textin.com/ocr_image/external/079a1f8e6e9cffd5.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/2075aa8219f9890d.jpg)

$P ( d ) = \log _ { 1 0 } ( d + 1 ) - \log _ { 1 0 } ( d ) = \log _ { 1 0 } ( \frac { d + 1 } { d } ) = \log _ { 1 0 } ( 1 + \frac { 1 } { d } )$

<table border="1" ><tr>
<td colspan="1" rowspan="1">适用性：</td>
</tr><tr>
<td colspan="1" rowspan="1">① d 大于9时仍适用；</td>
</tr><tr>
<td colspan="1" rowspan="1">②数据不是十进制仍适用；</td>
</tr><tr>
<td colspan="1" rowspan="1">③ 数量级跨度越大越符合，整个国家的家庭收入／一个村的家庭收入</td>
</tr></table>

<table border="1" ><tr>
<td colspan="1" rowspan="1">首位数字</td>
<td colspan="1" rowspan="1">比例</td>
</tr><tr>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">30.1%</td>
</tr><tr>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">17.6%</td>
</tr><tr>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">17.6%</td>
</tr><tr>
<td colspan="1" rowspan="1">4</td>
<td colspan="1" rowspan="1">9.7%</td>
</tr><tr>
<td colspan="1" rowspan="1">5</td>
<td colspan="1" rowspan="1">7.9%</td>
</tr><tr>
<td colspan="1" rowspan="1">6</td>
<td colspan="1" rowspan="1">6.7%</td>
</tr><tr>
<td colspan="1" rowspan="1">7</td>
<td colspan="1" rowspan="1">5.8%</td>
</tr><tr>
<td colspan="1" rowspan="1">8</td>
<td colspan="1" rowspan="1">5.1%</td>
</tr><tr>
<td colspan="1" rowspan="1">9</td>
<td colspan="1" rowspan="1">4.6%</td>
</tr></table>

<!-- 30 Distribution of Leading Digits (3)Aauanbeus 25 20 15 10 5 0 2 4 6 8 First Digit  -->
![](https://web-api.textin.com/ocr_image/external/0bb98b5d97a20013.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a384847b92f46b20.jpg)


![](https://web-api.textin.com/ocr_image/external/fdf5a36ba34f30f7.jpg)

由度量单位制获得的数据：人口数量、股票价格、半衰期、物理书中的答案、素数、Fibonacci 数列、任何数字的幂（如2、3）

不符合数据主要是任意获得的和受限数据，如彩票数字、电话号码、汽油价格、日期、体重、身高

Benford's Law

<!-- Physical Constants 0.4 0.35 0.3 ouenbeLu 0.25 0.2 0.15 0.1 0.05 0 1 2 3 4 5 6 7 8 First Digit 9  -->
![](https://web-api.textin.com/ocr_image/external/a384ecea8058a28c.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/000e7c40e32f1d47.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/8c1947f2b3e9f646.jpg)

<!-- Distance of stars from Earth in light years Leading digit frequency 30% 2 14.67% 3 12% 4 10.33% 5 9.33% 6 5.67% 7 7% 8 7% 9 囲中D简？： Predicted by Benford's Law  -->
![](https://web-api.textin.com/ocr_image/external/82f4be9c98ee662e.jpg)

<!-- File sizes in the Linux 2.6.39.2 source tree Leading digit frequency 30.31% 17.38% 13.03% 9.65% 5 7.78% 6 6.37% 7 5.74% 8 5.09% 9 4.64% Predicted by Benford's Law  -->
![](https://web-api.textin.com/ocr_image/external/8d8791e1fe2198b9.jpg)

## http://www.testingbenfordslaw.com/google-

books-1-grams

西安交通大學


![](https://web-api.textin.com/ocr_image/external/fb5b387e69d6404d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/743f6b15cb31ba93.jpg)

## 固定倍率增长的数据，由数字a增长到a＋1起首的数的时间比a+1到a+2需要更多时间。（CPU主频、证券市场指数）

<!-- x=1 2 10 d=1 d=2 d=3 d=4 d=5 d=6 d=7 d=8 d=9 y=0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  -->
![](https://web-api.textin.com/ocr_image/external/b8c871cbabb0c27e.jpg)

<!-- Initially uniform 1 2 3456789 1 [5, 10) maps to 1 2 T요 PeMeS [1, 1.5) maps to 2 3 [1.5,2) maps to 3 4 (2, 2.5) maps to 4 5 [2.5, 3) maps to 5 6 [3,3.5)maps to 6 7 (3.5,4) maps to 7 8 [4, 4.5) maps to 8 9 [4.5,5) maps to 9  -->
![](https://web-api.textin.com/ocr_image/external/87ee54eee828d620.jpg)

# 尺度不变形

对Benford定律的解释仍是开放问题。不影响其应用。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/bc4683797f658bca.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

Table 1. One of the columns gives the land area of political states and territories in k㎡2. The other column contains faked data,

generated with a random number generator.

<table border="1" ><tr>
<td colspan="1" rowspan="1">State/Territory</td>
<td colspan="2" rowspan="1">Real or Faked Area(k㎡)</td>
</tr><tr>
<td colspan="1" rowspan="1">Afghanistan</td>
<td colspan="1" rowspan="1">645,807</td>
<td colspan="1" rowspan="1">796,467</td>
</tr><tr>
<td colspan="1" rowspan="1">Albania</td>
<td colspan="1" rowspan="1">28,748</td>
<td colspan="1" rowspan="1">9,943</td>
</tr><tr>
<td colspan="1" rowspan="1">Algeria</td>
<td colspan="1" rowspan="1">2,381,741</td>
<td colspan="1" rowspan="1">3,168,262</td>
</tr><tr>
<td colspan="1" rowspan="1">American Samoa</td>
<td colspan="1" rowspan="1">197</td>
<td colspan="1" rowspan="1">301</td>
</tr><tr>
<td colspan="1" rowspan="1">Andorra</td>
<td colspan="1" rowspan="1">464</td>
<td colspan="1" rowspan="1">577</td>
</tr><tr>
<td colspan="1" rowspan="1">Anguilla</td>
<td colspan="1" rowspan="1">96</td>
<td colspan="1" rowspan="1">82</td>
</tr><tr>
<td colspan="1" rowspan="1">Antigua and Barbuda</td>
<td colspan="1" rowspan="1">442</td>
<td colspan="1" rowspan="1">949</td>
</tr><tr>
<td colspan="1" rowspan="1">Argentina</td>
<td colspan="1" rowspan="1">2,777,409</td>
<td colspan="1" rowspan="1">4,021,545</td>
</tr><tr>
<td colspan="1" rowspan="1">Armenia</td>
<td colspan="1" rowspan="1">29,743</td>
<td colspan="1" rowspan="1">54,159</td>
</tr><tr>
<td colspan="1" rowspan="1">Aruba</td>
<td colspan="1" rowspan="1">193</td>
<td colspan="1" rowspan="1">367</td>
</tr><tr>
<td colspan="1" rowspan="1">Australia</td>
<td colspan="1" rowspan="1">7,682,557</td>
<td colspan="1" rowspan="1">6,563,132</td>
</tr><tr>
<td colspan="1" rowspan="1">Austria</td>
<td colspan="1" rowspan="1">83,858</td>
<td colspan="1" rowspan="1">64,154</td>
</tr><tr>
<td colspan="1" rowspan="1">Azerbaijan</td>
<td colspan="1" rowspan="1">86,530</td>
<td colspan="1" rowspan="1">71,661</td>
</tr><tr>
<td colspan="1" rowspan="1">Bahamas</td>
<td colspan="1" rowspan="1">13,962</td>
<td colspan="1" rowspan="1">9,125</td>
</tr><tr>
<td colspan="1" rowspan="1">Bahrain</td>
<td colspan="1" rowspan="1">694</td>
<td colspan="1" rowspan="1">755</td>
</tr><tr>
<td colspan="1" rowspan="1">Bangladesh</td>
<td colspan="1" rowspan="1">142,615</td>
<td colspan="1" rowspan="1">347,722</td>
</tr><tr>
<td colspan="1" rowspan="1">Barbados</td>
<td colspan="1" rowspan="1">431</td>
<td colspan="1" rowspan="1">818</td>
</tr><tr>
<td colspan="1" rowspan="1">Belgium</td>
<td colspan="1" rowspan="1">30,518</td>
<td colspan="1" rowspan="1">47,123</td>
</tr><tr>
<td colspan="1" rowspan="1">Belize</td>
<td colspan="1" rowspan="1">22,965</td>
<td colspan="1" rowspan="1">20,648</td>
</tr><tr>
<td colspan="1" rowspan="1">Benin</td>
<td colspan="1" rowspan="1">112,620</td>
<td colspan="1" rowspan="1">97,768</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d6eb17907fb0da37.jpg)

各类NLU任务大量使用了不同的机器学习、数据挖掘算法，这些算法很难直接处理自然语言中的原始文本

自然语言特点：非规范、歧义、动态演化

需求：结构化、向量长度固定、数值型、可计算

语言模型：自然语言在不同语言单位上的数学模型，旨在实现自然语言的可计算性。

词袋模型：用文档中出现的词汇来表示文档

两个问题：词汇表、重要性度量

· 优点：简单有效；缺点：忽略词序、上下文、稀疏

<!-- the quick The quick brown brown dog dog さnb UMOJa 5op Cun Azel jumps over jumps→jump 11 2 1 1 the lazy dog. over the lazy dog  -->
![](https://web-api.textin.com/ocr_image/external/4300321a4c1c62f7.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/45ee3cc2f2abf9a5.jpg)

XIAN JIAOTONG UNiVreSiTY


![](https://web-api.textin.com/ocr_image/external/b3a4da9d6922c28a.jpg)

# 概率语言模型：根据给定词汇序列来预测下一个词汇的概念模型

$S = w _ { 1 } , w _ { 2 } , \cdots , w _ { k }$

$P ( S ) = p ( w _ { 1 } ) p ( w _ { 2 } \vert w _ { 1 } ) p ( w _ { 3 } \vert w _ { 1 } w _ { 2 } ) \cdots p ( w _ { k } \vert w _ { 1 } w _ { 2 } \cdots w _ { k - 1 } )$

简化： $P _ { n } ( S ) = \prod _ { i = 1 } ^ { k } P ( w _ { i } \vert w _ { i - 1 } , w _ { i - 2 } , w _ { i - 3 } , \cdots , w _ { i - n + 1 } )$

·N元文法： n=1 unigram;n=2: bigram;n=3: trigram

·n＞4的情况很少，随着n增加，复杂度增高，数据稀疏性问题严重，需要更大的语料库

·词序影响不大的NLU应用，如IR，取n=1

·词序影响较大大的NLU应用，如MT，取n=3 4

西安交通大學


![](https://web-api.textin.com/ocr_image/external/aceb7cf151eb5ce6.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/50be1d5cd7681235.jpg)

- Synonymy（一义多词）：poor recall

- Polysemy（一词多义）：poor precision

make

hidden

Markov

<!-- auto car engine emissions bonnet （引擎盖） hood （引擎盖） tyres （轮胎） Make（品牌） lorry （货车） model（型号） boot（行李箱） trunk（行李箱）  -->
![](https://web-api.textin.com/ocr_image/external/3d91cf97e8388139.jpg)

model

emissions（输出概率）

normalize

Synonymy Polysemy from Lillian Lee

Will have small cosine Will have large cosine

but are related! but not truly related

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d09e857a754a408d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/b1935181f3debc8f.jpg)

## 主题模型：利用非监督方法获得文档中隐含的主题

·非监督：数据集的类别（标签）未知，识别数据的簇与隐含模式

·隐含主题所在空间是低维的

分布假设（Distributional Hypothesis）：经常出现在相同上下文环境中的两个词具有语义上的相似性

主题模型假设文档中的词是通过以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语生成的。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/35f5c2e8726c5a17.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/db18a40e2873458a.jpg)

## 神经网络语言模型：利用神经网络学习词汇、句子、字符等的分布式表示。

·One-hot Encoding：高维、稀疏、正交、距离相等，祖母细胞

·分布式表示（Distributed representation）：将语义信息分布到不同相互独立的维度上，低维、稠密，语义计算

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="2" rowspan="1">wordV</td>
</tr><tr>
<td colspan="1" rowspan="1">Rome $= [ 1$</td>
<td colspan="1" rowspan="1">$0$ $0 ,$ 0,0</td>
<td colspan="1" rowspan="1">,0,.., 0]</td>
</tr><tr>
<td colspan="1" rowspan="1">$P a r i s = l 0 ,$</td>
<td colspan="1" rowspan="1">0,,0,,0,1,</td>
<td colspan="1" rowspan="1">,0,,0]</td>
</tr><tr>
<td colspan="1" rowspan="1">$I t a 1 y = l 0 ,$$, 0 ,$</td>
<td colspan="1" rowspan="1">1,$0 $ 0</td>
<td colspan="1" rowspan="1">,0,, 0]</td>
</tr></table>

<!-- 0.15 ofour 02 ocuatro (four) 01 0.1 ouno (one) 0.0% ofive 0 oone ocinco(five) -4.08 otres(three) -0.3 othree -0.4 -0.5 dos(two) 01 0.2 03 0.4 0.5 06 0.7 0 0.2 0.4 0.6 0.8 1 12  -->
![](https://web-api.textin.com/ocr_image/external/f6e242e2ac81f169.jpg)

陕西-西安＝河南 - 郑州

西安交通大學


![](https://web-api.textin.com/ocr_image/external/8c31057d7ac3fc32.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/946cf9624933ab57.jpg)


![](https://web-api.textin.com/ocr_image/external/61af0d8310b145ac.jpg)

# Bag of Words：文本（段落或者文档）被看作是无序的词汇集合

✓基本假设：词与词之间概率分布条件独立（在给定类别后每个词的概率分布与其他词无关）

✓忽略语法、单词顺序

<!-- John is quicker than Mary Mary is quicker than John From Christopher's ppt  -->
![](https://web-api.textin.com/ocr_image/external/ddb551b7b1a9b708.jpg)

✓简单，但是有效

## McDonald's slims down spuds

Fast-food chain to reduce certain types of fat in its french fries with new cooking oil.NEW YORK (CNN/Money) - McDonald's Corp. is cutting the amount of "bad" fat in its french fries nearly in half, the fast-food chain said Tuesday as it moves to make all its fried menu items healthier.

But does that mean the popular shoestring fries won't taste the same? The company says no. "It's a win-win for our customers because they are getting the same great french-fry taste along with an even healthier nutrition profile," said Mike Roberts,president of McDonald's USA.

But others are not so sure. McDonald's will not specifically discuss the kind of oil it plans to use, but at least one nutrition expert says playing with the formula could mean a different taste.

Shares of Oak Brook, Ill.-based McDonald's (MCD: down &#36;0.54 to &#36;23.22, Research,Estimates) were lower Tuesday afternoon. It was unclear Tuesday whether competitors Burger King and Wendy's International (WEN: down &#36;0.80 to &#36;34.91, Research,Estimates) would follow suit. Neither company could immediately be reached for comment.

"Bag of Words”

14 X McDonalds

12xfat

11Xfries


![](https://web-api.textin.com/ocr_image/external/ac45c73b3b57264b.jpg)

8Xnew

7xfrench

6 X company, said, nutrition

5x food, oil, percent, reduce,taste,Tuesday

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a45c355784140c3f.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## Stopword

List

<table border="1" ><tr>
<td colspan="1" rowspan="1">for</td>
</tr><tr>
<td colspan="1" rowspan="1">is</td>
</tr><tr>
<td colspan="1" rowspan="1">of</td>
</tr><tr>
<td colspan="1" rowspan="1">the</td>
</tr><tr>
<td colspan="1" rowspan="1">to</td>
</tr></table>

## Document 1

The quick brown fox jumped over the lazy dog's back.

## Document 2

Now is the time

for all good men

to come to the

aid of their party.

Tueuns00

ZIueuna00

Term

<table border="1" ><tr>
<td colspan="1" rowspan="1">aid</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">all</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">back</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">brown</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">come</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">dog</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">fox</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">good</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">jump</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">lazy</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">men</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">now</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">over</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">party</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">quick</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">their</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">time</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## The most frequent words are not the most descriptive.

<!-- / Upper cut-off Lower cut-off Resolving power of SpOeTOA2uanDeh significant words Significant words Words by rank order r  -->
![](https://web-api.textin.com/ocr_image/external/6e0b60045837e347.jpg)

Figwe 2.1.Aplot of the Asperbolic cose rehtng t the feueneyofoee and the cnfer Adaped frun Sbultzpage 122)

Slide from Mitch Marcus

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVIRSITY

## ·TF-IDF measure:

- Term frequency (tf)

- Inverse document frequency (idf)

·A way to deal with some of the problems of the Zipf distribution

# ·Goal: Assign a tf*idf weight to each term in each document

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

$w _ { i k } = t f _ { i k } * \log ( N / n _ { k } )$

$T _ { k } = t e r m k \in d o c u m e n t D _ { i }$

tfik=frequency of termTkin document Di

idf,=inverse document frequency of term T, in C

N =total number of documents in the collection C

nk=the number of documents in C that contain Tk

$i d f _ { k } = \log ( \frac { N } { n _ { k } } )$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## · IDF provides high values for rare words and low values for common words

$\log ( \frac { 1 0 0 0 0 } { 1 0 0 0 0 } ) = 0$

For a

collection

of 10000$\log ( \frac { 1 0 0 0 0 } { 5 0 0 0 } ) = 0 . 3 0 1$

documents

(N=10000) $\log ( \frac { 1 0 0 0 0 } { 2 0 } ) = 2 . 6 9 8$

$\log ( \frac { 1 0 0 0 0 } { 1 } ) = 4$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/2026408d3f40e045.jpg)

·Normalize the term weights (so longer documents are not unfairly given more weight)

· normalize usually means force all values to fall within a certain range, usually between 0 and 1,inclusive.

<!-- t f _ { i k } \log ( N / n _ { k } ) w _ { i k } = \frac { t f _ { i k } \log ( N / n _ { k } ) } { \sqrt { \sum _ { k = 1 } ^ { t } ( t f _ { i k } ) ^ { 2 } [ \log ( N / n _ { k } ) ] ^ { 2 } } }  -->
![](https://web-api.textin.com/ocr_image/external/d49e2c93f87208c4.jpg)

Sde Mitch Marcus

西安交通大學


![](https://web-api.textin.com/ocr_image/external/1261bf7831363b7b.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

Now, the similarity of two documents is:

$\sin ( D _ { i } , D _ { j } ) = \sum _ { k = 1 } ^ { t } w _ { i k } * w _ { j k }$

This is also called the cosine, or normalized inner product.

(Normalization was done when weighting the terms.)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## TFIDF例子

D1 : This is a database system textbook

D2 : Oracle database sells for &#36;1000 this year

D3:My oracle database textbook for my database class

Raw frequencies:

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Database</td>
<td colspan="1" rowspan="1">System</td>
<td colspan="1" rowspan="1">Textbook</td>
<td colspan="1" rowspan="1">Oracle</td>
<td colspan="1" rowspan="1">Sells</td>
<td colspan="1" rowspan="1">Year</td>
<td colspan="1" rowspan="1">class</td>
</tr><tr>
<td colspan="1" rowspan="1">D1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D2</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D3</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## TFIDF例子

D1 : This is a database system textbook

D2 :Oracle database sells for &#36;1000 this year

D3:My oracle database textbook for my database class

Normalized frequencies:

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Database</td>
<td colspan="1" rowspan="1">System</td>
<td colspan="1" rowspan="1">Textbook</td>
<td colspan="1" rowspan="1">Oracle</td>
<td colspan="1" rowspan="1">Sells</td>
<td colspan="1" rowspan="1">Year</td>
<td colspan="1" rowspan="1">class</td>
</tr><tr>
<td colspan="1" rowspan="1">D1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D2</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D3</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.5</td>
<td colspan="1" rowspan="1">0.5</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.5</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## TFIDF例子

D1 : This is a database system textbook

D2 :Oracle database sells for &#36;1000 this year

D3 :My oracle database textbook for my database class

Document frequencies:

<table border="1" ><tr>
<td colspan="1" rowspan="1">Database</td>
<td colspan="1" rowspan="1">System</td>
<td colspan="1" rowspan="1">Textbook</td>
<td colspan="1" rowspan="1">Oracle</td>
<td colspan="1" rowspan="1">Sells</td>
<td colspan="1" rowspan="1">Year</td>
<td colspan="1" rowspan="1">class</td>
</tr><tr>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

normalized term-frequency(tfij) Document frequency(df;)

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Database</td>
<td colspan="1" rowspan="1">System</td>
<td colspan="1" rowspan="1">Textbook</td>
<td colspan="1" rowspan="1">Oracle</td>
<td colspan="1" rowspan="1">Sells</td>
<td colspan="1" rowspan="1">Year</td>
<td colspan="1" rowspan="1">class</td>
</tr><tr>
<td colspan="1" rowspan="1">D1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D2</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D3</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.5</td>
<td colspan="1" rowspan="1">0.5</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.5</td>
</tr></table>

<table border="1" ><tr>
<td colspan="1" rowspan="1">Database</td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">Sells</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">System</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">Year</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">Textbook</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">Class</td>
<td colspan="1" rowspan="1">1</td>
</tr><tr>
<td colspan="1" rowspan="1">Oracle</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

$W _ { i j } = t f _ { i j } * \log ( d / d f _ { j } )$

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">Database</td>
<td colspan="1" rowspan="1">System</td>
<td colspan="1" rowspan="1">Textbook</td>
<td colspan="1" rowspan="1">Oracle</td>
<td colspan="1" rowspan="1">Sells</td>
<td colspan="1" rowspan="1">Year</td>
<td colspan="1" rowspan="1">class</td>
</tr><tr>
<td colspan="1" rowspan="1">D1</td>
<td colspan="1" rowspan="1">$1 * \log 3 / 3 = 1 * \log 3 / 1 \\ 0 = 1 . 5 4 8$</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1*log(3/2)$= 0 . 5 4 8$</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D2</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.548</td>
<td colspan="1" rowspan="1">1.548</td>
<td colspan="1" rowspan="1">1.548</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">D3</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">$0 . 5 * \log ( 3 / 2 )$$= 0 . 2 7 4$</td>
<td colspan="1" rowspan="1">0.274</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.774</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JIAOTONG UNIVIESITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

Discourse on Floating Bodies by Galileo Galilei

<!-- Galilei,Galileo water hath equall aristotle board grave sidenote altitude natation gravity swim prisme ebony cone rampart 0.000 0.002 0.004 0.006  -->
![](https://web-api.textin.com/ocr_image/external/81c331e322f56812.jpg)

<!-- Huygens,Christiaan refraction crystal ray spheroid rays reflexion ab movement ac refractions rc ethereal cm wave refracted cg 0.000 0.001 0.002 0.003 0.004  -->
![](https://web-api.textin.com/ocr_image/external/fbe4741befe0b659.jpg)

Treatise on Light by Christiaan Huygens

Experiments with Alternate Currents of High Potential and High Frequency by Nikola Tesla

<!-- Tesla,Nikola bulb coil wire discharge currents conducting frequencies frequency electrode current wires impulses globe condenser fig 0.000 0.002 0.004 0.006  -->
![](https://web-api.textin.com/ocr_image/external/425acfcd2388c8aa.jpg)

<!-- Einstein,Albert relativity theory gravitational ordinates CO ordinate continuum system dimensional transformation euclidean embankment carriage tf-idf 0.000 0.002 0.004 0.006 0.008  -->
![](https://web-api.textin.com/ocr_image/external/5e78e8932a932b7a.jpg)

Relativity:The Special and General Theory by Albert Einstein.

https://www.tidytextmining.com/tfidf.html

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## 相似度计算（文档之间、文档与查询之间）


![](https://web-api.textin.com/ocr_image/external/a8004fd64f2a9f95.jpg)

利用距离（欧氏距离）来计算文档之间的相似性

$d ( d _ { 1 } , d _ { 2 } ) > d ( d _ { 1 } , d _ { 3 } )$

<!-- d _ { 1 } d _ { 2 } d _ { 3 }  -->
![](https://web-api.textin.com/ocr_image/external/c58ac3c1299a638b.jpg)


![](https://web-api.textin.com/ocr_image/external/7d338c9674da7d40.jpg)

如果$d _ { 2 } = 2 ^ { * } d _ { 1 }$呢？ $( d _ { 2 }$是

$d _ { 1 }$复制自身后附加到文档尾部形成的文档）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f94f280a5de1996e.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/265ac9aca2159178.jpg)

## 相似度计算 $\cos ( d _ { 1 } , d _ { 2 } )$

$d _ { 1 } = w _ { d _ { 1 1 } } , w _ { d _ { 1 2 } } , \cdots , w _ { d _ { t } }$

$d _ { 2 } = w _ { d _ { 2 1 } } , w _ { d _ { 2 2 } } , \cdots , w _ { d _ { 2 t } }$

如果w被归一化： $\sin ( d _ { 1 } , d _ { 2 } ) = \sum _ { j = 1 } ^ { t } w _ { 1 j } * w _ { 2 j }$

如果w未被归一化： $\sin ( d _ { 1 } , d _ { 2 } ) = \frac { \sum _ { j = 1 } ^ { r } w _ { 1 j } * w _ { 2 j } } { \sqrt { \sum _ { j = 1 } ^ { t } ( w _ { 1 j } ) ^ { 2 } * \sum _ { j = 1 } ^ { t } ( w _ { 2 j } ) ^ { 2 } } }$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVERSITY

## 主题模型概述


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

## BoW模型的局限性

-Synonymy（一义多词）：poor recall

-Polysemy（一词多义）：poor precision

make

hidden

Markov

<!-- auto car engine emissions bonnet （引擎盖） hood （引擎盖） tyres （轮胎） Make（品牌） lorry （货车） model（型号） boot（行李箱） trunk（行李箱）  -->
![](https://web-api.textin.com/ocr_image/external/88d597ed8bb9e401.jpg)

model

emissions（输出概率）

normalize

Synonymy Polysemy

Will have small cosine Will have large cosine but

but are related! not truly related from Lillian Lee

西安交通大學


![](https://web-api.textin.com/ocr_image/external/62b56804d485a179.jpg)

XIAN JIAOTONG UNIVERSITY

主题模型概述


![](https://web-api.textin.com/ocr_image/external/c0784d3137b0e39b.jpg)

## Representing words by their context

# Core idea: A word's meaning is given by the words that frequently appear close-by

"You shall know a word by the company it keeps" (J. R. Firth 1957:11)

·One of the most successful ideas of modern statistical NLP!

· When a word w appears in a text, its context is the set of words that appear nearby (within a fixed-size window).

# Use the many contexts of w to build up a representation of w

<!-- ...government debt problems turning into banking crises as happened in 2009... ...saying that Europe needs unified banking regulation to replace the hodgepodge... ...India has just given its banking system a shot in the arm... These context words will represent banking  -->
![](https://web-api.textin.com/ocr_image/external/566fec2b6083d2a8.jpg)

from Richard Socher's PPT(https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture2.pdf) 西安交通大學


![](https://web-api.textin.com/ocr_image/external/77fb08df1c74b548.jpg)

XIAN JIAOTONG UNIVERSITY

## 主题模型概述


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

主题模型通过分析文档中的词以发现蕴藏于其中的主题结构，无须事前标注文档

主题模型假设文档中的词是通过以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语生成的。

主题模型的中心问题就是利用文档推断出隐藏的主题结构，也就是产生文档的逆过程

(词语|文档)=∑p（词语｜主题）xp（主题｜文档）主题

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a7d59152d76d256f.jpg)

XIAN JIAOTONG UNIVIRSITY

## 主题模型概述


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">gene 0.04dna 0.02genetic 0.01</td>
</tr></table>

<!-- Topics Documents Topic proportions and assignments Seeking Life's Bare (Genetic) Necessities COLD SPRING HARBOR, NEW YORK- How many genes does an orginism nerd to "are not all that far apart,"especially in survive Last week at the genome meeting comparison to the 75,000 genes in the hu- here,"two genome researchers with radically mgenome.notes Siv Anderssonsal life 0.02 different approaches presentedl complemen- University in Swercho arrived ar the tary views of the hasic genes needed forlife 800numer.But coming up with a consen sus answer may be more than just a poeic evolve 0.01 One research team,using computer analy ses to compare known genomes, concluded numbers gmeprticlarmoreand organism 0.01 that toxday'sorpintsms can he sustained with more genomes are cletemeda just 250 genes,and that the earliest life forms sequenced."It may be a way of organizing required a mere 128 genes The any newly sequenced genome," explains other researcher mapped genes Arcady Mushegian,acomputational mo- in a simple parasite and esti- lecular biologist at the National Center mated that for this organism, Masnenhss for Biotechnology Information(NCBI) genome in Bethesda,Maryland.Comparing alt 800 genes are plenty to dothe 1703 ganes job-but that anythingshort Genes Redundsnt and needed 0a(254-50405 F60om gare5 of 100 wouldn't beenough Gares Tor beecherad -4 ganes TETENEd in conmon -122 gases nerve 0.01 Although the numbers don't 231 003 +22 geres match precisely,those pnolictions Mysoplaane 25% L Minlmel gename genes gene set 230 genes L 128 469 genes geres Hoearal *Genome Mapping and Sequenc- 09e 581 品2N AOtdGLa2 ing.Cold Spring Harbor,New York, May 8 to 12. Stripping down.Computer analysis yields an esti- mate of the minimum moder and ancient genomes SCIENCE·VOL. 272·24 MAY 1996  -->
![](https://web-api.textin.com/ocr_image/external/d614381adcf9630a.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">brain 0.04neuron 0.02</td>
</tr></table>

<table border="1" ><tr>
<td colspan="1" rowspan="1">data 0.02number 0.02computer 0.01</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/62b56804d485a179.jpg)

XIAN JIAOTONG UNIVIRSITY

### 主题模型的挑战


![](https://web-api.textin.com/ocr_image/external/bc9927f7f6f211e6.jpg)

## 主题事先未知，不是一个分类问题！

<!-- 主题1 主题2 主题3 主题4  -->
![](https://web-api.textin.com/ocr_image/external/bce12f51f25f0675.jpg)

此外科学家还有另一种方法推测月球岩石的年龄-“撞击坑定年法”：行星或卫星表面越古老的区域，累计遭受陨石的撞击越多，撞击坑数量也就越多；反之，越年轻的区域，累积遭受的撞击越少。通过统计行星或卫星表面单位面积内的撞击坑大小和数量，科学家可以估算这块区域的形成年代。而本次嫦娥五号采样的地点，正是通过撞击坑定年法推测较为年轻的月球区域。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0bbdf3831affdc86.jpg)

XIAN JIAOTONG UNIVIRSITY

## 主题模型的挑战


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

# 机器不理解单词、短语、句子等的语义。


![](https://web-api.textin.com/ocr_image/external/cd598cc8d856b116.jpg)

此外科学家还有另一种方法推测月球岩石的年龄-“撞击坑定年法”：行星或卫星表面越古老的区域，累计遭受陨石的撞击越多，撞击坑数量也就越多；反之，越年轻的区域，累积遭受的撞击越少。通过统计行星或卫星表面单位面积内的撞击坑大小和数量，科学家可以估算这块区域的形成年代。而本次嫦娥五号采样的地点，正是通过撞击坑定年法推测较为年轻的月球区域。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a7d59152d76d256f.jpg)

XIAN JIAOTONG UNIVIRSITY

## 主题模型的挑战


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

# 机器不理解单词、短语、句子等的语义。


![](https://web-api.textin.com/ocr_image/external/4b77fe0632192ea2.jpg)

<!-- eu 9n9° hpt m?0XgLXndC P+6mL yo-?wen P＋ムTC XHコPAω↑Ye十（hnA-73）ω-m＋yの-？h47mg 9A9①·ATP7リω-ヘプラピラのー?λのーウトデω·xプヘhラのー?9? 9eラチ xプクh 3の-?λPアアカ ウωーx3%+ e89A? X7H.U TPチP3ΑAUヘラチP7スTアセアチ Sチの：ΛXフUT下PナARPカム のA PAm-n nu39:hchteト 93 ATPセアF An Pay87Tω-hxツルXのれC少Aのλ↑ イチ Y00- Aλ71LU9° U7 ヘPHアラリチの↑ア虫四下 のAチチの0の％小s中 PAmク下の A：n＋neg nHGTC+ ag4①-nTスSブアAクロ  -->
![](https://web-api.textin.com/ocr_image/external/fc4f05c62485fb48.jpg)

非洲阿姆哈拉语

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a7d59152d76d256f.jpg)

XIAN JIAOTONG UNIVERSITY

## 主题模型概述


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

## 主要的主题模型

概率化

<table border="1" ><tr>
<td colspan="1" rowspan="1">LSA(SVD)</td>
<td colspan="1" rowspan="1">特点：利用SVD发现隐含主题，无监督、降维、处理一义多词局限性：一词多义、维度选取、复杂度高、缺少物理解释</td>
</tr><tr>
<td colspan="1" rowspan="1">pLSA</td>
<td colspan="1" rowspan="1">特点：概率模型，物理解释、一词多义局限性：概率模型不完备（文档的生成）、计算量大</td>
</tr><tr>
<td colspan="1" rowspan="1">LDA</td>
<td colspan="1" rowspan="1">特点：dirichlet先验，不易过拟合</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">局限性：主题之间关系的建模</td>
</tr><tr>
<td colspan="1" rowspan="1">NMF</td>
<td colspan="1" rowspan="1">特点：非负矩阵分解，物理解释局限性：灵活性不如LDA</td>
</tr></table>

贝叶斯化

西安交通大學


![](https://web-api.textin.com/ocr_image/external/62b56804d485a179.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a95291e2d34e4ca6.jpg)

Latent Semantic Analysis（LSA）：于1988年由Susan Dumais等人提出，用于解决关键词检索中由于一义多词引起的漏检索和误检索的问题。

Latent: present but not evident, hidden

LSA通过分析文档及其包含词汇的关系，发现与文档与词汇相关的隐含概念

适用于两类模态构成的二元组数据

西安交通大學


![](https://web-api.textin.com/ocr_image/external/31f3d6a29d510c78.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/8229fa8f4a8f1385.jpg)

分布假设（Distributional Hypothesis）：经常出现在相同上下文环境中的两个词具有语义上的相似性

LSA利用奇异值分解 （Singular Value Decomposition，SVD）把文档与词汇映射到一个隐含语义空间，维度也是隐含概念数，远小于词汇数。在这个空间中，同义词相互更接近

在信息检索中，也被称为Latent Semantic Indexing （LSI）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/90b05107ee5ebf00.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/48e487e07199d825.jpg)

奇异值分解（Singular Value Decomposition，SVD）-获取一般矩阵的重要特征。通过降维，找出隐含的“模式”，用在模式识别、数据压缩等方面$A = U \sum V ^ { T }$. (3.1)

$U \in R ^ { N \times N }$：矩阵$A A ^ { T }$的特征向量$u _ { 1 } , u _ { 2 } , \cdots , u _ { N }$（也称为左奇异向量）作为列构成的正交矩阵，满足$U U ^ { T } = E ;$,

· V∈RM×M：矩阵$A ^ { T } A$的特征向量$v _ { 1 } , v _ { 2 } , \cdots , v _ { M }$（也称为右奇异向量）作为列构成的正交矩阵，满足$V V ^ { T } = E ;$

$\sum \in R ^ { N \times M }$：是一个对角矩阵，主对角线上由大到小排列的非零数值$\sigma _ { 1 } \geq \sigma _ { 2 } \geq \cdots \geq$ $\sigma _ { R } > 0$称为奇异值，是$A A ^ { T }$或$A ^ { T } A$特征值的平方根；Σ记作$d i a g ( \sigma _ { 1 } , \sigma _ { 2 } , \cdots , \sigma _ { R } )$。

<!-- 0 VT A = U 0 m×n m×m m×n  -->
![](https://web-api.textin.com/ocr_image/external/08177612365250a6.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/9d5aaa340510181f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

# 奇异值分解（Singular Value Decomposition， SVD）

## $A _ { k }$与A是mxn矩阵，k&lt;&lt;r.

$A = U \sum V ^ { T } = ( u _ { 1 } , u _ { 2 } , \cdots , u _ { m } ) d i a g ( \sigma _ { 1 } , \sigma _ { 2 } , \cdots , \sigma _ { r } ) ( v _ { 1 } , v _ { 2 } , \cdots , v _ { n } ) ^ { T }$

$= \sum _ { i = 1 } ^ { r } \sigma _ { i } u _ { i } v _ { i } ^ { T }$

$A _ { k } = \sum _ { i = 1 } ^ { k } \sigma _ { i } u _ { i } v _ { i } ^ { T } .$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/6a93855fde140dda.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="2">例3.1 对矩阵</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">进行奇异值分解。</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

解答： $A = U \sum V ^ { T } = [ \begin{matrix} 0 . 1 3 & 0 . 7 8 & 0 . 6 1 \\ 0 . 4 5 & 0 . 5 1 & - 0 . 7 4 \\ 0 . 8 9 & - 0 . 3 7 & 0 . 2 8 \end{matrix} ] \times [ \begin{matrix} 6 8 . 6 8 & 0 & 0 \\ 0 & 0 & 0 & 1 . 1 7 & 0 \end{matrix} ]$

$\times [ \begin{matrix} 0 . 3 7 & - 0 . 4 9 & - 0 . 3 8 & 0 . 6 9 \\ 0 . 4 6 - 0 . 6 4 & 0 . 3 2 & - 0 . 6 7 \\ 0 . 5 2 & 0 . 6 7 & - 0 . 3 9 \\ 0 . 6 1 & 0 . 6 1 & 0 . 6 1 & 0 . 4 8 & 0 . 3 0 \end{matrix} ]$

$A = 6 8 . 6 8 \times [ \begin{matrix} 0 . 0 4 9 & 0 . 0 6 1 & 0 . 0 6 8 & 0 . 0 6 8 & 0 . 0 8 0 \\ 0 . 1 6 6 & 0 . 2 3 2 & 0 . 2 3 2 & 0 . 2 7 4 \\ 0 . 3 2 9 & 0 . 4 6 0 & 0 . 2 4 6 \times [ \begin{matrix} - 0 . 2 4 6 & - 0 . 3 2 3 & 0 $0.1760.3720.241

$+ 1 . 1 7 \times [ \begin{matrix} - 0 . 2 3 0 & 0 . 1 9 9 & - 0 . 4 1 2 \\ 0 . 2 7 7 & - 0 . 2 3 9 & 0 . 4 9 7 \\ - 0 . 1 0 5 & 0 . 0 9 1 & - 0 . 1 8 9 & 0 \end{matrix}$-0.406337.155

西安交通大學


![](https://web-api.textin.com/ocr_image/external/10b1ad61a99faf81.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/6a93855fde140dda.jpg)

# 奇异值分解（Singular Value Decomposition， SVD）

## ·SVD可用于计算最佳的低秩近似。旨在找出秩为k的近似矩阵$A _ { k }$

$A _ { k } = \arg \min _ { A ^ { \prime } \in R ^ { m \times n } , r a n k ( A }$ $r a n k ( A ^ { \prime } ) = k$ $\vert \vert A - A ^ { \prime } \vert \vert *$

·矩阵$A ^ { \prime } \in R ^ { m \times n }$的秩为k.

·＊可以是F或2，分别表示F-范数（Frobenius 范数）与2-范数；

$\vert \vert A \vert \vert F = \sqrt { \sum _ { i = 1 } ^ { m } } \sum _ { j = 1 } ^ { n } \vert a _ { i j } \vert ^ { 2 } ,$ $\vert \vert A \vert \vert _ { 2 } = \sigma _ { 1 } .$

·对于$A - A _ { k }$，可推导出：

$\vert \vert A - A _ { k } \vert \vert F = \sqrt { \sum _ { i = k + 1 } ^ { r } \sigma _ { i } ^ { 2 } } , \vert \vert A - A _ { k } \vert \vert _ { 2 } = \sigma _ { k + 1 } .$

## Eckart-Young-Mirsky 定理

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f94f280a5de1996e.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/6a93855fde140dda.jpg)

# 奇异值分解（Singular Value Decomposition， SVD）

<!-- 1 6 0.5 0.2 0 1 2 3 4 5 6 7 8  -->
![](https://web-api.textin.com/ocr_image/external/55deb385a3ea8500.jpg)

MRI图像

文本

<!-- 250 original \frac { 4 } { 6 } 40 0 1 3 L. 20  -->
![](https://web-api.textin.com/ocr_image/external/50b08a7cf816864b.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/10b1ad61a99faf81.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)


![](https://web-api.textin.com/ocr_image/external/63d839068d35e889.jpg)


![](https://web-api.textin.com/ocr_image/external/f2ab42e315a64b14.jpg)


![](https://web-api.textin.com/ocr_image/external/89b714c506eafe40.jpg)

原图（1280x865） K=20 K=100


![](https://web-api.textin.com/ocr_image/external/84092b7abe5b85f1.jpg)


![](https://web-api.textin.com/ocr_image/external/fb0e59a0566a747d.jpg)


![](https://web-api.textin.com/ocr_image/external/6899731682579f9b.jpg)

原图（1280x865） 引入高斯噪声 去噪效果

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/25a20e52ad6fce59.jpg)

# 奇异值分解（Singular Value Decomposition，SVD）


![](https://web-api.textin.com/ocr_image/external/3c07c30fa5e73ba9.jpg)

Jupyter

## 用随机矩阵看一下

# 奇异值的变化趋势

西安交通大學


![](https://web-api.textin.com/ocr_image/external/595ec5bfaf218030.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/e6b103c160803db2.jpg)

## LSA的四个步骤

## ① 构建Term-by-Document matrix Atxd

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">D1</td>
<td colspan="1" rowspan="1">D2</td>
<td colspan="1" rowspan="1">D3</td>
<td colspan="1" rowspan="1">D4</td>
<td colspan="1" rowspan="1">D5</td>
</tr><tr>
<td colspan="1" rowspan="1">complexity</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">3</td>
</tr><tr>
<td colspan="1" rowspan="1">algorithm</td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">4</td>
<td colspan="1" rowspan="1">4</td>
</tr><tr>
<td colspan="1" rowspan="1">entropy</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">traffic</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">network</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">4</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

<!-- t 2 Funtion bonod 2 of t 2 We study the complexity i of influencing elections through bribery: How computationally complex is it for an external actor to determine whether by a certain amount of bribing voters a specified S candidate can be made the election's winner? We study this problem for election systems as varied as scoring...  -->
![](https://web-api.textin.com/ocr_image/external/834f67526bf5f68d.jpg)


![](https://web-api.textin.com/ocr_image/external/7e34b37bd8aff98b.jpg)

Term-document matrix

## ② 对矩阵的每个元素赋予权重（如TF／IDF）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/39711b9a940f0782.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/10bb04cda34f6250.jpg)

## LSA的四个步骤

③ 奇异值分解： $A = U \sum V ^ { T }$

④ 利用k-维(k&lt;&lt;n)语义空间生成 A 的低秩近似矩阵


![](https://web-api.textin.com/ocr_image/external/716bff567aad8244.jpg)

Frobenius norm

（弗罗贝尼乌斯）

$\vert \vert A \vert \vert _ { F } = \sqrt { \sum _ { i = 1 } ^ { m } \sum _ { j = 1 } ^ { n } \vert a _ { i j } \vert ^ { 2 }$-

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d09e857a754a408d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a21df183eda9da6b.jpg)

## 奇异值分解

-奇异值在矩阵Σ中从大到小排列，而且减少特别快，很多情况下，前10％甚至1％的奇异值的和就占了全部奇异值之和的99％。可用前k个的奇异值来近似描述矩阵

<!-- A \approx A _ { k } = U _ { k } \sum _ { k } V _ { k } T Term Vectors k k k Document Vectors = A _ { k } U Σ VT k  -->
![](https://web-api.textin.com/ocr_image/external/84f0b97db9f211da.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ba7464a55713bcfc.jpg)

XIAN JIAOTONG UNIVIRSITY

### 3.3.2 LSA模型


![](https://web-api.textin.com/ocr_image/external/09214c18e1ee620b.jpg)

k值选取

两难问题与“特设”的问题

✓方法一：碎石图（Scree plot）

✓方法二：Guttman-Kaiser 准则

✓方法三：奇异值能量

<!-- 2.50 2.25 anlen JelIn6uIS 2.00 1.75 1.50 1.25 1.00 0.75 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Rank 5.0  -->
![](https://web-api.textin.com/ocr_image/external/25ffa86fc0f02e0b.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0d42d950f8fe9caa.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

## 奇异值分解


![](https://web-api.textin.com/ocr_image/external/c28360eaf1fce434.jpg)

$U _ { k }$与$V _ { k }$的每一列对应一个隐含概念


![](https://web-api.textin.com/ocr_image/external/b57167e98281a398.jpg)

根据隐含概念，词汇与文档具有一个新的表示

·词汇被表示为$U _ { k } \sum _ { k }$的行向量

·文档被表示为$\sum _ { k } V _ { k }$的列向量

·查询（有一个或多个词汇构成）可表示词汇对应向量的中心点

在低维空间，计算词汇、文档、查询之间的相似度或相关度

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/ae5e4dc1bb68a999.jpg)

## 文档集中的文档

<!-- A _ { K } \in R ^ { N \times M } U _ { K } \in R ^ { N \times K } \sum _ { K } \in R ^ { K \times K } V _ { K } ^ { T } \in R ^ { K \times M } w _ { 1 } w _ { 1 } ) X \sigma _ { 2 } .·. v _ { m } ^ { T } σK d _ { m } d _ { 1 } d _ { M } u _ { 1 } \cdots u _ { k } \cdots u _ { K } w _ { N } w _ { N } d _ { 1 } d _ { M }  -->
![](https://web-api.textin.com/ocr_image/external/577a9416ba29aa43.jpg)

$\hat { d } _ { m } = \sum _ { K } v _ { m } ^ { T } .$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/8bbeeb2e117e0b21.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/ae5e4dc1bb68a999.jpg)

## 文档集中的词汇

<!-- A _ { K } ^ { T } \in R ^ { M \times N } V _ { K } \in R ^ { M \times K } \sum _ { K } \in R ^ { K \times K } U _ { K } ^ { T } \in R ^ { K \times N } d _ { 1 } 02 u _ { n } ^ { T } w _ { n } \therefore O K ^ { . } O K ^ { . } O K 个 )K w _ { 1 } w _ { N } d _ { M } w _ { 1 } w _ { N }  -->
![](https://web-api.textin.com/ocr_image/external/b9e8a40cd03fc216.jpg)

$\hat { w } _ { n } = \sum _ { K } u _ { n } ^ { T }$.

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY

3.3.2 LSA模型


![](https://web-api.textin.com/ocr_image/external/ef6eee9133ffdfef.jpg)

## 文档集中的新文档

$U _ { K } ^ { T } d _ { m } = \sum _ { K } v _ { m } ^ { T } \rightarrow$ $\hat { d } _ { n e w } = U _ { K } ^ { T } d _ { n e w } .$

查询的表示

$\hat { q } = U _ { K } ^ { T } q .$ $\hat { q } = \frac { 1 } { \vert W _ { q } \vert } \sum _ { i \in W _ { q } } \sum _ { K } u _ { i } ^ { T }$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a7d59152d76d256f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

例3.4利用LSA对以下五个文档进行分析，并完成两项任务：一、对文档集进行聚类；二、对以“达芬奇＋艺术”为关键词的查询结果进行排序；三、对新文档“科幻是基于科技发展的艺术想象”在主题空间进行表示。

文档集：

$d _ { 1 }$：达芬奇的代表作包括《蒙娜丽莎》。

$d _ { 2 }$：大卫像是文艺复兴时期米开朗基罗的代表作。

$d _ { 3 }$：米开朗基罗、达芬奇是文艺复兴时期的艺术领域代表性人物。

$d _ { 4 }$：电影是科技与艺术相结合的产物。

$d _ { 5 }$：不少科幻电影中的科技已经成为现实。

# 词汇表：

［1：达芬奇 2：代表作 3：文艺复兴 4：米开朗基罗 5：艺术 6：电影

7：科技 8：科幻］

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/3a6db976464d424f.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">达芬奇</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">代表作</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">文艺复兴</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">米开朗基罗</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">艺术</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">电影</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">科技</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">科幻</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/2ff2a4ffe539d76f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/bd501b43bb15ea0a.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="8"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">0.42</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">-0.28</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">-0.14</td>
<td colspan="1" rowspan="1"> -0.47</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.28</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">0.11</td>
<td colspan="1" rowspan="1">-0.71</td>
<td colspan="1" rowspan="1">$0$</td>
<td colspan="1" rowspan="1">-0.14</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">0.11</td>
<td colspan="1" rowspan="1">0.71</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">$- 0 . 1 4$</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">-0.30</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.57</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">-0.11</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">-0.71</td>
<td colspan="1" rowspan="1"> -0.28</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">-0.11</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.71</td>
<td colspan="1" rowspan="1">-0.28</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

$\{ \begin{matrix} 2 5 1 & 0 & 0 . 0 3 1 \\ 0 & 2 2 0 & 0 & 0 & 0 \\ 0 & 0 1 1 3 & 0 \\ 0 & 0 1 1 3 & 0 \\ 0 & 0 & 1 1 7 & 0 \\ 0 & 0 & 0 . 0 1 1 \\ 0 & 0 & 0 & 0 & 0 . 3 1 \\ 0 & 0 & 0 & 0 & 0 & 0 . 3 1 \\ 0 & 0 & 0 & 0 &$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f66c5b26e1656451.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

Kaiser 准则。利用该准则得到的K值为4，因为只有一个奇异值（0.81）小于1。

奇异值的能量。假设ε=0.85,，前2个奇异值的能量占比为0.74，前3个奇异值的能量占比为0.86，因此，在当前设定下应选取K=3。

$\frac { 2 . 5 1 ^ { 2 } + 2 . 2 0 ^ { 2 } } { 2 . 5 1 ^ { 2 } + 2 . 2 0 ^ { 2 } + 1 . 3 5 ^ { 2 } + 1 . 1 7 ^ { 2 } + 0 . 8 1 ^ { 2 } } = 0 . 7 4 < 0 . 8 5 ,$

(3.15)

$\frac { 2 . 5 1 ^ { 2 } + 2 . 2 0 ^ { 2 } + 1 . 3 5 ^ { 2 } } { 2 . 5 1 ^ { 2 } + 2 . 2 0 ^ { 2 } + 1 . 3 5 ^ { 2 } + 1 . 1 7 ^ { 2 } + 0 . 8 1 ^ { 2 } } = 0 . 8 6 > 0 . 8 5 .$

奇异值的熵。同样假设ε=0.85，五个奇异值的能量占比分别为0.42、0.32、0.12、0.09、0.04，根据公式3.9，$E ( A _ { 3 } )$的熵值占比为0.73，$E ( A _ { 4 } )$的熵值占比为0.90，由此

可得，K值应为4。

0.42log0.42+0.32log0.32+0.12log0.12

0.42log0.42+0.32log0.32+0.12log0..12+0.09log0.09+0.04log0.04=0.73&lt;0.85, (3.16)

0.42log0.42+0.32log0.32+0.12log0.10.42log0.42+0.32log0.32+0.12lo $\frac { 0 . 1 2 \log 0 . 1 2 + 0 . 0 9 \log 0 . 0 9 } { 0 . 1 2 + 0 . 0 9 \log 0 . 0 9 + 0 . 0 4 \log 0 . 0 4 } = 0 . 9 0 > 0 . 8 5 $

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/6a93855fde140dda.jpg)

$U _ { 2 2 } , v _ { 2 2 } ^ { 3 3 } = 0 . 1 7 1 \\ 0 . 3 2 \times 1 7 } = 0 . 2 3 \\ 0 . 4 9 = 0 . 2 3 \\ 0 . 4 3 = 0 . 2 3 \\ 0 . 0 2 1 0 ^ { 0 . 3 1 } \times 0 . 0 3 1 \\ 0 . 0 3 4 0 \\ 0 . 0 3 3 1 1 \end{matrix}$

$d _ { 1 }$ 20 $d _ { 4 } d _ { 5 }$ $\sum _ { 2 } V _ { 2 } ^ { T } = ( \begin{matrix} 0 . 7 1 & 1 . 2 9 & 1 . 7 8 & 0 . 8 4 \\ - 0 . 3 9 - 0 . 6 7 - 0 . 4 3 1 . 3 7 \end{matrix}$10.51

<table border="1" ><tr>
<td colspan="1" rowspan="1">$w _ { 1 }$</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">$w _ { 2 }$</td>
<td colspan="1" rowspan="1">$w _ { 3 }$</td>
<td colspan="1" rowspan="1">$w _ { 4 }$</td>
<td colspan="1" rowspan="1">W5</td>
<td colspan="1" rowspan="1">$w _ { 6 }$</td>
<td colspan="1" rowspan="1">$w _ { 7 }$</td>
<td colspan="1" rowspan="1">$w _ { 8 }$</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1.23</td>
<td colspan="1" rowspan="1">1.23</td>
<td colspan="1" rowspan="1">1.06</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">$0 . 5 3 1 . 3 $0.53</td>
<td colspan="1" rowspan="1">0.20$0 . 6 6$</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">-0.51</td>
<td colspan="1" rowspan="1">-0.51</td>
<td colspan="1" rowspan="1">0.42</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">1.30</td>
<td colspan="1" rowspan="1"></td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f94f280a5de1996e.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/6a93855fde140dda.jpg)

查询：达芬奇＋艺术

<!-- 1.5 d _ { 5 } 2 ,A x w _ { 6 } ( 7 ) 1.0 ^ { \star } d _ { n e w } w _ { 8 } + 0.5 w _ { 5 } + 0.0 ·q -0.5 d _ { 1 } w _ { 2 } + w _ { 3 } ( (4) d _ { 3 } + w _ { 1 } + d _ { 2 } 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75  -->
![](https://web-api.textin.com/ocr_image/external/108296b4c5f811d2.jpg)

新文档“科幻是基于科技发展的艺术想象”的表示

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/843f0eb458f05a28.jpg)

# A 100 Millionths of a Typical

# Document-Term Matrix

<table border="1" ><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

From Zornitsa Kozareva's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/1d7a8b9ee3af86b1.jpg)

XIAN JIAOTONG UNIVIRSITY

## LSA模型-实际应用中的几个问题


![](https://web-api.textin.com/ocr_image/external/7aa2f6abce2cf76f.jpg)

## 问题1：时空开销过大


![](https://web-api.textin.com/ocr_image/external/705897750a91e5a3.jpg)

✓时间复杂度：0（n＾3），n是文档数，实际应用中文档可能是千万级的

✓不支持增量式计算

## 问题2：k值的选择


![](https://web-api.textin.com/ocr_image/external/83d311ad1577ea8c.jpg)

✓k较小的话，能对文档集中的文档做宽泛的比较；较大的话，能对文档集中的文档具体的比较

✓K受集合中文档数的影响。对于上万个文档，通常k大约取300；对于数百万个文档，k大约取400。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ab1f5e1f8901d749.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/af33fcb7e1daf1c6.jpg)

问题3：为什么要去掉第一个奇异值？

✓对于文档，第一维度与文档的长度相关

✓对于单词，它与单词在所有文档中的使用次数相关

✓去中心化可以解决这个问题，即把矩阵A每一列减去所有列的平均值；但是会减低矩阵的稀疏性

<table border="1" ><tr>
<td colspan="1" rowspan="1">$T o m e o = [ \begin{matrix} - 0 . 9 0 5 \\ 0 . 3 0 3 \end{matrix} ] - j 市 \dot { I } \dot { I } \dot { I } \dot { I } \dot { I } + = [ \begin{matrix} - 0 . 7 1 7 \\ 0 . 5 0 5 \end{matrix} ] , h o n g w = [ \begin{matrix} - 0 . 4 0 7 \\ 0 . 5 1 1 \end{matrix} ] - 1 . 0 0 1 \\ 0 . 4 2 \end{matrix} ]$</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
</tr></table>

$\lim _ { h \rightarrow 0 . 6 0 3 } ] , d i c = [ \begin{matrix} - 1 . 1 9 7 \\ - 0 . 4 9 4 \end{matrix} ] , f _ { 1 } c c = [ \frac { - 0 . 6 0 3 } { - 0 . 6 9 5 } ] ,$ $\pi e w - h u a n p s h i t i t - [ \frac { - 0 . 7 4 5 } { - 0 . 9 2 5 } ]$

$d _ { 1 } = [ \frac { - 0 . 7 1 1 } { 0 . 7 3 0 } ] , d _ { 2 } = [ \frac { - 0 . 9 3 0 } { 1 . 0 8 7 } ] ,$ $d _ { 3 } = [ \frac { - 1 . 3 5 7 } { 0 . 4 0 2 } ] ,$ $d _ { 4 } = [ \frac { - 1 . 3 7 8 } { - 1 . 3 9 7 } ] ,$ $, d _ { 5 } = [ \frac { - 0 . 3 2 7 } { - 0 . 4 6 0 } ]$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/5b4548eca2b3d820.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/2075aa8219f9890d.jpg)

利用LSA在TOEFL考试中，在“四选一”选择题中识别同义词／近义词。

✓预料数据：美联社新闻稿、《美国学术百科全书》、代表性儿童读物、Wikipedia

✓LSA达到65％的正确率，与非英语母语的国家的考试者相当

# Netflix 发起Netflix 奖公开竞赛，设计最新的算法来预测电影评级

✓用户和项目被表示为潜在因子向量空间里的向量，评级被定义为两个向量之间的点积

✓BellKor团队提出的基于SVD的算法获得了100万美元的奖金。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/282294d473b19be9.jpg)

XIAN JIAOTONG UNIVIRSITY

<!-- SVD的应用  -->
![](https://web-api.textin.com/ocr_image/external/fac2eeecf59ca812.jpg)


![](https://web-api.textin.com/ocr_image/external/80d83058f2f66d01.jpg)

# 两类模态的二元组数A∈Rd×n

向量。

<table border="1" ><tr>
<td colspan="1" rowspan="1">Data</td>
<td colspan="1" rowspan="1">Columns</td>
<td colspan="1" rowspan="1">Rows</td>
<td colspan="1" rowspan="1">d$1 0 ^ { 5 } - 1 0 ^ { 7 }$</td>
<td colspan="1" rowspan="1">$> 1 0 ^ { 1 0 }$</td>
<td colspan="1" rowspan="1">sparse</td>
</tr><tr>
<td colspan="1" rowspan="1">Textual</td>
<td colspan="1" rowspan="1">Documents</td>
<td colspan="1" rowspan="1">Words</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">yes</td>
</tr><tr>
<td colspan="1" rowspan="1">Actions</td>
<td colspan="1" rowspan="1">Users</td>
<td colspan="1" rowspan="1">Types</td>
<td colspan="1" rowspan="1">$1 0 ^ { 1 } - 1 0 ^ { 4 }$$1 0 ^ { 5 } - 1 0 ^ { 6 }$$\square { > 1 0 ^ { 8 } }$</td>
<td colspan="1" rowspan="1">$> 1 0 ^ { 7 }$</td>
<td colspan="1" rowspan="1">yes</td>
</tr><tr>
<td colspan="1" rowspan="1">Visual</td>
<td colspan="1" rowspan="1">Images</td>
<td colspan="1" rowspan="1">Pixels,SIFT</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">no</td>
</tr></table>

## 图像数据压缩

## https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-

Singular-Value-Decomposition/

<!-- m*n→?  -->
![](https://web-api.textin.com/ocr_image/external/33da1a2a6878189d.jpg)

用户画像与推荐

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY

<!-- SVD的应用  -->
![](https://web-api.textin.com/ocr_image/external/0387e44512db97b3.jpg)


![](https://web-api.textin.com/ocr_image/external/80d83058f2f66d01.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/b622771a7956ae0a.jpg)

## Apply the SVD on images


![](https://web-api.textin.com/ocr_image/external/4d76f1e405237cca.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/8bbeeb2e117e0b21.jpg)

XIAN JIAOTONG UNIVERSITY

<!-- SVD的应用  -->
![](https://web-api.textin.com/ocr_image/external/e976c4982bb0c0ac.jpg)


![](https://web-api.textin.com/ocr_image/external/2075aa8219f9890d.jpg)

Rd5n列

<table border="1" ><tr>
<td colspan="1" rowspan="1">Data</td>
<td colspan="1" rowspan="1">Columns</td>
<td colspan="1" rowspan="1">Rows</td>
<td colspan="1" rowspan="1">d$1 0 ^ { 5 } - 1 0 ^ { 7 }$</td>
<td colspan="1" rowspan="1">$\frac { n } { > 1 0 ^ { 1 0 } }$</td>
<td colspan="1" rowspan="1">sparse</td>
</tr><tr>
<td colspan="1" rowspan="1">Textual</td>
<td colspan="1" rowspan="1">Documents</td>
<td colspan="1" rowspan="1">Words</td>
<td colspan="1" rowspan="1">$\square { > 1 0 ^ { 7 } }$</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">yes</td>
</tr><tr>
<td colspan="1" rowspan="1">Actions</td>
<td colspan="1" rowspan="1">Users</td>
<td colspan="1" rowspan="1">Types</td>
<td colspan="1" rowspan="1">$1 0 ^ { 1 } - 1 0 ^ { 4 }$$1 0 ^ { 5 } - 1 0 ^ { 6 }$</td>
<td colspan="1" rowspan="1">$> 1 0 ^ { 8 }$</td>
<td colspan="1" rowspan="1">yes</td>
</tr><tr>
<td colspan="1" rowspan="1">Visual</td>
<td colspan="1" rowspan="1">Images</td>
<td colspan="1" rowspan="1">Pixels,SIFT</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">no</td>
</tr></table>

(a) 5 singular Vectors (b) 10 singular Vectors (c)25 singular Vectors


![](https://web-api.textin.com/ocr_image/external/5ae8325c90022c88.jpg)


![](https://web-api.textin.com/ocr_image/external/0db4099d2b07204d.jpg)


![](https://web-api.textin.com/ocr_image/external/ed38351ce7c61de4.jpg)

## 图像数据压缩

187*187 -&gt; k*(187+187+1)

(c) 35 singular Vectors (c)50 singular Vectors (d) Full image

用户画像与推荐 187


![](https://web-api.textin.com/ocr_image/external/c51c3ce6d6c4e256.jpg)


![](https://web-api.textin.com/ocr_image/external/78f342d86b21f954.jpg)


![](https://web-api.textin.com/ocr_image/external/9b4c53177756d680.jpg)

西安交通大學

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/80f601bcad50c74e.jpg)

## LSA的优点

主题空间表示可以缓解一义多词问题，意义相近的词会为对应同一主题。

✓降维可去除噪。

无监督过程。

## LSA的局限性

发现的“隐含概念”可能难以解释

-{(car),,(truck),,(flower)}→{1.3452*car+0.2828*truck),(flower)}

vehicle

- {(car),(bottle),,(flower)}→{(1.3452*ca?/0.2828 * bottle),(flower)}

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d15ac927c50b865e.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/d0c97130e6ae03f9.jpg)

LSA的局限性 from WIKIPEDIA

负号难以解释

✓没有考虑词频

✓LSA对上下文更敏感，nurse比doctor更接近physician

✓LSA无法发现一词多义，不同上下文中的词被当做具有相同的词义。

- the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same

对于m＊n矩阵，时间复杂度： O(mn^2);当有新文档时，需重新训练模型。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY

## 语言模型评估


![](https://web-api.textin.com/ocr_image/external/214475c48de2e8af.jpg)

## 困惑度Perplexity：评测一个概率模型预测样本的性能


![](https://web-api.textin.com/ocr_image/external/6c4bf5d30ea4beb2.jpg)

语言模型给测试集的句子赋予的概率（由单词数量进行了标准化）越大，语言模型越好，困惑度越低。

$P P ( W ) = P ( w _ { 1 } w _ { 2 } \cdots w _ { N } ) ^ { - \frac { 1 } { N } }$

$= \sqrt [ N ] { \frac { 1 } { P ( w _ { 1 } w _ { 2 } \cdots w _ { N } } ) }$

$P P ( W ) = ^ { N } \vert \prod _ { i = 1 } ^ { N } \frac { 1 } { P ( w _ { i } \vert w _ { 1 } \cdots w _ { i - 1 } ) }$ $P P ( W ) = ^ { N } \vert \prod _ { i = 1 } ^ { N } \frac { 1 } { P ( w _ { i } \vert w _ { i - 1 } ) }$

Chain rule bigrams

在Image Caption任务中，$P P L ( w _ { 1 } : L \vert I )$是根据图像I给出句子描述$w _ { 1 } :$&lt;语言的Perplexity。$P ( w _ { n } \vert w _ { 1 } : n - 1 , I )$是图像I与单词序列$w _ { 1 } :$n-1生成单词$w _ { n }$的概率$l o g _ { 2 } P P L ( w _ { 1 } : L \vert I ) = - \frac { 1 } { L } \sum _ { n = 1 } ^ { L } l o g _ { 2 } P ( w _ { n } \vert w _ { 1 , n - 1 } , I )$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/12fc61d5fc899430.jpg)

XIAN JIAOTONG UNIVERSITY

## 语言模型评估


![](https://web-api.textin.com/ocr_image/external/02e6119bb5999188.jpg)

### 困惑度Perplexity

· How hard is the task of recognizing digits

'0,1,2,3,4,5,6,7,8,9,:

perplexity=10

· How hard is recognizing (30,000) names at Microsoft.perplexity = 30,000

· If a system has to recognize

-A(1 in 4)

-B(1 in 4)

- C Support (1 in 4)

- D (1 in 120,000 each)

- Perplexity is ?

### Slide from Josh Goodman

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ab1f5e1f8901d749.jpg)

XIAN JIAOTONG UNIVIRSITY

## 语言模型评估


![](https://web-api.textin.com/ocr_image/external/e4fee80eac6f9963.jpg)

### 困惑度Perplexity

N-gram模型：Goodman的结果，其中|V|=50000,

·Trigram model: $p ( x _ { 1 } \cdots x _ { n } ) = \prod _ { i = 1 } ^ { n } q ( x _ { i } \vert x _ { i - 2 } , x _ { i - 1 } ) . P e r p l e x i t y = 7 4$

·Bigram model: $p ( x _ { 1 } \cdots x _ { n } ) = \prod _ { i = 1 } ^ { n } q ( x _ { i } \vert x _ { i - 1 } )$Perplexity=137

·Unigram model: $p ( x _ { 1 } \cdots x _ { n } ) = \prod _ { i = 1 } ^ { n } q ( x _ { i } )$ Perplexity=955

<!-- 具体到生成模型的困惑度 第m个文档 P ( \hat { M } V \vert M ) = \prod _ { M = 1 } ^ { M } p ( \hat { u } _ { M } ^ { * } \vert M ) ^ { - \frac { 1 } { N } } = \exp - \frac { \sum _ { n = 1 } ^ { M } \ln g p ( \hat { v } _ { m } ^ { * } \vert M ) } { \sum _ { m = 1 } ^ { M } N P ( \hat { N } _ { m } \vert M ) = \prod _ { n = 1 } ^ { N _ { m } } \sum _ { k = 1 } ^ { K } p ( w _ { n } = t \vert z _ { n } = k ) \cdot p ( z _ { n } = k \vert d = j i n ) 第m个文档的长度  -->
![](https://web-api.textin.com/ocr_image/external/3a109dd58dd15ab5.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/7bc566ddae9dcdb3.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

Probabilistic latent semantic analysis （pLSA）：1999年被Thomas Hofmann 提出

·同LSA类似，pLSA也引入隐含主题，每对(d,w)存在表示“主题”的隐藏变量z∈Z

·在概率框架对双模式或共现信息进行建模

pLSA假设每对(d,w)是由下面过程产生的：

·以P(d)概率选择文档d

<!-- d Z W document topic word  -->
![](https://web-api.textin.com/ocr_image/external/9d451823876bea7e.jpg)

·选定d后，以P(z|d)概率选中主题z

·选中主题z后，以P(w|z)概率选中单词w

P(z|d) P(w|z)

$p ( w _ { i } ,$ $, d _ { j } ) = p ( d _ { j } )$ $[ \sum _ { z } p ( w _ { i } \vert z ) p ( z \vert d _ { j } ) ]$ 假设P(z|d)与P(w|z)是多项分布

图书馆推荐

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

Probabilistic latent semantic analysis （pLSA）：1999年被Thomas Hofmann 提出

·同LSA类似，pLSA也引入隐含主题，每对(d,w)存在表示“主题”的隐藏变量z∈Z

·在概率框架对双模式或共现信息进行建模

$P ( D , W ) = \sum _ { Z } \frac { P ( Z ) P ( D \vert Z ) } { } P ($ Z)

<!-- d Z W document topic word P(z|d) P(w|z)  -->
![](https://web-api.textin.com/ocr_image/external/4ce25ddbdfaa2197.jpg)

2 $A \approx \underline { U _ { t } S _ { t } V _ { t } ^ { T } }$

$p ( w _ { i } ,$,dj)=$p ( _ { } [ \sum _ { z } p ( w _ { i } \vert z$jp(z|d;)|

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/a59586cb0b495d83.jpg)

### 预备知识

·生成模型及其图表达（Graphical Model）

·多项分布（Multinomial distribution）

·期望最大化算法（Expectation-Maximum）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0b1b1718decf7c56.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/3ba2ac6cb196ec83.jpg)

### Unigram model

<!-- W P(w) TV politics TV cooking health Z 生成文档 P ( W ) = \prod _ { n = 1 } ^ { N } P ( w _ { n } ) politics design health Cooking 的概率 词的先 验概率 all words in all documents are generated  -->
![](https://web-api.textin.com/ocr_image/external/f872756ec0ff5b9e.jpg)

from a distribution

7

From Tomoharu Iwata's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/78213539cef9650c.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/db49599f7541dcae.jpg)

## Mixture of unigrams

<!-- Z W politics tax election law politics 一篇文档只由一 P(z) 个主题生成 topic proportions for all documents  -->
![](https://web-api.textin.com/ocr_image/external/45b0ee6fe8a2dc38.jpg)

<!-- P(w|z) ... Z media newS Copyright program ... politics Congress election president medical hospital health sick food  -->
![](https://web-api.textin.com/ocr_image/external/d3a11579c639a2f8.jpg)

$P ( W ) = \sum _ { z } P ( z ) \prod _ { n = 1 } ^ { N } P ( w _ { n } \vert z )$

·draw a topic for each document

· all words in a document are generated from a distribution

## From Tomoharu Iwata's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f119ec1a414cf540.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/5b98cce96c1d0687.jpg)

# Topic model

<!-- P(w|z) ... N media newS Copyright program  -->
![](https://web-api.textin.com/ocr_image/external/92c055aa067c2694.jpg)

<!-- Z W politics medical tax hospital politics P(z|θ) topicproportions for each document  -->
![](https://web-api.textin.com/ocr_image/external/cac4b5fde6e46d9b.jpg)

<!-- politics CongresS election aX president  -->
![](https://web-api.textin.com/ocr_image/external/b86544182c1ed0ed.jpg)

$P ( W ) = \prod _ { n = 1 } ^ { N } \sum _ { z } P ( z \vert \theta ) P ( w _ { n } \vert z )$

<!-- ... medical hospital health Sick food  -->
![](https://web-api.textin.com/ocr_image/external/1c395069182a27df.jpg)

·draw a topic for each word

· words in a document can be generated from multiple distributions

9

## From Tomoharu Iwata's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/58546bbf186e0ac8.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/dd2c81c3dd82189a.jpg)

## 生成模型的图模型（Graphical Model）

结点及其颜色、边、矩形框的含义？

<!-- N W _ { d } d Z W P(d) P(z|d) P(w/z) PLAS  -->
![](https://web-api.textin.com/ocr_image/external/783eabe767ecb3ac.jpg)

假设P（z｜d）与P（w｜z）

是多项分布

<!-- P(z) P(w|z) Z W W N  -->
![](https://web-api.textin.com/ocr_image/external/a7cf09103e275dfa.jpg)

Unigram

<!-- P(w) W W  -->
![](https://web-api.textin.com/ocr_image/external/733c3b924173570a.jpg)

Mixture of Unigrams

N

西安交通大學


![](https://web-api.textin.com/ocr_image/external/a69ee148eca31124.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/9e5c28d08468aac1.jpg)

# 伯努利试验（Bernoulli experiment）

重复固定次数（n次）。

✓每个试验只有两个可能的结果，即“成功”和“失败”。

每次试验的成功概率均相同。将p表示成功的概率（在每个试验中），将q=1-p表示失败的概率。

✓试验是独立的（先前结果对下一试验的结果没有影响）。

A bag contains 6 red marbles and 4 blue marbles. Five marbles are drawn from the bag without replacement and the number of red marbles is observed. We might let a trial here consist of drawing a marble from the bag and let success be getting a red.

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ac9eba3683e0f2ba.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/d42091fd3d639838.jpg)

# 伯努利试验（Bernoulli experiment）

重复固定次数（n次）。

✓每个试验只有两个可能的结果，即“成功”和“失败”。

每次试验的成功概率均相同。将p表示成功的概率（在每个试验中），将q=1-p表示失败的概率。

✓试验是独立的（先前结果对下一试验的结果没有影响）。

##  伯努利分布

P(x=1)=∅

P(x=0)=1-φ

$P ( x = x ) = \Phi ^ { x } ( 1 - \phi ) ^ { 1 - x }$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0d42d950f8fe9caa.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/1ac7f8ca07a75697.jpg)

二项分布（Binomial distribution）：重复n次独立的伯努利试验，在每次试验中只有两个可能值，且概率恒定。

$P r ( k ; n , p ) = P r ( X = k ) = ( k ) p ^ { k } ( 1 - p ) ^ { n - k }$

多项分布（Multinomial distribution）：二项分布的推广。某随机实验有k个可能结果$X _ { 1 } , X _ { 2 } , \cdots , X _ { k } ,$ 概率分别是$p _ { 1 } ,$ $p _ { 2 } , \cdots , p _ { k } ,$ 那么在n次总结果中，$X _ { 1 }$出现$x _ { 1 }$次， $X _ { 2 }$出现$x _ { 2 }$次···$X _ { k }$出现$x _ { k }$次的事件的概率为：

$f ( x _ { 1 } , \cdots , x _ { k } ; n _ { 1 } , p _ { 1 } , \cdots , p _ { k } ) = P _ { r } ( X _ { 1 } = x _ { 1 }$and$1 \cdots a n d X _ { k } = x _ { k } )$

$= \{ \begin{matrix} \frac { n ! } { x _ { 1 } ! \cdots x _ { k } ! } p _ { 1 } ^ { x _ { 1 } } \times \cdots \times p _ { k } ^ { x _ { k } } , \end{matrix}$ when$\sum _ { i = 1 } ^ { k } x _ { i } = n$ otherwise,

$\frac { n ! } { x _ { 1 } ^ { 1 } x _ { 2 } ^ { 1 } \cdots x _ { k } ^ { 1 } } = ( x _ { 1 } ) ( \frac { n - x _ { 1 } } { x _ { 2 } } ) \cdots ( n - x _ { 1 } - x _ { 2 } - x _ { k - 1 } )$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/b429b859306f60a0.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/543966717a0be24b.jpg)

The probability that Player A would win is 0.40, Player B would win is 0.35, and the game would end in a draw is 0.25. The multinomial distribution can be used to answer questions such as: "If these two chess players played 12 games, what is the probability that Player A would win 7 games, Player B would win 2 games, and the remaining 3 games would be drawn?"

西安交通大學


![](https://web-api.textin.com/ocr_image/external/181668c14d3ab3a8.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/aafd4b8159b9f17c.jpg)

# 求解P(z|d)与P(w|z)

P(z|d)与P(w|z)的参数个数分别是NK与KD（K是超参，需要事先给出）

·使得似然函数最大化的$P ( z _ { k } \vert d _ { i } )$与$P ( w _ { j } \vert z _ { k } )$是合理的

什么分布？

$\prod _ { i = 1 } ^ { N } \prod _ { j = 1 } ^ { M } P ( w _ { j } \vert d _ { i } ) ^ { n ( d _ { i } , w _ { j } )$ $n ( d _ { i } , w _ { j } )$表示$w _ { j }$出现在$d _ { i }$中的次数 文档 主题

$\log ( \prod _ { i = 1 } ^ { N } \prod _ { j = 1 } ^ { M } P ( w _ { j } \vert d _ { i } ) ^ { n ( d _ { i } , w _ { j } ) )$

<!-- d _ { 1 } P(z|d) P(w|z) z _ { 1 } w _ { 1 } d _ { 2 } w _ { 2 } z _ { K } d _ { N } w _ { M } z是隐含变量  -->
![](https://web-api.textin.com/ocr_image/external/820568c85272347a.jpg)

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) \log P ( w _ { j } \vert d _ { i } )$

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) \log \sum _ { k = 1 } ^ { K } P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } )$


![](https://web-api.textin.com/ocr_image/external/8bbeeb2e117e0b21.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/12de52db16e26c68.jpg)

期望最大化（Expectation-Maximum，EM）算法

·在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐含随机变量

<!-- 已知 未知 数据分布模型 最大似然估计 分布模型的参数 观测数据  -->
![](https://web-api.textin.com/ocr_image/external/c5a77e65091eb4df.jpg)

·能求解的问题：已知20名学生的身高，但是不清楚他们的性别，如何估计男生与女生身高的均值？身高符合正态分布，且标准差为10cm。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/284bdf93e700e81f.jpg)

XIAN JIAOTONG UNIVIRSITY

期望最大化（Expectation-Maximum，EM）算法

·在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐含随机变量

<!-- 已知 未知 数据分布模型 最大似然估计 分布模型的参数 观测数据  -->
![](https://web-api.textin.com/ocr_image/external/297cfe8e1e364423.jpg)

·观测数据 $\{ x ^ { ( 1 ) } , \cdots , x ^ { ( m ) } \}$

·找出似然函数L（0）中参数θ的最大似然估计

<!-- d _ { 1 } P(z|d) P(w|z) z _ { 1 } w _ { 1 } d _ { 2 } w _ { 2 } z _ { K } d _ { N } z.是隐含变量 w _ { M }  -->
![](https://web-api.textin.com/ocr_image/external/b3ce397592bd61c9.jpg)

$L ( \theta ) = L ( x ^ { ( 1 ) } , \cdots , x ^ { ( m ) } ; \theta ) = \prod _ { i = 1 } ^ { m } p ( x ^ { ( i ) } ; \theta )$

$\log L ( \theta ) = \sum _ { i = 1 } ^ { m } \log p ( x ^ { ( i ) } ; \theta )$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d5af118ac4308a6b.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/169604f8b59d1ce4.jpg)

## EM算法

·设对于每个$x ^ { ( i ) }$ $Q _ { i }$是隐含变量z的分布$\sum _ { z } Q _ { i } ( z ) = 1$

$\sum _ { i } \log p ( x ^ { ( i ) } ; \theta ) = \sum _ { i } \log \sum _ { z ( i ) } p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta )$

$= \sum _ { i } \log \sum _ { z ( i ) } Q _ { i } ( z ^ { ( i ) } ) \frac { p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) } { Q _ { i } ( z ^ { i } ) }$

&gt; $\sum _ { i } \sum _ { z ( i ) } Q _ { i } ( z ^ { ( i ) } ) \log \frac { p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) } { Q _ { i } ( z ^ { i } ) }$

Log(x)?

Jensen不等式：设f是实数域的凸函数（二次导数大于0），X是随机变量，那么E[f(X)]≥f(EX)

<!-- f(a) f E[f(X)] f(b) f(EX) a E[X] b  -->
![](https://web-api.textin.com/ocr_image/external/9ea1874e4d6da62c.jpg)

当X为常数时，取等号。凹函数时，不等号方向反向。

$f ( \sum _ { i = 1 } ^ { M } \lambda _ { i } x _ { i } ) \leq \sum _ { i = 1 } ^ { M } \lambda _ { i } f ( x _ { i } )$

Andrew Ng's Lecture notes

西安交通大學


![](https://web-api.textin.com/ocr_image/external/54ef8328e5f62ce5.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/e66b61b3520866a3.jpg)

## EM算法

上述公式给出了logL（θ）的下界，应该选择合适的$Q _ { i }$使得等号成立

·根据Jensen不等式， $\frac { p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) } { Q _ { i } ( z ^ { ( i ) } ) } = c$ ，即 $Q _ { i } ( z ^ { ( i ) } ) \propto p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) .$

又由于$\sum _ { z } Q _ { i } ( z ^ { ( i ) } ) = 1$ $Q _ { i } ( z ^ { ( i ) } ) = \frac { p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) } { \sum _ { z } p ( x ^ { ( i ) } , z ; \theta ) } = \frac { p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) } { p ( x ^ { ( i ) } ; \theta ) }$ $= p ( z ^ { ( i ) } \vert x ^ { ( i ) } ; \theta )$

·算法流程：初始化参数θ，重复执行E步骤与M步骤，直到收敛（收敛

性可证明） Repeat until convergence {

(E-step) For each i, set

$Q _ { i } ( z ^ { ( i ) } ) = p ( z ^ { ( i ) } \vert x ^ { ( i ) } ; \theta ) .$

(M-step)Set

$\theta : = \arg \max \sum _ { \theta } \sum _ { i } \sum _ { z ( i ) } Q _ { i } ( z ^ { ( i ) } ) \log \frac { p ( x ^ { ( i ) } , z ^ { ( i ) } ; \theta ) } { Q _ { i } ( z ^ { ( i ) } ) } .$

Andrew Ng's Lecture notes 西安交通大學


![](https://web-api.textin.com/ocr_image/external/119e777513150c3f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

### EM算法-

## 几何解释

<!-- θ(t+2) log P(x;θ) θ(t+1) θ(t) q _ { t + 1 } gt  -->
![](https://web-api.textin.com/ocr_image/external/0a428ad8b5a87dd0.jpg)

Supplementary Figure 1 Convergence of the EM algorithm. Starting from initial parameters θ(),the E-step of the EM algorithm constructs a function 9t that lower-bounds the objective function logP(x;θ)..In the M-step,$\theta ^ { ( t + 1 ) }$is computed as the maximum of 9t.In the next E-step,a new lower-boundgt+1is constructed; maximization of 9t+1 in the next M-step gives$\theta ^ { ( + 2 ) }$,etc.

https://medium.com/@chloebee/the-em-algorithm-explained-52182dbb19d9 西安交通大學


![](https://web-api.textin.com/ocr_image/external/bd20a22eb932ea04.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/e66b61b3520866a3.jpg)

## EM算法-抛硬币例子

·假设有$C _ { A } 与$ $C _ { B }$两个硬币，$C _ { A }$硬币落地为面的概率为$\theta _ { A } ,$ $C _ { B }$硬币落地为面的概率为$\theta _ { B }$

·可以通过一组抛硬币实验估计$( \theta _ { A } , \theta _ { B } )$ 随机选择一个硬币，进行10次抛硬币，共抛50次

$\hat { \theta } _ { A } = \frac { \neq o f h e a d s u \sin g c o \in A } { t o t a l \neq o f f f i p s u \sin g c o \in A }$ $\hat { \theta } _ { B } = \frac { \neq o f h e a d s u \sin g c o \in B } { t o t a l \neq o f f f i p s u \sin g c o \in B }$

$\hat { \theta } _ { A } = \frac { 2 4 } { 2 4 + 6 } = 0 . 8 0$

<!-- B A A A  -->
![](https://web-api.textin.com/ocr_image/external/3d1c76c1c265be5f.jpg)

<!-- HTTTHHTHTH HHHH T H H H H H HTHHHHHTHH HTHTTTHHTT THHHTHHHTH  -->
![](https://web-api.textin.com/ocr_image/external/008471e59d543ba5.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">Coin A</td>
<td colspan="1" rowspan="1">Coin B</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">5H,5T</td>
</tr><tr>
<td colspan="1" rowspan="1">9H,1T</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">8H,2T</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">4H,6T</td>
</tr><tr>
<td colspan="1" rowspan="1">7H,3T</td>
<td colspan="1" rowspan="1"></td>
</tr><tr>
<td colspan="1" rowspan="1">24H,6T</td>
<td colspan="1" rowspan="1">9H,11T</td>
</tr></table>

$\hat { \theta } _ { B } = \frac { 9 } { 9 + 1 1 } = 0 . 4 5$

Do C B, Batzoglou S. What is the expectation maximization algorithm?. Nature Biotechnology, 2008. 西安交通大學


![](https://web-api.textin.com/ocr_image/external/119e777513150c3f.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/10debda9fc53e42a.jpg)

## EM算法-抛硬币例子

## ·现在假设没有办法区分硬币的标识，硬币标识$( P _ { A } ,$ $P _ { B } )$ 是隐变量，在这个设定下估计（$( \theta _ { A } , \theta _ { B } )$

$P _ { A } = C _ { 1 0 } ^ { 5 } ( \theta _ { A } ) ^ { 5 } ( 1 - \theta _ { A } ) ^ { 1 0 - 5 }$=0.2

$P _ { B } = C _ { 1 0 } ^ { 5 } ( \theta _ { B } ) ^ { 5 } ( 1 - \theta _ { B } ) ^ { 1 0 - 5 }$=0.246

归一化， $P _ { A } = 0 . 4 5$ $P _ { B } = 0 . 5 5$

<!-- Coin A Coin B ≈2.2H,2.2T ≈2.8H,2.8T ≈7.2H,0.8T ≈1.8H,0.2T ≈5.9H,1.5T ≈2.1H,0.5T ≈1.4H,2.1T ≈2.6H,3.9T ≈4.5H,1.9T ≈2.5H,1.1T ≈21.3H,8.6T ≈11.7H,8.4T  -->
![](https://web-api.textin.com/ocr_image/external/317af2602bc254b0.jpg)

<!-- E-step 2 HTTTHHTHTH HHHHTHHHHH 0 . 4 5 \times \textcircled { A } 0.55× HTHHHHHTHH HTHTTT HTT 0 . 8 0 \times \textcircled { ▲ } 0.20× THHHTH TH 0.73× 0.27× 0 . 3 5 \times \textcircled { &lt; } 0 . 6 5 \times \hat { \theta } _ { A } ^ { ( 0 ) } = 0 . 6 0 0 . 6 5 \times \textcircled { ▲ } 0.35× \hat { \theta } _ { B } ^ { ( 0 ) } = 0 . 5 0 \hat { \theta } _ { A } ^ { ( 1 ) } \approx \frac { 2 1 . 3 } { 2 1 . 3 + 8 . 6 } \approx 0 . 7 1 3 M-step  -->
![](https://web-api.textin.com/ocr_image/external/b16e0308338c5eb2.jpg)

<!-- 1  -->
![](https://web-api.textin.com/ocr_image/external/44ab06ffcdbed75f.jpg)

<!-- \hat { \theta } _ { A } ^ { ( 1 0 ) } \approx 0 . 8 0 4 \hat { \theta } _ { B } ^ { ( 1 0 ) } \approx 0 . 5 2  -->
![](https://web-api.textin.com/ocr_image/external/46487c786eb6ea42.jpg)

$\hat { \theta } _ { B } ^ { ( 1 ) } \approx \frac { 1 1 . 7 } { 1 1 . 7 + 8 . 4 } \approx 0 . 5 8$

Do C B, Batzoglou S. What is the expectation maximization algorithm?. Nature Biotechnology, 2008.西安交通大學


![](https://web-api.textin.com/ocr_image/external/079a1f8e6e9cffd5.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/8f73f6266c844fde.jpg)

## EM算法-抛硬币例子

## ·现在假设没有办法区分硬币的标识，硬币标识是隐变量，在这个设定下估计$( \theta _ { A } , \theta _ { B } )$ 在当前$P _ { A } ,$ $P _ { B } 下 ,$，估计头尾的

数量：

$A ^ { \prime \prime } : H = 0 . 4 5 \times 5 h e a d s$

=2.2heads;T=0.45×

0.55x 5tans=2.2tans


![](https://web-api.textin.com/ocr_image/external/19679470db3089ec.jpg)

<!-- E-step 2 HTTTHHTHTH HHHHT HHHHH 0 . 4 5 \times \textcircled { ▲ } HTHI HTHH HTHT HTT 0 . 8 0 \times \textcircled { ▲ } THHHTHHHTH 0 . 7 3 \times \textcircled { ▲ } 0.35×0 \hat { \theta } _ { A } ^ { ( 0 ) } = 0 . 6 0 0 . 6 5 \times \textcircled { ▲ }  -->
![](https://web-api.textin.com/ocr_image/external/7f514e7f9b89f39b.jpg)

0.20× ·"B":H=0.55×5heads


![](https://web-api.textin.com/ocr_image/external/7c9d39f2def8df9d.jpg)

=2.8heads;T=0.55×

<!-- Coin Coin B ≈2.2H,2.2T ≈2.8H.2.8T ≈7.2H,0.8T ≈1.8H,0.2T ≈5.9H, 1.5T ≈2.1H,0.5T ≈1.4H,2.1T ≈2.6H,3.9T ≈4.5H,1.9T ≈2.5H,1.1T ≈21.3H,8.6T ≈11.7H,8.4T  -->
![](https://web-api.textin.com/ocr_image/external/3ffe11b3984742a8.jpg)


![](https://web-api.textin.com/ocr_image/external/236adc8cfb5f780c.jpg)

0.27×5tails=2.8tails

0.65×


![](https://web-api.textin.com/ocr_image/external/880d9e10786cd812.jpg)

0.35×


![](https://web-api.textin.com/ocr_image/external/da0458c3ab7be5f6.jpg)


![](https://web-api.textin.com/ocr_image/external/217c700d4a6483fd.jpg)

$\hat { \theta } _ { B } ^ { ( 0 ) } = 0 . 5 0$

<!-- 1  -->
![](https://web-api.textin.com/ocr_image/external/2edfefdeae1f487f.jpg)

$\hat { \theta } _ { A } ^ { ( 1 ) } \approx \frac { 2 1 . 3 } { 2 1 . 3 + 8 . 6 } \approx 0 . 7 1$ 3$\hat { \theta } _ { B } ^ { ( 1 ) } \approx \frac { 1 1 . 7 } { 1 1 . 7 + 8 . 4 } \approx 0 . 5 8$ $\hat { \theta } _ { A } ^ { ( 1 0 ) } \approx 0 . 8 0$M-step 4 $\hat { \theta } _ { B } ^ { ( 1 0 ) } \approx 0 . 5 2$


![](https://web-api.textin.com/ocr_image/external/1c50ede729d1ff4e.jpg)

Do C B, Batzoglou S. What is the expectation maximization algorithm?. Nature 西安交通大學


![](https://web-api.textin.com/ocr_image/external/f4b293205ae3c10c.jpg)

Biotechnology, 2008. XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/dd949d2fef19c025.jpg)

# EM算法-抛硬币例子（数学公式）

$p ( X _ { 1 } , X _ { 2 } , \cdots , X _ { 5 } ,$ $Z _ { 1 } , Z _ { 2 } , \cdots ,$ $, z _ { 5 } \vert \theta )$ $z _ { i } = [ \begin{matrix} z _ { i 1 } \\ z _ { i 2 } \end{matrix} ] \in \{ [ \begin{matrix} 1 \\ 0 \end{matrix} ] , [ \begin{matrix} 0 \\ 1 \end{matrix} ] \}$

$= p ( \{ x _ { 1 } ^ { 1 } , \cdots ,$ $x _ { 1 } ^ { 1 0 } \} , \cdots ,$ $\{ x _ { 5 } ^ { 1 } , \cdots ,$ $x _ { 5 } ^ { 1 0 } \} ,$ $Z _ { 1 } , Z _ { 2 } , \cdots , Z _ { 5 } \vert \theta )$

$= p ( \{ x _ { 1 } ^ { 1 } , \cdots ,$ $x _ { 1 } ^ { 1 0 } \} , \cdots ,$ $\{ x _ { 5 } ^ { 1 } , \cdots ,$ $, x _ { 5 } ^ { 1 0 } \} \vert z _ { 1 } , z _ { 2 } , \cdots ,$ $z _ { 5 } , \theta ) p ( z _ { 1 } , z _ { 2 } , \cdots , z _ { 5 } )$

S $= \prod _ { i = 1 } ^ { s } p ( \{ x _ { i } ^ { 1 } , \cdots , x _ { i } ^ { 1 0 } \} \vert z _ { i } , \theta ) \prod _ { i = 1 } ^ { s } p ( z _ { i } )$

$p ( Z _ { i } ) = \prod _ { k = 1 } ^ { 2 } \pi _ { k } z _ { i k } ,$T is the probability of selecting coin k E {1,2}

$p ( \{ x _ { i } ^ { 1 } , \cdots , x _ { i } ^ { 1 0 } \} \vert z _ { i } , \theta ) = \prod _ { j = 1 } ^ { 1 0 } p ( x _ { i } ^ { j } \vert z _ { i } , \theta )$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/e9de5253839eb686.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/87d406acf4651d0c.jpg)

# EM算法-抛硬币例子（数学公式）

$x _ { i } ^ { j } = 1$ If j toss of i run is head

$x _ { i } j = 0$If j toss of i run is head

tail

$p ( x _ { i } j \vert z _ { i } , \theta ) = \prod _ { k = 1 } ^ { 2 } [ \theta _ { k } x _ { i } ^ { j } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } j } ] ^ { z _ { i k } }$

then

$p ( X _ { 1 } , X _ { 2 } , \cdots ,$ $X _ { 5 } , Z _ { 1 } , Z _ { 2 } , \cdots , Z _ { 5 } \vert \theta )$

5 10 2 5=∏$\prod _ { = 1 } ^ { \infty } [ \int _ { k = 1 } ^ { c } [ \theta _ { k } x _ { i } ^ { j } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } ^ { j } } ] ^ { z _ { i k } }$ $2 \\ 7 \sdiv 1$ $T _ { k } z _ { i k }$j=1 k=1 $\overline { i } = 1 \overline { k } = 1$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ce458a636e1e2cce.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/dd2c81c3dd82189a.jpg)

# EM算法-抛硬币例子（数学公式）

$\ln p ( X _ { 1 } , X _ { 2 } , \cdots ,$ $X _ { 5 } ,$ $Z _ { 1 } , Z _ { 2 } , \cdots , Z _ { 5 } \vert \theta )$

$= \sum _ { i = 1 } ^ { 5 } \sum _ { j = 1 } ^ { 1 0 } \sum _ { k = 1 } ^ { 2 } z _ { i k } \ln \theta _ { k } x _ { i } ^ { j } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } j } + \sum _ { i = 1 } ^ { 5 } \sum _ { k = 1 } ^ { 2 }$

$p ( z _ { i } \vert \{ x _ { i } ^ { 1 } , \cdots , x _ { i } ^ { 1 0 } \} ) = \frac { p ( \{ x _ { i } ^ { 1 } , \cdots , x _ { i } ^ { 1 0 } j \vert z _ { i } \theta ) p ( z _ { i } ) } { p ( x _ { i } ^ { 1 } , \cdots , x _ { i }$

$= \frac { \prod _ { j = 1 } ^ { 1 0 } [ \begin{matrix} 1 ^ { 2 } k = 1 [ \theta _ { k } ^ { 2 } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } ^ { j } ] ^ { 2 i k } \pi _ { k } ^ { 2 } } { \sum _ { z _ { i } } \prod _ { j = 1 } ^ { 1 0 } [ \begin{matrix} 0$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/dd70146b49388c2f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/caa9d977d4b312b3.jpg)

# EM算法-抛硬币例子（数学公式）

$E _ { p } ( Z _ { i k } ] = \sum _ { Z _ { i k } } Z _ { i k } \frac { \prod _ { j = 1 } ^ { 1 0 } [ I _ { k = 1 } ^ { 2 } } { \sum _ { i } , \prod _ { j = 1 } ^ { 1 0 } [ \begin{matrix} 1 - \theta _ { k } ^ { 1 } ( 1 - \theta _ { k } ) ^$

$= \frac { \sum _ { z _ { i } } z _ { i k } \prod _ { j = 1 } ^ { 1 0 } } { \sum _ { i } , \prod _ { j = 1 } ^ { 2 } [ \theta _ { k } ^ { x _ { i } ^ { j } } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } ^ { j } ] ^ { 2 i k } \pi _ {$

$= \frac { \pi _ { k } [ 1 ^ { 1 0 } j ^ { 1 0 } - 1 \theta _ { k } ^ { 1 } ( 1 - \theta _ { k } ) ^ { 1 - x _ { k } j ^ { j } } { \pi _ { 1 } \prod _ { j } ^ { 1 0 } ( 1 - \theta _ { 1 } ) ^ { 1 - x _ { i } j } + \pi _ { 2 } [$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/dd70146b49388c2f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/904a56ef1c625c28.jpg)

# EM算法-抛硬币例子（数学公式）

Expectation(E)Step(fixθ):

$\pi _ { 1 } = \frac { 1 } { 2 } \pi _ { 2 } = \frac { 1 } { 2 }$

$E _ { p } ( z _ { k } ) [ z _ { i k } ] = \frac { \prod _ { j = 1 } ^ { 1 0 } \theta _ { k } ^ { x _ { i } ^ { j } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } ^ { j } } } { \prod _ { j = 1 } ^ { 1 0 } \theta _ { 1 } ^ { x _ { j$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/77914470d3d6631e.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/65d9d7d3b58cb393.jpg)

# EM算法-抛硬币例子（数学公式）

MaximizationStep$( f i \times E _ { p ( Z \vert X ) } [ Z _ { i k } ] ) .$

$\max L ( \theta ) = E _ { p } ( Z \vert X ) [ \ln p ( X _ { 1 } , X _ { 2 } , \cdots ,$ $X _ { 5 } ,$ $Z _ { 1 } , Z _ { 2 } , \cdots , Z _ { 5 } \vert \theta ) ]$

<!-- 5 10 = \sum _ { i = 1 } ^ { \infty } \sum _ { k = 1 } ^ { \infty }  -->
![](https://web-api.textin.com/ocr_image/external/01d2d9e4b081728d.jpg)

$= { { \sim } E _ { p } ( Z \vert X ) [ Z _ { i k } ] \ln \theta _ { k } ^ { x _ { i } } ($ $^ { j } ( 1 - \theta _ { k } ) ^ { 1 - x _ { i } } ^ { j }$ $+ \sum _ { i = 1 } ^ { 5 } \sum _ { k = 1 } ^ { 2 } E _ { p } ( z \vert X ) [ Z _ { i k } ] \ln \pi _ { k }$

<!-- = \sum _ { i = 1 } ^ { 5 } \sum _ { i = 1 } ^ { 1 0 } \sum _ { k = 1 } ^ { 2 }  -->
![](https://web-api.textin.com/ocr_image/external/d347865585f37fa5.jpg)

$E _ { p } ( z \vert x ) [ Z _ { i k } ] ( x _ { i } j \in \theta _ { k } + ( 1 - x _ { i } j ) \ln ( 1 - \theta _ { k } ) ) + c o n s t$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/973a91948cdd2685.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/93f28ab4dd0b8f6e.jpg)

# EM算法-抛硬币例子（数学公式）

$\frac { d L ( \theta ) } { d \theta _ { 1 } } = \sum _ { i = 1 } ^ { 5 } \sum _ { j = 1 } ^ { 1 0 } E _ { p } ( z \vert z ) [ z _ { i 1 } ] ( x _ { i } j \frac { 1 } { \theta _ { 1 } } - ( 1 - x _ { i } j ) \frac { 1 } { 1 - \theta _ { 1 }$


![](https://web-api.textin.com/ocr_image/external/cfb3aaa09df8f7ec.jpg)

$\sum _ { i = 1 } ^ { 5 } \sum _ { j = 1 } ^ { 1 0 } E _ { p ( z \vert x ) } [ z _ { i 1 } ] ( x _ { i } j ( 1 - \theta _ { 1 } ) - ( 1 - x _ { i } j ) \theta _ { 1 } ) = 0$

$\theta _ { 1 } = \frac { \sum _ { i = 1 } ^ { 5 } \sum _ { j = 1 } ^ { 1 0 } E _ { p } ( z \vert x ) [ Z _ { i 1 } ] x _ { i } ^ { j } } { \sum _ { i = 1 } ^ { 5 } 1 0 E _ { p } ( z \vert x ) [ z _ { i 1 } ] }$ $\theta _ { 2 } = \frac { \sum _ { i = 1 } ^ { 5 } \sum _ { j = 1 } ^ { 1 0 } E _ { p } ( z \vert x ) [ Z _ { i 2 } ] x _ { i } ^ { j } } { \sum _ { i = 1 } ^ { 5 } 1 0 E _ { p } ( z \vert x ) [ z _ { i 2 } ] }$

Stefanos Zafeiriou's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/fea99546b709a5d9.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/3107bd96ef75872f.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/53f169153f9575d6.jpg)

## 从掷硬币中估计偏差

西安交通大學


![](https://web-api.textin.com/ocr_image/external/d7b05ba0b2fbc741.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/f59cb8caf63a2565.jpg)

## EM算法

·在阴天，温度一般比晴天低。晴天和阴天是隐变量，温度符合$N ( \mu , \sigma ^ { 2 } )$，阴天和晴天的温度可以实验出10度左右变化，即σ=10。

·给定10个随机日子的温度：Days＝｛70,62,89,54,97,75，82,56,32,78}

·晴天和阴天的平均温度μs、μc是多少？

Francisco lacobelli' PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f969cd867e814085.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a59586cb0b495d83.jpg)

## EM算法

Days = {70,62,89,54,97,75,82,56,32,78}

·给晴天和阴天分配随机的均值，分别是80和55。

## E Step

Sunny (80) {0.65 0.20, 0.99, 0.03, 0.99, 0.86,0.97,0.053,0.00,0.93}Cloudy (55) {0.34, 0.79,0.00, 0.96, 0.00,0.13,0.02,0.94,0.99,0.067}

$E [ Z _ { i j } ] = \frac { p ( \times = \times _ { i } \vert \mu = \mu _ { j } ) } { \sum _ { n = 1 } ^ { 2 } p ( x = x _ { i } \vert \mu = \mu _ { n } ) }$ $e ^ { - \frac { ( 7 0 - 8 0 ) ^ { 2 } } { 2 0 0 } } + e ^ { - \frac { ( 7 0 - 5 5 ) ^ { 2 } } { 2 0 0 } }$ $e ^ { - \frac { ( 7 0 - 8 0 ) ^ { 2 } } { 2 0 0 } }$

$= \frac { e ^ { - \frac { 1 } { 2 \sigma ^ { 2 } } ( \chi _ { i } - \mu _ { j } ) ^ { 2 } } } { \sum _ { n = 1 } ^ { 2 } e ^ { - \frac { 1 } { 2 \sigma ^ { 2 } } ( \chi _ { i } - \mu _ { n } ) ^ { 2 } } }$

## Francisco lacobelli' PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/1549f9b1ff74cc98.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a59586cb0b495d83.jpg)

## EM算法

Days= {70,62,89,54,97,75,82,56,32,78}

·给晴天和阴天分配随机的均值，分别是80和55。

## E Step

Sunny (80) {0.65,0.20,0.99,0.03,0.99,0.86,0.97,0.053,0.00,0.93}Cloudy (55) {0.34,0.79, 0.00, 0.96, 0.00, 0.13, 0.02, 0.94,0.99,0.067}

M Step 0.65*70

Sunny: 45.59+12.51+88.58+1.78+96.93+65.02+79.87+2.99+0.00+72.73=81.7

Cloudy: $\frac { 2 4 . 4 + 4 9 . 4 8 + 0 . 4 1 + 5 2 . 2 1 + 0 . 0 6 + 9 . 9 7 + 2 . 1 2 + 5 3 . 0 0 + 3 1 . 9 9 + 5 . 2 6 } { 4 . 2 9 } = 5 3 . 3 5$*

j $\frac { \sum _ { i = 1 } ^ { m } E [ Z _ { i j } ] \times _ { i } } { \sum _ { i = 1 } ^ { m } E [ Z _ { i j } ] }$/

## Francisco lacobelli' PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/32299e3a1ed6b1d1.jpg)

XIAN JAOTONG UNIVERSITY

### 3.3.3 pLSA模型


![](https://web-api.textin.com/ocr_image/external/111929b93b6e1134.jpg)

## EM算法

Days = {70, 62, 89, 54, 97, 75, 82, 56, 32,78}

·给晴天和阴天分配随机的均值，分别是80和55。

Iteration 2

$\mu _ { S } : 8 1 . 7 9$ $\mu _ { C } : 5 3 . 0 0$

Iteration 3

$\mu _ { S } : 8 1 . 7 7$ $\mu _ { C } : 5 2 . 9 1$

Francisco lacobelli' PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/97f5781863cb60eb.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/3a77b97369f1110d.jpg)

# 求解P(z|d)与P(w|z)

P(z|d)与P(w|z)的参数个数分别是NK与KD

·使得似然函数最大化的$P ( z _ { k } \vert d _ { i } )$与$P ( w _ { j } \vert z _ { k } )$是合理的

$\prod _ { i = 1 } ^ { N } \prod _ { j = 1 } ^ { M } P ( w _ { j } \vert d _ { i } ) ^ { n ( d _ { i } , w _ { j } ) }$ $n ( d _ { i } , w _ { j } )$表示$w _ { j }$出现在$d _ { i }$中的次数 文档 主题 单词

$\log ( \prod _ { i = 1 } ^ { N } \prod _ { j = 1 } ^ { M } P ( w _ { j } \vert d _ { i } ) ^ { n ( d _ { i } , w _ { j } ) )$

<!-- d _ { 1 } P(z|d) P(w|z) z _ { 1 } w _ { 1 } d _ { 2 } w _ { 2 } z _ { K } d _ { N } w _ { M }  -->
![](https://web-api.textin.com/ocr_image/external/f7f9e52d7b5706f4.jpg)

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) \log P ( w _ { j } \vert d _ { i } )$

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) \log \sum _ { k = 1 } ^ { K } P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } )$


![](https://web-api.textin.com/ocr_image/external/727f1f6d7b11453b.jpg)


![](https://web-api.textin.com/ocr_image/external/18a5150263bc5ea2.jpg)

# 求解P(z|d)）与P(w|z)

$\sum _ { i = 1 } ^ { N } \sum _ { i = 1 } ^ { N } n ( i , v i , v j ) \cos \sum _ { k = 1 } ^ { F } P ( \omega j , \vert z ) P ( z _ { k } \vert \cdot j k ) ] = \sum _ { i = 1 } ^ { N } \sum _ { k = 1 } ^ { N } n ( d , v _ { j } ) \log \sum$

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) \sum _ { k = 1 } ^ { K } [ Q ( i j ) ( z _ { k } ) \cdot \log \frac { P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } ) } { Q ^ { ($

$Q ^ { ( i j ) } ( z _ { k } ) \propto P ( w _ { j } \vert z _ { k } )$ $\sum _ { k = 1 } ^ { K } Q ^ { ( i j ) } ( z _ { k } ) = 1$ $) P ( z _ { k } \vert d _ { i } ) \Rightarrow Q ^ { ( i j ) } ( z _ { k } ) = \frac { P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } ) } { \sum _ { k = 1 } ^ { K } P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _$

Q确定后，求导公式，红线部分是常数项

$\sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) \sum _ { k = 1 } ^ { K } [ Q ^ { ( i j ) ( z _ { k } ) } \cdot \log \frac { P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } ) } { Q _$

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , v _ { j } ) \sum _ { k = 1 } ^ { K } \{ Q ( y ) ( z ) \cdot [ \log P ( u _ { j } \vert z _ { k } ) P ( z _ { k } \vert d z ) - \log Q ( i y ) ( z _ { k } ) ]$

$= \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } n ( d _ { i } , v _ { j } ) \sum _ { k = 1 } ^ { K } [ Q ( i j ) ( z _ { k } ) \cdot \log P ( u _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } ) - Q ( i \partial ) \cdot \log$

http://www.mamicode.com/info-detail-943284.html

西安交通大學


![](https://web-api.textin.com/ocr_image/external/43138b5f23df1a32.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/18a5150263bc5ea2.jpg)

# 求解P(z|d)与P(w|z)

·需要保证P（w／z）和P（z｜d）的概率归一性，也就是需要加上两个拉格朗日乘数

$\sum _ { i = 1 } ^ { N _ { 1 } } \sum _ { i = 1 } ^ { N _ { 1 } } n d _ { i , w _ { j } } \frac { F } { \sum _ { k = 1 } ^ { N } ( 0 , j k ) , P ( z _ { j } \vert z ) P ( z _ { k } \vert _ { j = 1 } ^ { K } P ( 1 - \frac {$

# ·分别对P(z|d)与P(w|z)求导

$\frac { \partial L } { \partial P ( w _ { j } \vert z _ { k } ) } = \sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } )$ $) \cdot \frac { P ( z _ { k } \vert d _ { i } ) } { P ( w _ { j } \vert z _ { k } ) P ( z _ { k } \vert d _ { i } ) } - \alpha$

$= \sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } ) .$ $) \cdot \frac { 1 } { P ( w _ { j } \vert z _ { k } ) } - \alpha$


![](https://web-api.textin.com/ocr_image/external/b31c9f54a176a9a1.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/9e3705a277235cc8.jpg)

求解P(z|d)与P(w|z)

·令 $\frac { \partial L } { \partial P ( w _ { j } \vert z _ { k } ) } = 0$，求解α

$\sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } ) \cdot \frac { 1 } { P ( w _ { j } \vert z _ { k } ) } - \alpha = 0$ (6)

$\sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q _ { i j } ( z _ { k } ) = \alpha P ( w _ { j } \vert z _ { k } )$

$\sum _ { j = 1 } ^ { M } \sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } ) = \alpha \sum _ { j = 1 } ^ { M } P ( w _ { j } \vert z _ { k } )$

$\alpha = \sum _ { j = 1 } ^ { M } \sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } )$

把α代回到上式，就可以解出来P(w|z):

$P ( w _ { j } \vert z _ { k } ) = \frac { \sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } ) } { \sum _ { j = 1 } ^ { M } \sum _ { i = 1 } ^ { N } n ( d _ { i } , w _ { j } ) Q _ { k$ 同理， $P ( z _ { k } \vert d _ { i } ) = \frac { \sum _ { j = 1 } ^ { M } n ( d _ { i } , w _ { j } ) Q ^ { ( i j ) } ( z _ { k } ) } { n ( d _ { i } ) }$


![](https://web-api.textin.com/ocr_image/external/b4be1393875246be.jpg)

http://www.mamicode.com/info-detail-943284.html 文档d的长度

西安交通大學


![](https://web-api.textin.com/ocr_image/external/3a7b3dfd5abc495e.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

知识卡片3.4-拉格朗日（Lagrange）乘数定理．设$x = ( x _ { 1 } , x _ { 2 } , \cdots , x _ { n } )$，求解$g _ { 1 } ( x ) = c _ { 1 }$、$g _ { 2 } ( x ) = c _ { 2 } 、 \cdots g _ { k } ( x ) = c _ { k }$共k个约束条件下f(x)的极值，可转化为求解以下n+k 个方程[0](或n+k个变量）：

$\frac { \partial } { \partial x _ { i } } ( f ( x ) + \sum _ { j = 1 } ^ { k } \lambda _ { j } g _ { j } ( x ) ) = 0 ,$1≤i≤n; (3.38)

$g _ { j } ( x ) = c _ { j } ,$ 1≤j≤k.

上式中新引入的变量$\lambda _ { 1 } , \lambda _ { 2 } , \cdots , \lambda _ { k }$称为Lagrange 乘数；利用该定理求解极值的方法称为Lagrange乘数法。（注：这里的n、k与定理框外的n、k无关）

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY

## 3.3.3 pLSA模型


![](https://web-api.textin.com/ocr_image/external/7aa2f6abce2cf76f.jpg)

## 优点

# ·相对LSA性能有明显提升

##  缺点

·没有关于P（d）的参数，无法给新来的文档赋予一个概率

·P（z｜d）的参数数随着文档数线性增长，导致过拟合问题

西安交通大學


![](https://web-api.textin.com/ocr_image/external/93154cbd9c606e99.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

## 例3.6 利用PLSA对以下五个文档进行分析。

文档集：

$d _ { 1 }$：达芬奇的代表作包括《蒙娜丽莎》。

$d _ { 2 }$：大卫像是文艺复兴时期米开朗基罗的代表作。

$d _ { 3 }$：米开朗基罗、达芬奇是文艺复兴时期的艺术领域代表性人物。

$d _ { 4 }$：电影是科技与艺术相结合的产物。

$d _ { 5 }$：不少科幻电影中的科技已经成为现实。

# 词汇表：

［1：达芬奇 2：代表作 3：文艺复兴 4：米开朗基罗 5：艺术 6：电影

7：科技 8：科幻］

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

达芬奇代表作文 开 艺电 科科艺兴米朗复 基 术影 技幻罗

$d _ { 1 } / 1 & 1 & 0 & 0 \\ d _ { 2 } [ 0 & 1 & 1 \\ d _ { 3 } [ \begin{matrix} 1 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 & 0 \\ d _ { 5 } & 0 & 0 & 0 \end{matrix}$ $0 0 0 0 0 \\ 0 0 0 0 0 \\ 1 0 0 0 0 \\ 1 1 1 0 \\ 0 1 1 1 1$ $d _ { 1 } ( 0 . 9 9 0 . 0 1 \\ d _ { 2 } \\ d _ { 2 } \\ d _ { 3 } \\ d _ { 4 } ^ { 1 } ( 0 . 7 4 & 0 . 2 6 \\ d _ { 5 } & 0 . 0 0 \\ d _ { 5 } & 0 & 1 . 0 0 \end{matrix} )$


![](https://web-api.textin.com/ocr_image/external/d14fa105427b136e.jpg)

p(w|z) 0.24 0.25 0.25 0.25 0.01 0 0 0-达芬奇代表作文 米开 艺 电 科艺兴复 朗基罗 术 影科技0.01 0 0 0 0.28 0.28 0.29 0.14

西安交通大學


![](https://web-api.textin.com/ocr_image/external/4ade1719f793bc2d.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/e36f32160c2342e2.jpg)

Latent Dirichlet Allocation： David Blei、Andrew Ng和Michael I．Jordan在2003年提出的生成主题模型

·与pLSA相同，每个文档都视为各种主题的混合；不同之处在于LDA中P（z｜d）与P（w｜z）不是固定的，受Dirichlet先验控制

·作用：缓解小规模数据集导致的过拟合

·Dirichlet分布是什么，为什么选择这个分布，如何控制？

<!-- 45 40 35 Overfitting Underfitting Overfitting 30 (8)Ou 25 20 Training set Test set 15 10 50 50 100 150 200 250 300 umber of nodes  -->
![](https://web-api.textin.com/ocr_image/external/6129ec87283d6d17.jpg)


![](https://web-api.textin.com/ocr_image/external/58d7a09286db7901.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/e5847f8444c9aba5.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/e36f32160c2342e2.jpg)

Latent Dirichlet Allocation： David Blei、Andrew Ng和Michael I．Jordan在2003年提出的生成主题模型

·与pLSA相同，每个文档都视为各种主题的混合；不同之处在于LDA中P（z｜d）与P（w｜z）不是固定的，受Dirichlet先验控制

·作用：缓解小规模数据集导致的过拟合

·Dirichlet分布是什么，为什么选择这个分布，如何控制？

·为什么能缓解？（稀疏Dirichlet分布）

<table border="1" ><tr>
<td colspan="1" rowspan="1">a b d$\overrightarrow { \alpha } = ( 1 , 1 , 1 )$ $\overrightarrow { a } = ( 1 0 0 0 , 1 0 0 , 5 0 0 )$ $\overrightarrow { \alpha } = ( 0 . 1 , 0 . 1 , 0 . 1 )$ =(0.1,0.01,0.0<img src="https://web-api.textin.com/ocr_image/external/fee77e993034d7e7.jpg"><img src="https://web-api.textin.com/ocr_image/external/e904229e457c37f7.jpg"><img src="https://web-api.textin.com/ocr_image/external/e4ecb26a3b4f82dd.jpg"><img src="https://web-api.textin.com/ocr_image/external/22b441f991728d91.jpg"></td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/38e6d339009fab6f.jpg)

XIAN JIAOTONG UNIVERSITY

## Latent Dirichlet Allocation

·k - Number of topics a document belongs to (a fixed number)

<!-- [0.12 0.5.0.33] [0.08 0.18 0.2] 0.78 [0.9 0.1 0.001] n \beta _ { k } [0010] [01 00] N [0001] α 0 z w N M 0.3 [0.2 0.005 0.3] [0 0 10] [0.03 0.08 0.1] M [0 1 0 0] N [0.1 0.08 0.04] [0 0 1]  -->

·V-Size of the vocabulary

·M - Number of documents

·N- Number of words in each document

θ，z，andβ是分布

西安交通大學

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/1bd5fd8f4fd42f3b.jpg)

## Latent Dirichlet Allocation

·k - Number of topics a document belongs to (a fixed number)

<!-- [0.12 0.5.0.33] [0.08 0.18 0.2] 0.78 [0.9 0.1 0.001] n \beta _ { k } [0010] [01 00] N [0001] α 0 z w N M 0.3 [0.2 0.005 0.3] [0 0 10] [0.03 0.08 0.1] M [0 1 0 0] N [0.1 0.08 0.04] [0 0 1]  -->
![](https://web-api.textin.com/ocr_image/external/ff2de456f9b515ad.jpg)

·V-Size of the vocabulary

·M - Number of documents

·N- Number of words in each document

θ，z，andβ是分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/7937a669380cfd87.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/bd28c4c3b64e1450.jpg)

# Latent Dirichlet Allocation

·w -A word in a document.This is represented as a one hot encoded vector of size V(i.e.V- vocabulary size)

<!-- [0.12 0.5 0.33] [0.08 0.18 0.2] 0.78 [0.9 0.1 0.001] n \beta _ { k } [0010] [010 0] N [00 01] α θ z W N M 0.3 [0.2 0.005 [0.03 0.08 0.3] 0.1] [0 0 0] M [0 1 0 0] N [0.1 0.08 0.04] [00 0 k  -->
![](https://web-api.textin.com/ocr_image/external/242478af1c5b22b2.jpg)

·w (bold w): represents a document (i.e.vector of “w"s) of N words

·D - Corpus, a collection of M documents

θ，z， and β是分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/8fa9bf5d26a86706.jpg)

XIAN JIAOTONG UNIVIRSITY

Latent Dirichlet Allocation

·a是文档集级参数，dirichlet分布的参数，用于生成文档的主题分布θ（多项分布）

<!-- [0.12 0.5..0.33] [0.08 0.180.2] ·..... 0.78 [0.9 0.1 0.001] n \beta _ { k } [0 010] [01..0 0] N [00 01] α 0 z w N M 0.3 [0.2 0.005 0.3] [00 [0.03 0.08 0.1] ⋯10] M [0 1 00] N [0.1 0.08 0.04] [0 0  -->
![](https://web-api.textin.com/ocr_image/external/7a52ae4e15eaea6c.jpg)

·θ用于生成文档中各个词的主题z

·η是文档集级参数，dirichlet分布的参数，用于生成每个主题的词分布β（多项分布）

θ，z，and β是分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/b2202c1962185900.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/9fc9774c40f43f05.jpg)

<!-- [0.12 0.5.0.33] Latent Dirichlet Allocation [0.08 0.18 0.2] 0.78 [0.9 0.1 0.001] n 8 [0010] -- k [010 0] N [00..01] V··· ·z-A topic from a set α 0 z W N of k topics. Atopic is a M distribution words. 0.3 [0.2 0.005 0.3] [00 10] [0.03 0.08 0.1] M [0 1 .0 0] N [0.1 0.08 0.04] [00 1] 实际上没有标签  -->
![](https://web-api.textin.com/ocr_image/external/7b27f748cf931861.jpg)


![](https://web-api.textin.com/ocr_image/external/3b58dd692bac88bc.jpg)

β

<table border="1" ><tr>
<td colspan="1" rowspan="1">topic</td>
<td colspan="1" rowspan="1">proof</td>
<td colspan="1" rowspan="1">induction</td>
<td colspan="1" rowspan="1">object</td>
<td colspan="1" rowspan="1">bouquet</td>
<td colspan="1" rowspan="1">memory</td>
</tr><tr>
<td colspan="1" rowspan="1">maths</td>
<td colspan="1" rowspan="1">0.45</td>
<td colspan="1" rowspan="1">0.35</td>
<td colspan="1" rowspan="1">0.2</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">comp sc.</td>
<td colspan="1" rowspan="1">0.23</td>
<td colspan="1" rowspan="1">0.17</td>
<td colspan="1" rowspan="1">0.35</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.25</td>
</tr><tr>
<td colspan="1" rowspan="1">wine</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.1</td>
<td colspan="1" rowspan="1">0.75</td>
<td colspan="1" rowspan="1">0.15</td>
</tr></table>

https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c°1220159

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f1b5a8f78137a1dc.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/1d1773484f4f1526.jpg)

Latent Dirichlet Allocation

<!-- 3. Pick another ball from the new ground and you have a single word Cats Loyal Dogs Evil 2.Based on the ball you pick,you're sent to another ground "Beta" 1.Pick a ball from Flying Olympics the ground "Theta" Sport Players AI Cats Loyal Organize ground Theta Degs Evil Alpha Organize Animals Tech ground Beta Eta Flying Olympics Players AI Cats Cats Loyal AI →Dogs Evil Dogs Evil Cats Flying Olympics Document Players AI Do this 5 times to get 5 words out for the document  -->
![](https://web-api.textin.com/ocr_image/external/3102b12321988dfa.jpg)

https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c°1220159

西安交通大學


![](https://web-api.textin.com/ocr_image/external/c6912c471ae95642.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/e36f32160c2342e2.jpg)

## Latent Dirichlet Allocation

“topic plate”

for all topics k € [1,K] do

sample mixtre components$\overrightarrow { \varphi } _ { k } \sim D i r ( \overrightarrow { \beta } )$ 主题的词分布

end for

"document plate":

for all documents m E [1,M] do

sample mixture proportion$\overrightarrow { \theta } _ { } \sim D i ( \overrightarrow { \alpha } )$文档的主题分布

<!-- C \overrightarrow { \theta } _ { m } z _ { m , n } β \psi _ { k } 0m,n k∈[1,K 12 E 11 1 , N _ { m } m 1, M  -->
![](https://web-api.textin.com/ocr_image/external/5bab0ef782da1238.jpg)

sample document length Nm\~ Poiss(5)

“word plate”:

for all words n € [1,Nm] in document m do

sample topic index Zm.n\~Mult( m)

sample term for word$w _ { , n } \sim M u l t ( \overrightarrow { \varphi } _ { z , n } $

end for

end for

D. M. Blei, et al., "Latent Dirichlet allocation," Journal of Machine Learning Research, vol. 3, pp. 993-1022, 2003

西安交通大學


![](https://web-api.textin.com/ocr_image/external/2a4c296cb4968b3d.jpg)

XIAN JIAOTONG UNIVIESITY


![](https://web-api.textin.com/ocr_image/external/f62c8b2d370c2dbd.jpg)

## Latent Dirichlet Allocation

·为什么要采样？得到未知或复杂的分布的参数。

## ·为什么能采样？

Dirichlet分布是Multinomial分布的共轭先验，能够明确后验分布的类型，简化推断难题

样本是每个单词，状态空间是样本所属的主题。虽然不知道所有单词在主题上的分布，但是可求出转移概率

Gibbs Sampling或MCMC算法，可以保证对于任意初始分布，迭代计算得到到只与转移概率（矩阵形式）相关的分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0b1b1718decf7c56.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/ed82174a66da8b74.jpg)

## 预备知识

·共轭分布（Conjugate Distribution）、共轭先验（Conjugate Prior)

·Dirichlet分布

·Monte Carlo采样与Gibbs采样

西安交通大學


![](https://web-api.textin.com/ocr_image/external/43138b5f23df1a32.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/e483547b7cdfde64.jpg)

## 共轭分布与共轭先验：

·贝叶斯公式中，如果后验概率p（H｜D）与先验概率p（H）在相同的概率分布族（形式相同，参数不同）中，则先验分布和后验分布称为共轭分布，而先验分布p（H）为似然估计p（D｜H）的共轭先验。

<!-- 30 10  -->
![](https://web-api.textin.com/ocr_image/external/8516c0d34d2bf9b0.jpg)

<!-- 20 20  -->
![](https://web-api.textin.com/ocr_image/external/469289c0eeb3a224.jpg)

<!-- 似然估计 Likelihood 先验 p ( H \vert D ) = \underline { p } Prior \frac } 后验 Posterior Evidence  -->
![](https://web-api.textin.com/ocr_image/external/9ab0925b1b1a19fc.jpg)


![](https://web-api.textin.com/ocr_image/external/051c9c8c5a936634.jpg)


![](https://web-api.textin.com/ocr_image/external/d94b6a048a532812.jpg)

#1 #2

如果有相同分布形式

<!-- 共轭分布 先验＋似然估计→后验 共轭先验  -->
![](https://web-api.textin.com/ocr_image/external/e263a821a92adf16.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/c749e49cc89b9339.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d7d304473d1d1804.jpg)

## 共轭先验的好处

·简化了贝叶斯分析，只用修改后验概率分布的参数

·可得到解析表达式，而不需要数值计算

·解析表达式：由有限次常见运算函数的组合给出的形式。

·通常只有初等函数被看作常见函数（具有闭包性质）；广义上，把特殊函数，如gamma函数也看作常见函数。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/39711b9a940f0782.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/ed82174a66da8b74.jpg)

## 共轭分布举例

·抛不均匀的硬币。假设使用Bernoulli分布对抛硬币进行建模。参数θ表示出现正面的可能性。假设值1对应于正面，值0对应于反面。抛硬币结果的分布为：

$P ( x \vert \theta ) = \theta ^ { T } ( 1 - \theta ) ^ { 1 - x }$

·贝叶斯学派看来，θ不是固定值，而是概率分布，假设是一个Beta分布

$f ( x ; \alpha , \beta ) = \frac { x ^ { \alpha - 1 } ( 1 - x ) ^ { \beta - 1 } } { \int _ { 0 } ^ { 1 } u ^ { \alpha - 1 } ( 1 - u ) ^ { \beta - 1 } d u }$ Gamma函数

$\Gamma ( x ) = \int _ { 0 } ^ { \infty } t ^ { x - 1 } e ^ { - t } d t$

定值 $= \frac { \Gamma ( \alpha + \beta ) } { \Gamma ( \alpha ) \Gamma ( \beta ) } x ^ { \alpha - 1 } ( 1 - x ) ^ { \beta - 1 }$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/43138b5f23df1a32.jpg)

XIAN JIAOTONG UNIVIRSITY

## Gamma函数

The Gamma function T(x) is the generalization of the factorial x/ (or rather(x-1)!)to the reals:

$\Gamma ( \alpha ) = \int _ { 0 } ^ { \infty } x ^ { \alpha - 1 } e ^ { - x } d x$ forα&gt;0

<!-- 25 20 15 인 10 ? 5 출  -->
![](https://web-api.textin.com/ocr_image/external/a7883996826af331.jpg)

Forx&gt;1,Γ(x)=(x-1)Γ(x-1)

For positive integers,Γ(x)=(x-1)!

Julia Hockenmaier's ppt

·欧拉提出Gamma函数，把阶乘从整数域扩展到实数域，就像$f ( x ) = x ^ { 2 }$

$\Gamma ( \frac { 1 } { 2 } ) = \sqrt { \pi }$ Γ(1)=1 Γ(2)=?

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0b1b1718decf7c56.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/9dff9a659760c2dc.jpg)

## Beta分布


![](https://web-api.textin.com/ocr_image/external/7b078c80bbf0d5ee.jpg)

A random variable X (0&lt;x&lt; 1) has a Beta distribution with (hyper)parameters a(a&gt;0and β (β&gt;0) if X has a continuous distribution with probability density function

$P ( x \vert \alpha , \beta ) = \frac { \Gamma ( \alpha + \beta ) } { \Gamma ( \alpha ) \Gamma ( \beta ) } x ^ { \alpha - 1 } ( 1 - x ) ^ { \beta - 1 }$

Julia Hockenmaier's ppt

a=β=0.5

a=5,β=1

a=1,β=3

<!-- 2.0 2.4 2.2 2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 0.1 0.2 0.3 0.4 0.4 0.6 0.7 0.8 0.9 1  -->
![](https://web-api.textin.com/ocr_image/external/dc925b9446e19ebf.jpg)

⇔a=2,β=2

⇔a=2,β=5

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0b1b1718decf7c56.jpg)

XIANJIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/ed82174a66da8b74.jpg)

## 共轭分布举例

P(θ|x)∝P(x|θ)P(θ)

$\alpha ( \theta ^ { x } ( 1 - \theta ) ^ { 1 - x } ) ( \theta ^ { \alpha - 1 } ( 1 - \theta ) ^ { \beta - 1 } )$

$= \theta ^ { x + \alpha - 1 } ( 1 - \theta ) ^ { ( 1 - x ) + \beta - 1 }$

·后验概率P(θ|x) 符合Beta(a+x,1-x+β)，与先验概率属于同类分布。

·Beta分布是二项分布的共轭先验。

西安交通大學


![](https://web-api.textin.com/ocr_image/external/43138b5f23df1a32.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/9dff9a659760c2dc.jpg)

## 共轭分布

<table border="1" ><tr>
<td colspan="1" rowspan="1">Likelihood</td>
<td colspan="1" rowspan="1">Prior</td>
<td colspan="1" rowspan="1">Posterior</td>
</tr><tr>
<td colspan="1" rowspan="1">Binomial</td>
<td colspan="1" rowspan="1">Beta</td>
<td colspan="1" rowspan="1">Beta</td>
</tr><tr>
<td colspan="1" rowspan="1">Negative Binomial</td>
<td colspan="1" rowspan="1">Beta</td>
<td colspan="1" rowspan="1">Beta</td>
</tr><tr>
<td colspan="1" rowspan="1">Poisson</td>
<td colspan="1" rowspan="1">Gamma</td>
<td colspan="1" rowspan="1">Gamma</td>
</tr><tr>
<td colspan="1" rowspan="1">Geometric</td>
<td colspan="1" rowspan="1">Beta</td>
<td colspan="1" rowspan="1">Beta</td>
</tr><tr>
<td colspan="1" rowspan="1">Exponential</td>
<td colspan="1" rowspan="1">Gamma</td>
<td colspan="1" rowspan="1">Gamma</td>
</tr><tr>
<td colspan="1" rowspan="1">Normal (mean unknown)</td>
<td colspan="1" rowspan="1">Normal</td>
<td colspan="1" rowspan="1">Normal</td>
</tr><tr>
<td colspan="1" rowspan="1">Normal (variance unknown)</td>
<td colspan="1" rowspan="1">Inverse Gamma</td>
<td colspan="1" rowspan="1">Inverse Gamma</td>
</tr><tr>
<td colspan="1" rowspan="1">Normal (mean and variance unknown)</td>
<td colspan="1" rowspan="1">Normal/Gamma</td>
<td colspan="1" rowspan="1">Normal/Gamma</td>
</tr><tr>
<td colspan="1" rowspan="1">Multinomial</td>
<td colspan="1" rowspan="1">Dirichlet</td>
<td colspan="1" rowspan="1">Dirichlet</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/31f8d560ebb09c6b.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/55b675d7dc2dc829.jpg)

## 分布的分布

## ·一个分布的定义域正好是另一个分布的参数，对另一个分布具有控制作用

## ·Beta分布是Bernoulli分布的分布。

二项分布呢？ $P r ( k ; n , p ) = P r ( X = k ) = ( k ) p ^ { k } ( 1 - p ) ^ { n - k }$

P(θ|x)∝P(x|θ)P(θ)

$\alpha ( \theta ^ { x } ( 1 - \theta ) ^ { 1 - x } ) ( \theta ^ { \alpha - 1 } ( 1 - \theta ) ^ { \beta - 1 } )$

$= \theta ^ { x + \alpha - 1 } ( 1 - \theta ) ^ { ( 1 - x ) + \beta - 1 }$

## ·Dirichlet分布是Multinomial分布的分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/1f991dd8be585206.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/c593a6519c528ffb.jpg)

# 狄利克雷分布（Dirichlet Distribution）

$\overrightarrow { X } = ( x _ { 1 } , \cdots , x _ { k } ) ,$ $, ( 0 < x _ { i } < 1 , \sum _ { i = }$ $1 , \sum _ { i = 1 } ^ { k } x _ { i } = 1 )$ ，的k维Dirchlet分布，参数

$\overrightarrow { \alpha } = ( \alpha _ { 1 } , \cdots , \alpha _ { k } ) ,$ $, ( \alpha _ { i } > 0 ,$,i=1,2,⋯,k) ，它的概率密度函数如下：

$D i r ( \overrightarrow { \alpha } ) \rightarrow P ( \overrightarrow { X } \vert \overrightarrow { \alpha } ) = \frac { \Gamma ( \sum _ { i = 1 } ^ { k } \alpha _ { i } ) } { \prod _ { i = 1 } ^ { k } \Gamma ( \alpha _ { i } ) } \frac { k } { j } x _ { j } ^ { \alpha _ { j } - 1 }$

其中$\overrightarrow { X } , \overrightarrow { \alpha }$维数是一样的。

与Beta分布的关系？ $P ( x \vert \alpha , \beta ) = \frac { \Gamma ( \alpha + \beta ) } { \Gamma ( \alpha ) \Gamma ( \beta ) } x ^ { \alpha - 1 } ( 1 - x ) ^ { \beta - 1 }$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/83f9b2b2fc870bb8.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/aa654ad209232463.jpg)

# 狄利克雷分布（Dirichlet Distribution）

# ·Dirichlet-Multinomial 共轭

$D i r ( \overrightarrow { p } \vert \overrightarrow { \alpha } ) = \frac { \Gamma ( \sum _ { k = 1 } ^ { K } \alpha _ { k } ) } { \prod _ { k = 1 } ^ { K } \Gamma ( \alpha _ { k } ) } \frac { K } { \prod _ { k = 1 } ^ { \alpha _ { k } - 1 }$

$\overrightarrow { a } , \overrightarrow { n }$是K维向量

$M u l t ( \overrightarrow { n } \vert \overrightarrow { p } , N ) = ( \frac { N } { n } ) \prod _ { k = 1 } ^ { K } p _ { k } ^ { n _ { k } }$


![](https://web-api.textin.com/ocr_image/external/63e54ad4522a3188.jpg)

$D i r ( p \vert \overrightarrow { a } + \overrightarrow { n } )$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/213defab7b4ca806.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/e6b103c160803db2.jpg)

# 狄利克雷分布（Dirichlet Distribution）

<!-- Alpha=[1,1,1] 1.0 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 Y 0.6 0.4 0.2 .0 0.8 10 1.0 0.8 0.6 x  -->
![](https://web-api.textin.com/ocr_image/external/3f62694dccb28d63.jpg)

<!-- Alpha=[5.0.1.0.1.0] 1.0 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.4 0.2 0.0 0.8 1.0 0.6 10 0.8  -->
![](https://web-api.textin.com/ocr_image/external/a4226731347250a7.jpg)

<!-- Alpha=[0.2,0.2,0.2] 10 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 Y 0.6 0.2 0.0 0.8 0.6 0.4 10 1.0 0.8 X  -->
![](https://web-api.textin.com/ocr_image/external/701b93b20aa09c76.jpg)

<!-- Alpha=[1.0,1.0,0.2] 10 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.4 0.2 0.0 0.8 10 0.6 1.0 0.8  -->
![](https://web-api.textin.com/ocr_image/external/504b79e5c81802b4.jpg)

<!-- Alpha=[5.0.5.0.5.0] 1.0 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 Y 0.6 0.2 0.0 0.8 0.6 0.4 10 1.0 0.8 X  -->
![](https://web-api.textin.com/ocr_image/external/e00ce0e9d291b94d.jpg)

<!-- Alpha=[1.0.5.0.0.2] 1.0 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 0.6 0.4 02 0.0 1.0 10 0.8  -->
![](https://web-api.textin.com/ocr_image/external/054bf8f18b61f053.jpg)

Large a values push the distribution to the middle of the triangle, where smaller a values push the distribution to the corners.

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0d42d950f8fe9caa.jpg)

XIAN JIAOTONG UNIVIESITY


![](https://web-api.textin.com/ocr_image/external/4ed0f28c518be209.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/1c118f428277fd6a.jpg)

## Probability distributions

and their stories

西安交通大學


![](https://web-api.textin.com/ocr_image/external/b61d50cf4ad4e46a.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/257902444ac2347c.jpg)

# 狄利克雷分布（Dirichlet Distribution）

<!-- Alpha=[1,1,1] 10 0.8 Z 0.6 0.4 0.2 0.0 0.0 0.2 0.4 ¥ 0.6 0.2 0. 0.8 0.6 0.4 10 10 0.8 X  -->
![](https://web-api.textin.com/ocr_image/external/c7c1d22be779a78f.jpg)

LDA -&gt; PLSA

西安交通大學


![](https://web-api.textin.com/ocr_image/external/6cfac393c5ee36db.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/1e62d97d771230c2.jpg)

## 1.Sample a Topic

<!-- Sports politics θ science a Science Politics Sports politics 0.1 0.8 0.1 sports Science politics Politics politics  -->
![](https://web-api.textin.com/ocr_image/external/7c6d376dc026993b.jpg)

https://eng.ftech.ai/?p=578

西安交通大學


![](https://web-api.textin.com/ocr_image/external/284bdf93e700e81f.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/1e62d97d771230c2.jpg)

## 1.Sample a Topic

<!-- LDA: Sample a Topic θ Doc 1 science politics sports 0.1 0.8 0.1 Sports Doc 2 science politics sports 0.7 0.1 0.2 Q Doc 3 science politics sports 0.4 0.5 0.1 Doc 4 Science Politics Doc 5  -->
![](https://web-api.textin.com/ocr_image/external/8928d3d72c9eef47.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">science</td>
<td colspan="1" rowspan="1">politics</td>
<td colspan="1" rowspan="1">sports</td>
</tr><tr>
<td colspan="1" rowspan="1">0.05</td>
<td colspan="1" rowspan="1">0.05</td>
<td colspan="1" rowspan="1">0.9</td>
</tr></table>

<table border="1" ><tr>
<td colspan="1" rowspan="1">science</td>
<td colspan="1" rowspan="1"> politics</td>
<td colspan="1" rowspan="1">sports</td>
</tr><tr>
<td colspan="1" rowspan="1">0.05</td>
<td colspan="1" rowspan="1">0.45</td>
<td colspan="1" rowspan="1">0.5</td>
</tr></table>

https://eng.ftech.ai/?p=578

西安交通大學

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/ec77ccfd125f3c5b.jpg)

## 2.Sample a Word

<!-- LDA: Sample a Word space climate climate B rule vote space climate vote rule 0.4 0.4 0.1 0.1 space vote space climate climate rule space  -->
![](https://web-api.textin.com/ocr_image/external/5fb33b46ef7c978d.jpg)

https://eng.ftech.ai/?p=578


![](https://web-api.textin.com/ocr_image/external/a18c71c9f57c991a.jpg)


![](https://web-api.textin.com/ocr_image/external/8440398736f5efa2.jpg)

## 2. Sample a Word

<table border="1" ><tr>
<td colspan="1" rowspan="1">space</td>
<td colspan="1" rowspan="1">climate</td>
<td colspan="1" rowspan="1">vote</td>
<td colspan="1" rowspan="1">rule</td>
</tr><tr>
<td colspan="1" rowspan="1">0.4</td>
<td colspan="1" rowspan="1">0.4</td>
<td colspan="1" rowspan="1">0.1</td>
<td colspan="1" rowspan="1">0.1</td>
</tr></table>

<!-- rule Topic 1 β vote Topic 2 space climate Topic 3  -->
![](https://web-api.textin.com/ocr_image/external/ee69f53191ba7ba2.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">space</td>
<td colspan="1" rowspan="1">climate</td>
<td colspan="1" rowspan="1">vote</td>
<td colspan="1" rowspan="1">rule</td>
</tr><tr>
<td colspan="1" rowspan="1">0.1</td>
<td colspan="1" rowspan="1">0.2</td>
<td colspan="1" rowspan="1">0.5</td>
<td colspan="1" rowspan="1">0.2</td>
</tr></table>

<table border="1" ><tr>
<td colspan="1" rowspan="1">space</td>
<td colspan="1" rowspan="1">climate</td>
<td colspan="1" rowspan="1">vote</td>
<td colspan="1" rowspan="1">rule</td>
</tr><tr>
<td colspan="1" rowspan="1">0.1</td>
<td colspan="1" rowspan="1">0.05</td>
<td colspan="1" rowspan="1">0.05</td>
<td colspan="1" rowspan="1">0.8</td>
</tr></table>

https://eng.ftech.ai/?p=578

西安交通大學


![](https://web-api.textin.com/ocr_image/external/85538a962d0a10d6.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/fdff9e8bcc15f024.jpg)

## 3. Combining the Models

<!-- rule Topic Model β vote space climate Sports θ a science politics sports 0.7 0.2 0.1 space climate vote rule space climate vote rule space climate vote rule 0.4 0.4 0.1 0.1 0.1 0.2 0.5 0.2 0.1 0.05 0.05 0.8 Science Politics topics words science politics science science science politics science sports science sports Real Articles space vote climate space vote rule climate space rule climate Fake Doc 1 Space space climate rule space politics sports sports science politics sports science science sports sports compare Ruled play rule rule vote space climate rule rule space climate rule rules space Fake Doc 2 vote rules climate vote climate  -->
![](https://web-api.textin.com/ocr_image/external/effe93b276c31282.jpg)

https://eng.ftech.ai/?p=578

西安交通大學


![](https://web-api.textin.com/ocr_image/external/43138b5f23df1a32.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/557bf61494562abd.jpg)

·一类通过重复随机采样进行数值计算的方法。

·数学家Ulam提出20世纪40年代提出，和曼哈顿计划相关，由，以著名赌场Monte Carlo命名

·可追溯到18世纪，人工大量采样难

$P = \int _ { \theta = 0 } ^ { \frac { \pi } { 2 } } \int _ { z = 0 } ^ { ( l / 2 ) \sin \theta } \frac { 4 } { t \pi } d x d \theta = \frac { 2 l } { t \pi }$


![](https://web-api.textin.com/ocr_image/external/0f52c4bc33a3f3e6.jpg)

<!-- 2 1.5 1 0.5 0 -1 -1.5  -->
![](https://web-api.textin.com/ocr_image/external/e014adc24091e3d8.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">试验者</td>
<td colspan="1" rowspan="1">时间</td>
<td colspan="1" rowspan="1">投掷次数</td>
<td colspan="1" rowspan="1">相交次数</td>
<td colspan="1" rowspan="1">圆周率估计值</td>
</tr><tr>
<td colspan="1" rowspan="1">Wolf</td>
<td colspan="1" rowspan="1">1850年</td>
<td colspan="1" rowspan="1">5000</td>
<td colspan="1" rowspan="1">2532</td>
<td colspan="1" rowspan="1">3.1596</td>
</tr><tr>
<td colspan="1" rowspan="1">Smith</td>
<td colspan="1" rowspan="1">1855年</td>
<td colspan="1" rowspan="1">3204</td>
<td colspan="1" rowspan="1">1218.5</td>
<td colspan="1" rowspan="1">3.1554</td>
</tr><tr>
<td colspan="1" rowspan="1">C.DeMorgan</td>
<td colspan="1" rowspan="1">1860年</td>
<td colspan="1" rowspan="1">600</td>
<td colspan="1" rowspan="1">382.5</td>
<td colspan="1" rowspan="1">3.137</td>
</tr><tr>
<td colspan="1" rowspan="1">Fox</td>
<td colspan="1" rowspan="1">1884年</td>
<td colspan="1" rowspan="1">1030</td>
<td colspan="1" rowspan="1">489</td>
<td colspan="1" rowspan="1">3.1595</td>
</tr><tr>
<td colspan="1" rowspan="1">Lazzerini</td>
<td colspan="1" rowspan="1">1901年</td>
<td colspan="1" rowspan="1">3408</td>
<td colspan="1" rowspan="1">1808</td>
<td colspan="1" rowspan="1">3.1415929</td>
</tr><tr>
<td colspan="1" rowspan="1">Reina</td>
<td colspan="1" rowspan="1">1925年</td>
<td colspan="1" rowspan="1">2520</td>
<td colspan="1" rowspan="1">859</td>
<td colspan="1" rowspan="1">3.1795</td>
</tr></table>

Buffon's needle problem, 1777

Laplace's method of calculating pi(1886)

百度百科

·采样：一种算法设计的范式！

西安交通大學


![](https://web-api.textin.com/ocr_image/external/3b84d2a52fe584a3.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/a4f40a24928595df.jpg)

·更一般的，求解 $\theta = \int _ { a } ^ { b } f ( x ) d x$

·通常假设x符合Uniform（a，b） $\frac { b - a } { n } \sum _ { i = 0 } ^ { n - 1 } f ( x _ { i } )$

·如果x符合其它分布，分布函数为p(X)

$\theta = \int _ { a } ^ { b } f ( x ) d x = \int _ { a } ^ { b } \frac { f ( x ) } { p ( x ) } p ( x ) d x \approx \frac { 1 } { n } \sum _ { i = 0 } ^ { n - 1 } \frac { f ( x _ { i } ) } { p ( x _ { i } ) }$ Monte Carlo方法

·核心问题：给定一个概率分布p(x)，如何生成对应的样本？

·与概率密度估计问题相反

https://cloud.tencent.com/developer/article/1189419 西安交通大學


![](https://web-api.textin.com/ocr_image/external/bc477ff909deaa08.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/80f601bcad50c74e.jpg)

## ·均匀分布Uniform（0,1）：线性同余发生器生成伪随机数

$x _ { n + 1 } = ( a x _ { n } + c ) m o d m$

·二项分布与多项分布？ $f ( x ) = ( _ { x } ^ { n } ) p ^ { x } ( 1 - p ) ^ { n - x }$ $f ( \overrightarrow { n } \vert \overrightarrow { p } , N ) = ( \frac { N } { n } ) \prod _ { k = 1 } ^ { K } p _ { k } ^ { n _ { k } }$

·高斯分布（Gaussian distribution）：Box-Muller 变换

$f ( x ) = \frac { 1 } { \sqrt { 2 \pi } \sigma } \exp ( - \frac { ( x - \mu ) ^ { 2 } } { 2 \sigma ^ { 2 } } )$

如果随机变量U1，U2独立且服从Uniform［0,1］， $X = \cos ( 2 \pi U _ { 1 } ) \sqrt { - 2 \ln U _ { 2 } }$

X，Y独立且服从均值为0，方差为1的高斯分布 $Y = \sin ( 2 \pi U _ { 1 } ) \sqrt { - 2 \ln U _ { 2 } }$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0bbdf3831affdc86.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/611f4f6cfd6e4fe3.jpg)

## ·Direct sampling (Inversion)

1. Generating a single u \~Uniform (0,1)

2. Suppose we want to simulate x whose distribution has a given CDF F

3. Then sampling u \~ Unif(0,1)and setting$x = F ^ { - 1 }$(u)

$F ^ { - 1 } ( u ) = \inf \{ x : F ( x ) \geq u \}$

为什么不是反函数？

<!-- 1.0 -PDF 0.8 u CDF 0.6 x 0.4 0.2 0.04 -2 0 2 4  -->
![](https://web-api.textin.com/ocr_image/external/d1b2fdc952459fc6.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/c766b83d2f1f851a.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/220a3333bc77b10f.jpg)

# Direct sampling (Inversion)

For an exponential distribution with rate λ, we have

$f ( x ) = \lambda e ^ { - \lambda x }$ and $F ( x ) = 1 - e ^ { - \lambda x } .$

It is easy to check that the inverse cdf is

$F ^ { - 1 } ( u ) = - \log ( 1 - u ) / \lambda ,$ u∈(0,1)

Therefore, to sample X from an exponential distribution:

1 Sample U \~ Unif(0,1).

2StX=-log(-)/λ

Ryan Martin's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/50e817e5888307e3.jpg)

XIAN JIAOTONG UNIVEESITY


![](https://web-api.textin.com/ocr_image/external/db9e651b1e76ac53.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/39da597258ae7b3d.jpg)

Direct sampling (Inversion)

·如果概率分布p(x)的F或F-1不容易表示呢？很多概率分布的累积分布函数没有解析表达式，如高斯分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/ad4dfe0aab2152a1.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/db49599f7541dcae.jpg)

## Monte Carlo方法

·接受-拒绝采样：设定一个可采样的分布q(X)，如高斯分布，然后按照一定的方法拒绝某些样本，以达到接近p(X)的目的① 设定一个常量k，使得p(X)总在kq(X)的下方。

②采样得到q(X)的一个样本$Z _ { 0 } ,$ 从均匀分布(0,$k q ( Z _ { 0 } ) )$中采样得到一个值u。如果u落在灰色区域，则拒绝这次抽样，否则接受样本$Z _ { 0 }$。

③重复以上过程得到n个接受的样本$Z _ { 0 } ,$ $Z _ { 1 } , \cdots Z _ { n - 1 }$

缺点？

<!-- k q ( z _ { 0 } ) kq(z) 高概率拒绝 高概率接收 u0 \tilde { p } ( z ) 20 z  -->
![](https://web-api.textin.com/ocr_image/external/ab6cc7e04e1b5c5d.jpg)

https://cloud.tencent.com/developer/article/1189419

西安交通大學


![](https://web-api.textin.com/ocr_image/external/8186e273d42da033.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/9fdb8a207c80cb24.jpg)

## ·优点：样本独立、可以并行

## ·局限性

①某些二维分布p(X,y)，只能得到条件分布p(x|y)和p(y|x),很难得到p(X,y)的一般形式，无法使用接受-拒绝采样。

②一些高维的复杂非常见分布$p ( X _ { 1 } , X _ { 2 } , \cdots , X _ { n } )$，找到一个合适的q(X)和k非常难。


![](https://web-api.textin.com/ocr_image/external/4b9de75195e34373.jpg)


![](https://web-api.textin.com/ocr_image/external/aa2496f8d59cc048.jpg)


![](https://web-api.textin.com/ocr_image/external/94a7cf1ddf249522.jpg)

Jupyter

## 接受-拒绝采样

·https://colab.research.google.com/github/cranmer/stats-ds-book/blob/master/book/distributions/accept-reject.ipynb#scrollTo=JBuBj8dbb3_6

西安交通大學


![](https://web-api.textin.com/ocr_image/external/461cb79af4a2edbb.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/53ddd0e3b41a3cef.jpg)

# Metropolis 采样

·易于抽样和又近似的分布q(X)通常是很难的。

·Metropolis采样：一种具体的马尔可夫链蒙特卡罗（Markov Chain Monte Carlo,MCMC)

·MCMC：依赖Markov属性在概率空间进行的Monte Carlo采样来估计后验分布的参数。 A Nasty Looking Likelihood

<!-- 0.10 0.08 AHIdedod 0.06 0.04 A Nasty Prior 0.02 0.00 40 50 60 70 80 Height in Inches 90 100 110 120  -->
![](https://web-api.textin.com/ocr_image/external/09572c8a7ef890bd.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/85538a962d0a10d6.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/dba4313ec86b848c.jpg)

# Metropolis 采样

·Markov 链是一个满足Markov性质随机过程$X _ { 1 } ,$ $X _ { 2 } , \cdots ,$即$x _ { t + 1 }$的状态分布只依赖于最邻近的状态$x _ { t }$

$\cdot X _ { t + 1 } , X _ { t } , X _ { t - 1 }$的状态取值于{sl,⋯,sm}

$P [ X _ { t + 1 } = X _ { t + 1 } \vert X _ { t } = X _ { t }$3$X _ { t - 1 } = X _ { t - 1 } ,$,⋯,$X _ { 1 } = X _ { 1 }$,$X _ { 0 } = X _ { 0 } ]$ $= P [ X _ { t + 1 } = X _ { t + 1 } \vert X _ { t } = X _ { t } ]$

<!-- S1 S2 S3 S _ { i - 1 } Si S _ { i + 1 }  -->
![](https://web-api.textin.com/ocr_image/external/ce4b77c8e0802da7.jpg)

From Hadas Barkay and Anat Hashavit's PPT

西安交通大學


![](https://web-api.textin.com/ocr_image/external/c282fd8ab331951e.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/80f601bcad50c74e.jpg)

# Metropolis 采样

## Markov链包含两部分：

1．m维的初始分布向量

$( \pi ( s _ { 1 } ) , \cdots , \pi ( s _ { m } ) )$

2.m×m转移矩阵$P = ( A _ { i j } ) s . t P ( i , j ) = P r [ x _ { t + 1 } = s _ { i } \vert x _ { t } = s _ { j } ]$

<!-- 0.4 0.3 1 0.3 3 0.2 4 0.4 1 0.1 0.5 0.7 0.3 2 5 6 0.6 0.5 0.7  -->
![](https://web-api.textin.com/ocr_image/external/72c19380b6411227.jpg)

$P = [ \begin{matrix} 0 . 4 & 0 & 0 \\ 0 . 3 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 . \end{matrix}$00000 00.20.1 00.330.7 05 0 0.53 0 0.7

西安交通大學


![](https://web-api.textin.com/ocr_image/external/50e817e5888307e3.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/1ee36e403ac420a0.jpg)

# Metropolis 采样

## $X _ { t }$与$X _ { t + 2 }$的转移概率

$P r ( x _ { t + 2 } = s _ { k } \vert x _ { t } = s _ { i } ) = \sum _ { j } P r ( x _ { t + 2 } = s _ { k } , x _ { t + 1 } = s _ { j } \vert x _ { t } = s _ { i } )$

$= \sum _ { j } P r ( x _ { i + 2 } = s _ { k } \vert x _ { i } = s _ { j } ) P r ( x _ { i + 1 } = s _ { j } \vert x _ { t } = s _ { i } )$

$= \sum _ { j } p _ { i j } p _ { j k } = P ^ { 2 } i k$

·更一般地， $P ^ { n } _ { i k } = P r ( x _ { t + n } = s _ { k } \vert x _ { t } = s _ { i } )$

<table border="1" ><tr>
<td colspan="1" rowspan="1">.24</td>
<td colspan="1" rowspan="1">0.76</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">$0 . 3 0 4$</td>
<td colspan="1" rowspan="1">0.696</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">$0 . 1 6 8$</td>
<td colspan="1" rowspan="1">0.3</td>
<td colspan="1" rowspan="1">0.064</td>
<td colspan="1" rowspan="1">0.194</td>
<td colspan="1" rowspan="1">0.149</td>
<td colspan="1" rowspan="1">0.125</td>
</tr><tr>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.342</td>
<td colspan="1" rowspan="1">0.308</td>
<td colspan="1" rowspan="1">0.35</td>
</tr><tr>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.37</td>
<td colspan="1" rowspan="1">0.21</td>
<td colspan="1" rowspan="1">0.42</td>
</tr><tr>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.342</td>
<td colspan="1" rowspan="1">0.21</td>
<td colspan="1" rowspan="1">0.448</td>
</tr></table>

<table border="1" ><tr>
<td colspan="1" rowspan="1">0.4</td>
<td colspan="1" rowspan="1">0.6</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1">0.24</td>
<td colspan="1" rowspan="1">0.76</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
</tr><tr>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">0.3</td>
<td colspan="1" rowspan="1">0.16</td>
<td colspan="1" rowspan="1">0.19</td>
<td colspan="1" rowspan="1">0.18</td>
<td colspan="1" rowspan="1">0.05</td>
</tr><tr>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.44</td>
<td colspan="1" rowspan="1">0.21</td>
<td colspan="1" rowspan="1">0.35</td>
</tr><tr>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.3</td>
<td colspan="1" rowspan="1">0.35</td>
<td colspan="1" rowspan="1">0.35</td>
</tr><tr>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0</td>
<td colspan="1" rowspan="1">0.3</td>
<td colspan="1" rowspan="1">0.21</td>
<td colspan="1" rowspan="1">0.49</td>
</tr></table>

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0aba8478e9be3a5f.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/7cd8dc60fef9a22a.jpg)

# Metropolis 采样

## ·Markov链的属性

✓不可约性（Irreducible）：所有的状态都是连通（Communicate）的；状态i，j连通是指存在一个n，使得$P ^ { n } _ { i j } > 0$，也就是在演变过程中，随机变量可以在任意状态间转移

重现性（Recurrence）：若Markov链在到达一个状态后，在演变中能反复回到该状态，则该状态具有重现性。给定状态的返回时间是其所有可能返回时间的下确界（infimum）

$T _ { i } = \inf \{ n > 0 : X _ { n } = S _ { i } \vert X _ { 0 } = S _ { i } \} ,$ $T _ { i } = \infty i f \forall n > 0 : X _ { n } \neq S _ { i }$

若$T _ { i } = \infty$，则该状态不存在重现性

$P = [ \begin{matrix} \frac { 1 } { 2 } & \frac { 1 } { 2 } & \frac { 1 } { 2 } & 0 \\ 0 & \frac { 1 } { 4 } & \frac { 1 } { 4 } & \frac { 1 } { 4 } \\ 0 & \frac { 1 } { 3 } & \frac { 2 } { 3 } \end{matrix} ] P$ $P = [ \begin{matrix} \frac { \frac { 1 } { 2 } & \frac { 1 } { 2 } & \frac { 1 } { 2 } & 0 & 0 \\ 0 & 0 & \frac { 1 } { 4 } & \frac { 3 } { 4 } & \frac { 3 } { 4 } \end{matrix} ]$ B%E9%93%BE/6171383?fr=aladdin#3 2https://baike.baidu.com/item/%E9%A9%A C%E5%B0%94%E5%8F%AF%E5%A4%A

西安交通大學


![](https://web-api.textin.com/ocr_image/external/9bf9ae3545c9be4e.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/6181640906f5d274.jpg)

# Metropolis 采样

·周期性（periodicity）：一个重现的Markov链可能具有周期

性，能够按大于1的周期重现其状态。 https://baike.baidu.com/item/%E9%A9%AC%E 5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/6171383?fr=aladdin#3_2

$d = g c d \vert n > 0 : p ( X _ { n } = s _ { i } \vert X _ { 0 } = s _ { i } ) > 0 \}$

·一个不可约且非周期的Markov链具有平稳分布。

·平稳分布（stationary distribution）

分布$\prod = ( \pi 1 _ { i \cdot \cdot r } m )$）是平稳的当Πp=Π且II是唯一时✓状态j长期概率是IIj，与初始状态无关.$\lim _ { N \rightarrow \infty } P ^ { n } = [ \begin{matrix} \pi ( 1 ) \pi ( 1 ) \cdots \pi ( 2 ) \cdots \\ \pi ( 0 ) \pi ( 1 ) \cdots \pi ( 2 ) \cdots \\ \pi ( 0 ) \pi ( 1 ) \cdots \pi ( 2 ) \cdots$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/39711b9a940f0782.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d0a30be66017adc4.jpg)

# Metropolis 采样

## ·分布II的含义？

## ·如果分布是连续的？

<!-- S7.0 o2.0 ST.0 DI.0 S0.0 00.0 4 6 8 10 12 14  -->
![](https://web-api.textin.com/ocr_image/external/457a90532810811c.jpg)

西安交通大學


![](https://web-api.textin.com/ocr_image/external/8bbeeb2e117e0b21.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/80f601bcad50c74e.jpg)

# Metropolis 采样

·细致平衡条件（Detailed Balance Condition）：若Markov链具有唯一的分布π和转移矩阵P满足下式，则π为平稳分布

$\pi _ { i } P _ { i j } = \pi _ { j } P _ { j i } ,$ ∀i,j

# ·DBC是充分条件

·证明：· $\sum _ { i } \pi _ { i } P _ { i j } = \sum _ { i } \pi _ { j } P _ { j i } = \pi _ { j } \sum _ { i } P _ { j i } = \pi _ { j }$

.. πP=π

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0aba8478e9be3a5f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/7cd8dc60fef9a22a.jpg)

# Metropolis 采样

·平稳分布是什么？

<!-- 0.3 E 0.7 0.4 A 0.6  -->
![](https://web-api.textin.com/ocr_image/external/5a5e02c9702da49b.jpg)

·符合DBC吗？

·DBC是充要条件的条件是什么？

西安交通大學


![](https://web-api.textin.com/ocr_image/external/284bdf93e700e81f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/1a2a9b19a2f0cb52.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/58845dd22247aac0.jpg)

## 马尔科夫链

西安交通大學


![](https://web-api.textin.com/ocr_image/external/98164e20ec16874b.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/b7ea9a7b19c7290c.jpg)

Metropolis采样：假设需要采集复杂分布π(x)的样本，转移矩阵P以π(x)为平稳分布。从一个已知的对称分布θ(x)中采样，转移矩阵Q以θ(x)为平稳分布。从$\theta ( x \vert x _ { n - 1 } )$采样得到一个候选$\frac { \pi ( x ) } { \pi ( x _ { n - 1 } ) } )$接受x。当随机生成的概率样本$\hat { x } _ { 1 }$以概率α=min(1,值小于$\alpha _ { 5 }$ $x _ { n } = \hat { x }$,否则$x _ { n } = x _ { n - 1 }$

·解释：设随机变量从状态$S _ { i }$转移到$s _ { j }$的接受概率为$\alpha _ { i j }$

$P _ { i j } = \alpha _ { i j } * Q _ { i j }$

如果证明P满足DBC，即$\pi ( i ) P _ { i j } = \pi ( j ) P _ { j i }$，则P的平稳分布为π(x)

$\pi ( i ) P _ { i j } = \pi ( i ) * \alpha _ { i j } * Q _ { i j } = \pi ( i ) * \min ( 1 , \frac { \pi ( j ) } { \pi ( i ) } ) * Q _ { i j }$

https://blog.csdn.net/jingjishisi/article/details/79291258

西安交通大學


![](https://web-api.textin.com/ocr_image/external/6feff89e54dad855.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/1ee36e403ac420a0.jpg)

# Metropolis 采样

$\pi ( i ) P _ { i j } = \min ( \pi ( i ) * Q _ { i j } , \pi ( j ) * Q _ { i j } )$ 对称分布

$\pi ( i ) P _ { i j } = \min ( \pi ( i ) * Q _ { j } , \pi ( j ) * Q _ { j } ) = \pi ( j ) * \min ( 1 , \frac { \pi ( i ) } { \pi ( j ) } ) * Q _ { j i }$

$\pi ( i ) P _ { i j } = \pi ( j ) * \alpha _ { j i } * Q _ { j i } = \pi ( j ) * P _ { j i }$

https://blog.csdn.net/jingjishisi/article/details/79291258

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0d42d950f8fe9caa.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/28e3bde2f010aff4.jpg)

# Metropolis 采样

<table border="1" ><tr>
<td colspan="1" rowspan="1">Metropolis 采样算法</td>
</tr><tr>
<td colspan="1" rowspan="1">step1．初始化：$t = 0$，随机生成$一 个 x _ { 0 }$赋值给当前的$x _ { t }$，迭代终止条件为t＝T</td>
</tr><tr>
<td colspan="1" rowspan="1">step2．令$t = t + 1$，从条件概率分布$\theta ( x \vert x _ { t - 1 } )$中生成候选样本x</td>
</tr><tr>
<td colspan="1" rowspan="1">step3．计算接受概率$\alpha _ { 1 }$ $\alpha = \min ( 1 ,$$\frac { \pi ( x ) } { \pi ( x _ { t - 1 } ) } )$</td>
</tr><tr>
<td colspan="1" rowspan="1">step4．从均匀分布中生成一个随机数$\alpha _ { t }$</td>
</tr><tr>
<td colspan="1" rowspan="1">step5．若$\alpha _ { t } \leq \alpha _ { t }$ 则接受候选样本， $x _ { t } = \hat { x }$，否则，拒绝候选样本并令$x _ { t } = x _ { t - 1 }$</td>
</tr><tr>
<td colspan="1" rowspan="1">step6．若t＝T，停止迭代，否则回到第2步继续迭代</td>
</tr></table>

算法停止后，生成样本序列 $( x _ { 0 } , x _ { 1 } , \cdots , x _ { T - 1 } , x _ { T } )$ ，根据需要截取尾部的n个样本 (xT-n+1, ⋯,xT-1,xT) ，可近似地认为从分布π(x)中采样得到。,xT-n+2,n

https://blog.csdn.net/jingjishisi/article/details/79291258

西安交通大學


![](https://web-api.textin.com/ocr_image/external/0aba8478e9be3a5f.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/c3cdc5c9f2688513.jpg)

# Metropolis 采样


![](https://web-api.textin.com/ocr_image/external/9d65459434a8f0fe.jpg)

一个政客访问一些岛屿以争取支持，他使用一个简单规则来决定下一个访问的岛屿：每天选择一个邻近的岛屿，并将邻岛人口与当前岛人口进行比较。如果邻岛人口较多就过去。如果邻岛人口较少，就以p＝邻岛人口数／当前岛人口数的概率进行访问。这样多天之后，政客最终在每个岛上的时间与每个岛的人口成正比。即估计了岛屿人口的分布。

例子来源：Kruschke， J．（2014）．Doing Bayesian data analysis： A tutorial with R，JAGS, and Stan.

西安交通大學


![](https://web-api.textin.com/ocr_image/external/77914470d3d6631e.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/fb05f452b80eccd3.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/8f0b4d429366466c.jpg)

# Metropolis 采样

西安交通大學


![](https://web-api.textin.com/ocr_image/external/544b31806e5bf249.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/0f66c1e7c1b966db.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/b7d2cf470a80c043.jpg)

# 采用Metropolis-Hastings算法

## 估计硬币偏差

西安交通大學


![](https://web-api.textin.com/ocr_image/external/c701fdbac1f1b3a7.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/2b074003aa659175.jpg)

Gibbs采样：Metropolis算法在逐个元素基础上的特例，适用于采样多元联合概率分布$p ( x ) = p ( x _ { 1 } , x _ { 2 } , \cdots , x _ { n } )$，其中$x = ( x _ { 1 } , x _ { 2 } , \cdots ,$ $x _ { n } )$为n维随机变量。

·基于满条件分布：第i个满条件分布是x的第i个成分的条件分布，以x的所有其它成分的最近值为条件；

·Gibbs采样是一次一次地更新x的组成部分。

 满条件分布：如果条件概率分布$p ( x _ { 1 } \vert x _ { - 1 } )$中出现了以上所有n 个变量，就被称作满条件分布（full conditional distribution）

$x _ { I } = \{ x _ { i } \vert i \in I \}$

$x _ { - I } = \{ x _ { i } \vert i \notin I \}$

I⊂K={1,2,⋯,n}

西安交通大學


![](https://web-api.textin.com/ocr_image/external/3d4d5d7e01bf7abb.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/08b848abc414c591.jpg)

## Gibbs采样：n维样本可以分解为解为n个子过程


![](https://web-api.textin.com/ocr_image/external/f1b1246af6e209cd.jpg)

1.Initialise xo,1:n.

2.Fori=0toN-1

- Sample$x _ { 1 } ^ { ( i + 1 ) }$ $\sim p ( x _ { 1 } \vert x _ { 2 } ^ { ( i ) }$,$x _ { 3 } ^ { ( i ) } , \cdots , x _ { n } ^ { ( i ) } ) .$

- Sample$x _ { 2 } ^ { ( i + 1 ) }$ $\sim p ( x _ { 2 } \vert x _ { 1 } ^ { ( i + 1 ) } ,$ $x _ { 3 } ^ { ( i ) } , \cdots , x _ { n } ^ { ( i ) } )$

<!-- 基于坐标轴轮换，每 次只更新一个维度  -->
![](https://web-api.textin.com/ocr_image/external/9a5e090fd999d645.jpg)

·.·

Sample$x _ { j } ^ { ( i + 1 ) } \sim p ( x _ { j } \vert x _ { 1 } ^ { ( i + 1 ) } , \cdots , x _ { j - 1 } ^ { ( i + 1 ) }$, .$x _ { j + 1 } ^ { ( i ) } , \cdots , x _ { n } ^ { ( i ) } )$

Sample$x _ { n } ^ { ( i + 1 ) } \sim p ( x _ { n } \vert x _ { 1 } ^ { ( i + 1 ) } ,$.$x _ { 2 } ^ { ( i + 1 ) } , \cdots x _ { n - 1 } ^ { ( i + 1 ) } )$

·相同：也满足DBC，需要burn-in过程，适合高维空间的采样

·不同：高维数据，条件概率分布较容易获得，联合分布不好求

西安交通大學


![](https://web-api.textin.com/ocr_image/external/39711b9a940f0782.jpg)

XIAN JIAOTONG UNIVIRSITY

<!-- LDA模型  -->
![](https://web-api.textin.com/ocr_image/external/26f3f3e2ae5a90c3.jpg)


![](https://web-api.textin.com/ocr_image/external/7ec434d1916b9be3.jpg)

Jupyter


![](https://web-api.textin.com/ocr_image/external/b7d2cf470a80c043.jpg)

## 采用Gibbs采样算法

## 采样2D正态分布

西安交通大學


![](https://web-api.textin.com/ocr_image/external/98164e20ec16874b.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/21fd7e181a6a24c2.jpg)

用Gibbs采样训练LDA模型

<!-- ·求解联合概率分布 p ( \overline { w } , \overline { z } \vert \overrightarrow { \alpha } , \overrightarrow { \beta } ) = p ( \overline { w } \vert \overline { z } , \overline { \beta } ) p ( \overline { z } \vert \overrightarrow { \alpha } ) \overrightarrow { \alpha } \in R ^ { K } \overrightarrow { \beta } \in R ^ { V } p ( \overline { w } \vert \overline { z } , \overline { \beta } ) = \prod _ { k = 1 } ^ { K } p ( \overline { \omega } _ { k } \vert \overline { \beta } ) = \prod _ { k = 1 } ^ { K } \frac { \Delta ( \overline { \beta } + \overline { n } _ { k } ) } { \Delta ( \overline { \beta } ) } p ( \overrightarrow { z } \vert \overrightarrow { \alpha } ) = \prod _ { m = 1 } ^ { M } p ( \overrightarrow { z } _ { m } \vert \overrightarrow { \alpha } ) = \prod _ { m = 1 } ^ { M } \frac { \Delta ( \overrightarrow { \alpha } + \overrightarrow { n } _ { m } ) } { \Delta ( \overrightarrow { \alpha } ) } 第k个主题的 第k个主题中 第m篇文档的 第m篇文档不同 词概率分布 不同词的个数 主题概率分布 主题出现的次数  -->
![](https://web-api.textin.com/ocr_image/external/b8fc64f082334e6d.jpg)

$D i r i c h l e t ( \overline { p } \vert \overline { \alpha } ) = \frac { \Gamma ( \sum _ { k = 1 } ^ { K } \alpha _ { k } ) } { \prod _ { k = 1 } ^ { K } \Gamma ( \alpha _ { k } ) } \frac { K } { \prod _ { k = 1 } ^ { \mu } } p _ { k } ^ { \alpha k - 1 } \frac { 1 } { \sum$ $p ( \overline { u } , \overline { z } \vert \overrightarrow { \alpha } , \overrightarrow { \beta } ) = \prod _ { k = 1 } ^ { K } \frac { \Delta ( \overline { \beta } + \overline { n } _ { k } ) } { \Delta ( \overline { \beta } ) } \frac { M } { \prod _ { m = 1 } } \frac { \Delta ( \overline { \alpha } + \overline { n } _ { m } ) } { \Delta ( \overline {$

归一化参数

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f23b7809be636268.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/d7d304473d1d1804.jpg)

用Gibbs采样训练LDA模型

·求Gibbs采样需要的条件分布$p ( z _ { i } = k \vert \overrightarrow { z } _ { - i } , \overrightarrow { w } ) .$

i=(m,n) 第m篇文档中第n个词的下标，$w _ { i } = t$,$z _ { i } = k$

-i 表示去除下标为i的词

除去下标为i这个词，剩下的所有词 $\overrightarrow { n } _ { k , - i } = ( n _ { k } ^ { ( 1 ) } ,$ $, n _ { k } ^ { ( 2 ) } , \cdots , n _ { k } ^ { ( t ) } - 1 , \cdots , n _ { k } ^ { ( V ) } )$中，词t属于主题k的统计次数

除去下标为i的这个词，第m篇文 $\overrightarrow { n } _ { m , - i } = ( n _ { m } ^ { ( 1 ) } , n _ { m } ^ { ( 2 ) } , \cdots , n _ { m } ^ { ( k ) } - 1 , \cdots , n _ { m } ^ { ( k ) } )$档中主题m产生词的个数

$p ( z _ { i } = k \vert \overline { z } _ { - i } , \overline { w } ) = \frac { p ( \overline { u } , \overline { z } ) } { p ( \overline { v } , \overline { z } _ { - i } ) }$

$= \frac { p ( \overline { u } \vert \overline { z } ) } { p ( \overline { u } _ { - i } \vert \overline { z } _ { - i } ) p ( w _ { i } ) } \cdot \frac { p ( \overline { z } ) } { p ( \overline { z } _ { - i } ) }$

$\alpha \frac { \Delta ( \overrightarrow { n } _ { k } + \overline { \beta } ) } { \Delta ( \overrightarrow { n } _ { k , - i } + \overline { \beta } ) } \cdot \frac { \Delta ( \overrightarrow { n } _ { m } + \overline { \alpha } ) } { \Delta ( \overrightarrow { n } _ { m , - i } + \overline { \alpha } ) }$

$= \frac { n _ { k _ { k _ { j - i } } ^ { ( k ) } + \beta ( t ) } { \sum _ { v _ { k _ { j - i } } ^ { V } ( n _ { k } ) } + \beta ( v ) } \cdot \frac { n _ { m _ { j } - i } ^ { ( k ) } + \alpha ( k ) } { \sum _ { j = 1 } ^ { K$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/f168c5826db169ec.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/2075aa8219f9890d.jpg)

算法2.2-LDAGibbs算法．

输入：D，K，α，η；输出：θ与β

初始化：

对于D中所有文档m∈[1,M]：将$\hat { n } _ { m } = ( \hat { n } _ { m _ { s } 1 , \hat { n } _ { m _ { s } 2 } , \cdots , \hat { n } _ { m _ { s } } K )$初始化为零向量；

对于所有主题k∈[1,K]：将$n _ { k } = ( n _ { k , 1 } , n _ { k , 2 } , \cdots , n _ { k , V } )$初始化为零向量；

对于所有m∈[1,M]。

对于m中所有词汇$w _ { m , n } ( n \in [ 1 , N ] )$:

执行$z _ { m , n } \sim M u l t ( 1 / K )$,

若$z _ { m , n }$为k，则$\hat { n } _ { m , k } + = 1$

若$w _ { m , n }$为v，则nk,v+=1

Gibbs 采样： $p ( z _ { i } = k \vert Z _ { - i j } D ) \alpha \frac { n _ { k , 0 , - i } + \eta _ { 0 } } { \sum _ { 0 = 1 } ^ { V } ( n _ { k , 0 , i } + \eta _ { 0 } ) } \cdot \frac { \hat { n } _ { m } k _ { k - i } + \alpha _ { k } } { \sum _ { k$

重复执行以下过程，直到收敛：

对于D中所有词汇$w _ { m , n }$ $\alpha \frac { n _ { k , 0 , - i } + \eta _ { 0 } } { \sum _ { v = 1 } ^ { V } ( n _ { k } , v _ { j } - i + \eta _ { 0 } ) } \cdot ( \hat { n } _ { m , k , - i } + \alpha _ { k } ) .$

若$z _ { m , n }$为k， $w _ { m , n }$为v，则$\hat { n } _ { m , k } - = 1$, nk,v-=1

利用公式2.18采样得到$w _ { m , n }$的新主题： $\hat { k } \sim p ( z _ { m , n } \vert Z _ { - } ( m , n ) , D )$;

$\hat { n } _ { m , k } + = 1 ,$ $n _ { k , v } + = 1$

参数计算：

根据公式2.20计算β；根据公式2.22计算θ。

$\beta _ { k } = ( \frac { n _ { k , 1 } + \eta _ { 1 } } { \sum _ { 0 = 1 } ^ { V } ( n _ { k , 0 } + \eta _ { 0 } ) } , \frac { n _ { k , 2 } + \eta _ { 2 } } { \sum _ { 0 = 1 } ^ { V } ( n _ { k , 0 } + \eta _ { 0 } ) } , \cdots ,$ $\theta _ { m } = ( \frac { \hat { n } _ { m 1 } + \alpha _ { 1 } } { \sum _ { k = 1 } ^ { k } ( \hat { n } _ { m k } + \alpha _ { k } ) } , \frac { \hat { n } _ { m _ { 2 } } + \alpha _ { 2 } } { \sum _ { k = 1 } ^ { k } ( \hat { n } _ { m _ {$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/bd20a22eb932ea04.jpg)

XIAN JIAOTONG UNIVIRSITY


![](https://web-api.textin.com/ocr_image/external/caa53577320793cb.jpg)

例2.1 利用LDA对以下五个文档进行分析，在此基础上进行文档聚类。文档集：

$d _ { 1 }$：达芬奇的代表作包括《蒙娜丽莎》。

$d _ { 2 }$：大卫像是文艺复兴时期米开朗基罗的代表作。

$d _ { 3 }$：米开朗基罗、达芬奇是文艺复兴时期的艺术领域代表性人物。

$d _ { 4 }$：电影是科技与艺术相结合的产物。

$d _ { 5 }$⋅：不少科幻电影中的科技已经成为现实。

# 词汇表：

［1：达芬奇 2：代表作 3：文艺复兴 4：米开朗基罗 5：艺术 6：电影

7：科技 8：科幻］

西安交通大學


![](https://web-api.textin.com/ocr_image/external/80afeff9b1b6b577.jpg)

XIAN JIAOTONG UNIVERSITY


![](https://web-api.textin.com/ocr_image/external/e42a6087bbc3b0dd.jpg)

<!-- D d _ { 1 } 1 2 1 2 d _ { 2 } 2 3 4 2 d _ { 3 } 一 4 3 5 d _ { 4 } 5 6 7 5 d _ { 5 } 6 7 8 6  -->
![](https://web-api.textin.com/ocr_image/external/cf2335732b301933.jpg)

<table border="1" ><tr>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">2</td>
</tr><tr>
<td colspan="1" rowspan="1">2</td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">4</td>
<td colspan="1" rowspan="1">2</td>
</tr><tr>
<td colspan="1" rowspan="1">1</td>
<td colspan="1" rowspan="1">4</td>
<td colspan="1" rowspan="1">3</td>
<td colspan="1" rowspan="1">5</td>
</tr><tr>
<td colspan="1" rowspan="1">$5 \vert 6$</td>
<td colspan="1" rowspan="1"></td>
<td colspan="1" rowspan="1">7</td>
<td colspan="1" rowspan="1">5</td>
</tr><tr>
<td colspan="1" rowspan="1">6</td>
<td colspan="1" rowspan="1">7</td>
<td colspan="1" rowspan="1">8</td>
<td colspan="1" rowspan="1">6</td>
</tr></table>

图2.3：文档集D及其初始的主题分配Z

$p ( z _ { i } = k \vert Z _ { - i } , D ) \alpha \frac { n _ { k , v , - i } + 0 . 1 } { \sum _ { 0 = 1 } ^ { V } n _ { k , v , - i + 0 . 8 } \cdot ( \hat { n } _ { m , k , - i } + 0 . 3 )$

$= \frac { n _ { k , v , - i + 0 . 1 } { n _ { k } - 0 . 2 } \cdot ( \hat { n } _ { m , k , - i } + 0 . 3 )$

西安交通大學


![](https://web-api.textin.com/ocr_image/external/90b05107ee5ebf00.jpg)

XIAN JIAOTONG UNIVIRSITY

