# $\textbf{0. }$写在前面

> :one:本课程总体结构
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241119154414877.png" alt="image-20241119154414877" width=600 /> 
>
> - ==这门课由于由两位老师授课，个人感觉结构比较混乱==
>
> - ==由于时间紧任务重经费无，所以笔记还是按$\text{PPT}$内容和以上结构展开，即使有很多不合理的地方==
>
> :two:考试有关事项
>
>  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/24950624a141b9941650b6187682aa9.jpg" alt="24950624a141b9941650b6187682aa9" width=350 /> 

# $\textbf{1. }$基于矩阵分解的主题模型

> ## $\textbf{1.0. }$概述
>
> > :one:分布模型：$\text{Doc.}\xrightarrow[非监督学习(聚类)]{主题分布}
> > \begin{cases}
> > \textbf{Topic 1}(P_{T_1})\xrightarrow[非监督学习(聚类)]{词语分布}
> > \begin{cases}
> > \text{Word}_{11}(P_{W_{11}})\\
> > \text{Word}_{12}(P_{W_{11}})\\
> > \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots    \\
> > \text{Word}_{1m}(P_{W_{1m}})\\
> > \end{cases}
> > \\
> > \textbf{Topic 2}(P_{T_2})\xrightarrow[非监督学习(聚类)]{词语分布}
> > \begin{cases}
> > \text{Word}_{21}(P_{W_{21}})\\
> > \text{Word}_{22}(P_{W_{21}})\\
> > \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots    \\
> > \text{Word}_{2m}(P_{W_{2m}})\\
> > \end{cases}
> > \\
> > \,\,\,\,\,\,\,\,\,\,\,\,\,\vdots
> > \\
> > \textbf{Topic n}(P_{T_n})\xrightarrow[非监督学习(聚类)]{词语分布}
> > \begin{cases}
> > \text{Word}_{n1}(P_{W_{n1}})\\
> > \text{Word}_{n2}(P_{W_{n1}})\\
> > \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\vdots    \\
> > \text{Word}_{nm}(P_{W_{nm}})\\
> > \end{cases}
> > \end{cases}$ 
> >
> > 1. 主题分布：每篇文档由若干主题按一定比例构成
> > 2. 词语分布：每个主题包含一组特定的词语，每个词具有不同的出现概率
> >
> > :two:概率模型
> >
> > 1. 公式：$\displaystyle{}p(w | \mathrm{Doc})=\sum_{i=1}^n p\left(w | T_i\right) \cdot p\left(T_i | \mathrm{Doc}\right)$ 
> >
> > 2. 含义：将文档的内容视为不同主题的组合$\to$由每主题的词语概率预测文档中词语的分布
>
> ## $\textbf{1.1. LSA(SVD)}$模型
>
> > :one:奇异值分解
> >
> > 1. 含义：对任意$A_{m\text{×}n}$可将其分解为三个矩阵$A_{m\text{×}n}\text{=}U_{m\text{×}m}\Sigma_{m\text{×}n}V_{n\text{×}n}^{T}\text{}$
> >
> >    |            矩阵类型             | 描述                                                         |
> >    | :-----------------------------: | :----------------------------------------------------------- |
> >    |   左奇异矩阵$U_{m\text{×}m}$    | 为正交矩阵即$U_{m\text{×}m}U_{m\text{×}m}^{T}\text{=}I_{m\text{×}m}$ |
> >    | 奇异值矩阵$\Sigma_{m\text{×}n}$ | 为对角矩阵(对角为是奇异值)，如$\small\begin{bmatrix}\alpha_1 & 0 & 0 & \cdots & 0& \cdots & 0 \\0 & \alpha_2 & 0 & \cdots & 0& \cdots & 0 \\0 & 0 & \alpha_3 & \cdots & 0& \cdots & 0 \\\vdots & \vdots & \vdots & \ddots & \vdots & &\vdots\\0 & 0 & 0 & \cdots & \alpha_m& \cdots & 0 \\\end{bmatrix}_{m \text{×} n}$ |
> >    |   右奇异矩阵$V_{n\text{×}n}$    | 为正交矩阵即$V_{n\text{×}n}V_{n\text{×}n}^{T}\text{=}I_{n\text{×}n}$ |
> >
> > 2. $\text{Eckart–Young–Mirsky}$定理：$A_k=U_k \Sigma_k V_k^T$奇异值的截断
> >
> >    - $U_k$ 和 $V_k$ 分别是 $U$ 和 $V$ 的前 $k$ 列
> >    - $\Sigma_k$ 是奇异值矩阵 $\Sigma$ 中前 $k$ 个最大的奇异值组成的 $k\text{×}k$ 子矩阵
> >
> > :two:$\text{LSA}$模型步骤：原始$\text{Word-Doc}$矩阵$\xrightarrow[近似]{奇异分解}$其近似的低阶矩阵
> >
> > 1. $\text{Word-Doc}$矩阵：
> >
> >    - $A_{t \text{×} d} = \begin{bmatrix}
> >      \text{Doc}_1 \text{→} \text{Word}_{11} & \text{Doc}_2 \text{→} \text{Word}_{12} & \cdots & \text{Doc}_n \text{→} \text{Word}_{1d} \\
> >      \text{Doc}_1 \text{→} \text{Word}_{21} & \text{Doc}_2 \text{→} \text{Word}_{22} & \cdots & \text{Doc}_n \text{→} \text{Word}_{2d} \\
> >      \vdots & \vdots & \ddots & \vdots \\
> >      \text{Doc}_1 \text{→} \text{Word}_{t1} & \text{Doc}_2 \text{→} \text{Word}_{t2} & \cdots & \text{Doc}_n \text{→} \text{Word}_{td} \\
> >      \end{bmatrix}$
> >
> >    - $\text{Doc}_i\text{→Word}_{ij}$可为词$\text{Word}_{ij}$的词频或者$\text{TF-IDF}$值
> >
> > 2. $A_{t\text{×}d}$奇异分解：$A_{t\text{×}d}\text{=}T_{t\text{×}n}S_{n\text{×}n}D_{d\text{×}n}^{T}\text{}$ 
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109022707515.png" alt="image-20241109022707515" width=580 /> 
> >
> >    |     矩阵类型     | 描述                                           |
> >    | :--------------: | :--------------------------------------------- |
> >    | $S_{n\text{×}n}$ | 奇异值按降序排列，代表重要的==潜在语义的强度== |
> >    | $T_{t\text{×}n}$ | 词汇矩阵，每列蕴含一个隐含概念(主题)           |
> >    | $D_{d\text{×}n}$ | 文档矩阵，每列蕴含一个隐含概念(主题)           |
> >
> > 3. 低秩近似： $A\text{→}A_k$ 
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片tsrhy4.png" alt="图片dq121" width=580 />   
> >
> >    - 降维： $S_{n\text{×}n}\xrightarrow{只保留前k个最大的奇异值}S_{k\text{×}k}$，其中$k$又称为==预期主题数==
> >
> >    - 降噪：$A_{t\text{×}d}\text{=}T_{t\text{×}n}S_{n\text{×}n}D_{d\text{×}n}^{T}\text{}\xrightarrow{S_{n\text{×}n}降维}A_{t\text{×}d}\text{=}T_{t\text{×}k}S_{k\text{×}k}D_{d\text{×}k}^{T}\text{}$，滤掉不重要的主题
> >
> > :three:文档与词汇的表示
> >
> > 1. 词汇：$T_{t\text{×}k}S_{k\text{×}k}$的行向量，且$\hat{w}_n=u_n\text{×}\textbf{S}$
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片ssddddddddd8.png" alt="图dsdff片5"  width=500 />  
> >
> > 2. 文档：$D_{d\text{×}k}S_{k\text{×}k}$的行向量($S_{k\text{×}k}D_{d\text{×}k}^T$的列向量)，且$\hat{d}_m\text{=}\textbf{S}\text{×}v_{m}^{T}$ 
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片dfdfdfdfdw7.png" alt="图片sdsdsdsdsdsdsdssd6" width=500 />  
>
> ## $\textbf{1.2. MNF}$建模
>
> > :one:建模过程
> >
> > 1. 对$\textbf{V}$寻找非负矩阵$\textbf{HW}$使$\textbf{V}\text{≈}\textbf{WH}$
> >
> > 2. 使得代价函数$\displaystyle{}\|V-W H\|\text{=}\sqrt{\sum_{i, j}\left(V_{i, j}-(W H)_{i, j}\right)^2}$ 尽可能小
> >
> > :two:建模的意义
> >
> > 1. 非负：使分解结果更有意义
> > 2. 示例：**(文档-单词)**$\xrightarrow{\text{NMF}}$**(文档-主题)**$\textbf{×}$**(主题-单词)**

# $\textbf{2. }$基于概率的主题模型

> ## $\textbf{2.0. }$概率模型概述
>
> > :one:符号：其中$K$为话题数，$K\text{≪}M$且为预先定义的超参数
> >
> > |                集合                 | 含义                                      |   随机变量    |
> > | :---------------------------------: | :---------------------------------------- | :-----------: |
> > | 文本集$D\text{=}\{d_1,d_2,...d_N\}$ | 包含所有文本，$N$为文本总数               | $d$(观测变量) |
> > | 话题集$Z\text{=}\{z_1,z_2,...z_K\}$ | 包含所有可能的话题，$K$为==预设==话题总数 | $z$(隐藏变量) |
> > | 词汇集$W\text{=}\{w_1,w_2,...w_M\}$ | 所有可能的单词，$M$为单词总数             | $w$(观测变量) |
> >
> > :two:三类分布：$P(d)$为可观测参数，如何估计$P(z|d)$和$P(w|z)$两参数派生了$\text{pLAS}$和$\text{LDA}$方法
> >
> > |   分布   | 表示                   | 含义                                             |
> > | :------: | :--------------------- | :----------------------------------------------- |
> > | 文档分布 | $P(d)\sim{}$多项分布   | 生成文本$d$的概率                                |
> > | 主题分布 | $P(z|d)\sim{}$多项分布 | 文本$d$生成话题$z$的概率，每个文本都有其主题分布 |
> > | 单词分布 | $P(w|z)\sim{}$多项分布 | 话题$z$生成单词$w$的概率，每个主题都有其单词分布 |
> >
> > :three:观测表征
> >
> > 1. 观测数据：文本-单词共现矩阵，其中$n($单词$i,$ 文本$j)$表示单词$i$在文本$j$中出现的次数
> >
> >    | 共现矩阵$T$  |        文$d_1$        |        文$d_2$        | $\text{...}$ |        文$d_N$        |
> >    | :----------: | :-------------------: | :-------------------: | :----------: | :-------------------: |
> >    |   词$w_1$    | $n($词$w_1,$ 文$d_1)$ | $n($词$w_1,$ 文$d_2)$ | $\text{...}$ | $n($词$w_1,$ 文$d_N)$ |
> >    |   词$w_2$    | $n($词$w_2,$ 文$d_1)$ | $n($词$w_2,$ 文$d_2)$ | $\text{...}$ | $n($词$w_2,$ 文$d_N)$ |
> >    | $\text{...}$ |     $\text{...}$      |     $\text{...}$      | $\text{...}$ |                       |
> >    |   词$w_M$    | $n($词$w_M,$ 文$d_1)$ | $n($词$w_M,$ 文$d_2)$ | $\text{...}$ | $n($词$w_M,$ 文$d_N)$ |
> >
> > 2. 生成概率：假设每个单词分布独立，则有$\displaystyle{}P(T)\text{=}\prod_{(w, d)} P(w, d)^{n(w, d)}$ 
> >
> > :four:$\text{LDA}$与$\text{pLSA}$  
> >
> > |     模型      |  思想  | 对于两$P(z\mid{}d)$和$P(w\mid{}z)$待估参数                   |
> > | :-----------: | :----: | :----------------------------------------------------------- |
> > | $\text{pLSA}$ | 频率学 | 视作固定值(即均匀分布)，用最大似然估计解出来                 |
> > | $\text{LDA}$  | 贝叶斯 | 视作服从$\text{Dirichlet}$分布的随机变量，先验分布$\xrightarrow{修正}$最终分布 |
>
> ## $\textbf{2.1 pLSA}$模型
>
> > :one:生成模型：
> >
> > 1. 定义：对生成概率$\displaystyle{}P(w, d)\text{=}P(d) \sum_z P(z | d) P(w | z)$形式的拆解
> >
> > 2. 概率依赖：文本$\text{→}$话题$\text{→}$单词
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109184747859.png" alt="image-20241109170227089" width=300 />    
> >
> >    | 选择 | 描述                                                         | 备注               |
> >    | :--: | :----------------------------------------------------------- | :----------------- |
> >    | 文本 | 从$D$中，按$P(d)$选择文本$d$$\xrightarrow{重复N次}$生成$N$个文本 | $N$为文本数量      |
> >    | 话题 | 对每个文本，按$P(z|d)$选择话题$z$$\xrightarrow{重复L次}$生成$L$个话题 | $L$为文本(定/变)长 |
> >    | 单词 | 对每个话题，按$P(w|z)$选择一单词                             | $\text{N/A}$       |
> >
> > :two:共现模型：
> >
> > 1. 定义：对生成概率$\displaystyle{}P(w, d)\text{=}\sum_{z \text{∈} Z} P(z) P(w | z) P(d | z)$形式的拆解
> >
> > 2. 概率依赖：话题$\text{→}$单词，话题$\text{→}$文本
> >
> >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109184707676.png" alt="image-20241109184707676" width=270 /> 
> >
> >    | 选择 | 描述                                                         | 备注                |
> >    | :--: | :----------------------------------------------------------- | :------------------ |
> >    | 话题 | 从$Z$中，按$P(z)$选择话题$z$$\xrightarrow{重复L次}$生成$L$个话题 | $L$为文本(定/变)长  |
> >    | 单词 | 对每个话题，按$P(w|z)$选择一单词                             | 单词/文本的选择独立 |
> >    | 文本 | 从$D$中，按$P(d|z)$选择文本$d$$\xrightarrow{重复N次}$生成$N$个文本 | $N$为文本数量       |
>
> ## $\textbf{2.2. LDA}$模型简述
>
> > :anger:别看$\text{PPT}$了那就是一坨屎，以下内容来自维基百科
> >
> > :one:$\text{LDA}$模型要素 
> >
> > 1. 三种分布：
> >
> >    |        分布        |          维度          |                   元素                   | 隐藏/观测 |
> >    | :----------------: | :--------------------: | :--------------------------------------: | :-------: |
> >    |  主题分布$\Theta$  | 文档数$\text{×}$主题数 |  $\theta_{i,j}$为文档$i$中主题$j$的比例  |   隐藏    |
> >    |  词汇分布$\beta$   | 主题数$\text{×}$词汇数 |  $\beta_{i,j}$为主题$i$中词汇$j$的频次   |   隐藏    |
> >    | 主题分布$\text{w}$ | 文档数$\text{×}$词汇数 | $\text{w}_{i,j}$为文档$i$中词汇$j$的频次 |   观测    |
> >
> > 2. 两种超参数：
> >
> >    |  超参数  | 描述                                     | 功能                   |
> >    | :------: | :--------------------------------------- | :--------------------- |
> >    | $\alpha$ | 文档集级参数，$\text{Dirichlet}$分布参数 | 生成文档的主题$\Theta$ |
> >    |  $\eta$  | 文档集级参数，$\text{Dirichlet}$分布参数 | 生成每个主题的$\beta$  |
> >
> > :two:$\text{LDA}$的生成：分布$\displaystyle{}p\left(w_i, z_i, \theta_i, \Phi \mid \alpha, \beta\right)=\prod_{j=1}^N p\left(\theta_i \mid \alpha\right) p\left(z_{i, j} \mid \theta_i\right) p(\Phi \mid \beta) p\left(w_{i, j} \mid \phi_{z_{i, j}}\right)$ 
> >
> > <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241109234613737.png" alt="image-20241109234613737" width=400 /> 
> >
> > 1. 第一部分：
> >    - 从先验$\text{Dirichlet}$分布$\alpha$中抽样$\text{→}$生成某一文档$i$的主题(多项式)分布$\theta_i$ 
> >    - 从$\theta_i$分布中抽样$\text{→}$生成某一文档$i$的某一主题$z_{i,j}$
> > 2. 第二部分：
> >    - 从先验$\text{Dirichlet}$分布$\eta$中抽样$\text{→}$生成主题$z_{i,j}$的词语分布$\beta_{z_{i,j}}$
> >    - 从$\beta_{z_{i,j}}$分布中抽样$\text{→}$生成词语$w_{i,j}$
> >
> > :four:$\text{LDA}$的求解(训练)：<font color=red>我也不信考试会考这$\text{B}$玩意儿</font>
> >
> > 1. $\text{EM}$算法($\text{Old-Fashioned}$)
> > 2. $\text{Gibbs}$采，$\text{MCMC(Markov Chain Monte Carlo)}$算法
>
> ### $\textbf{3.2.3. }$番外: $\textbf{pLSA}$的$\textbf{EM}$求解
>
> > :zero:总体思路
> >
> > 1. 极大似然估计：找到时$P(T)$最大的参数
> > 2. $\text{EM}$算法：直接最大化对数似然函数非常困难，从而通过$\text{EM}$迭代的方式实现
> >
> > :one:极大似然函数
> >
> > 1. 似然函数推导
> >
> >    - 给定共现数据$\textbf{T}\text{=}\{n(w_i,d_j)\}\text{→}$要让$\displaystyle{}P(T)\text{=}\prod_{i,j}P(w_i,d_j)^{n(w_i,d_j)}$最大
> >
> >    - 取对数$+$引入隐含变量：
> >
> >      $\begin{flalign*}
> >      &\displaystyle{}\log{P(T)}\text{=}\sum_{i=1}^{M}\sum_{j=1}^{N}n(w_i,d_j)\text{×}\log{P(w_i,d_j)}&\\
> >      & \Bigg\Downarrow {\\引入隐含变量\text{: }\small\displaystyle{}p\left(d_j\right) \sum_z p\left(z_k | d_j\right) p\left(w_i | z_k\right)\\} &\\
> >      &\displaystyle{}\log{P(T)}\text{=}\sum_{i=1}^{M}\sum_{j=1}^{N}\left(n(w_i,d_j)\text{×}\left( \log{p(d_j)} \text{+} \log\left(\sum_z p(z_k | d_j) p(w_i | z_k)\right) \right)\right) &
> >      \end{flalign*}$ 
> >
> > 2. 似然函数分析：
> >
> >    - 已知值：$n(w_i,d_j)$在$\textbf{T}$向量中就有之，$p(d_j)$可由真实大量文本集得到
> >    - 参数值：$\displaystyle{}\log\left(\sum_z p(z_k | d_j) p(w_i | z_k)\right)$，其中$\displaystyle{}\log\sum$形式适合用$\text{EM}$算法解决
> >
> > :two:极大似然函数的下界
> >
> > 1. $\text{Jensen}$不等式
> >
> >    |      情况      | $E(f(x))$与$f(E(x))$   |
> >    | :------------: | :--------------------- |
> >    | $f(x)$为凸函数 | $E(f(x))\geq{}f(E(x))$ |
> >    | $f(x)$为凹函数 | $E(f(x))\leq{}f(E(x))$ |
> >    |  $x\text{=}C$  | $E(f(x))=f(E(x))$      |
> >
> > 2. $\displaystyle{}\log\left(\sum_z p(z_k | d_j) p(w_i | z_k)\right)$的处理：构建方差$+$应用$\text{Jensen}$不等式
> >
> >    - $\displaystyle{}\sum_z p(z_k | d_j) p(w_i | z_k)\xrightarrow{z的分布Q(z)}\sum_z {Q(z)}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\xrightarrow{\small{}X\text{=}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}}E(X)$ 
> >    - 原始$\text{=}\log(E(X))\xrightarrow[\text{Jensen不等式}]{\log(x)为凹函数}$$\text{≥}E(\log(X))\text{=}\displaystyle{}\sum_z\left(\log{}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\right)Q(z)$  
> >
> > 3. 下界与极大似然：提升下界$\displaystyle{}\sum_z\left(\log{}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\right)Q(z)$的最大值$\text{→}$最大化似然函数
> >
> > :three:$\text{EM}$算法：详细步骤就不写了，<font color=red>我不信考试会考这$\text{B}$玩意儿</font>
> >
> > 1. $\text{E}$步：确定$Q$函数$\text{→}$表示当前参数估计下==完全数据(观测数据$+$隐含变量)==的对数似然的期望
> >    - 此处$Q\text{=}Q(z)\text{=}p(z_k|w_i,d_j)$ 
> > 2. $\text{M}$步：迭代$Q$函数，不断更新参数$\to$使当前参数估计靠近最优值
> >    - 此处需要更新的参数为文档-主题分布$P(z|d)$，主题-词汇分布$P(w|z)$
> >    - 最终使$\displaystyle{}\sum_z\left(\log{}\cfrac{p(z_k | d_j) p(w_i | z_k)}{Q(z)}\right)Q(z)$最大，从而使$P(T)$最大