<img src="https://i-blog.csdnimg.cn/direct/6714da6e55a94e5caad154e67dae8455.png" alt="image-20241112204045174" width=600 /> 

[toc]

有关[$\text{Github}$仓库](https://github.com/DANNHIROAKI/XJTU-CS-Courses)，欢迎来$\text{Star}$ 

# $\textbf{1. }$概述: 分类定义$\textbf{\&}$评价指标

> :one:分类概念：
>
> 1. 定义：将数据分为不同类别的过程
> 2. 过程：学习已有数据的特征和标签$\text{→}$构建模型$\text{→}$将新数据分配到预定义类别中
>
> :two:评估方法
>
> |            方法             | 描述                                                         |
> | :-------------------------: | :------------------------------------------------------------ |
> |   $\text{Holdout Method}$   | 将数据集(随机)分为<span style="color:red;">训练集</span>/<span style="color:green;">测试集</span>分别<span style="color:red;">训练</span>/<span style="color:green;">测试</span> |
> | $\text{Random Subsampling}$ | 进行多轮$\text{Holdout Method}$，取多轮评估的平均作为最终结果 |
> |  $\text{Cross-Validation}$  | 将数据分为$k$个小块，让每一小块轮流作测试集(其余$k\text{-1}$块为训练集 |
>
> :three:评估指标
>
> 1. $\text{Confusion Matrix}$：
>
>    <img src="https://i-blog.csdnimg.cn/direct/7fad1a2c3e46443f83d9b597e0c3218e.png" alt="image-20241109152520072" width=199 /> 
>
>    |      **实际\预测**      |         **$\mathbf{C}_1$**         |      **$\neg \mathbf{C}_1$**       |
>    | :---------------------: | :--------------------------------: | :--------------------------------: |
>    |   **$\mathbf{C}_1$**    | $\text{True Positives(TP)}$真阳性  | $\text{False Negatives(FN)}$假阴性 |
>    | **$\neg \mathbf{C}_1$** | $\text{False Positives(FP)}$假阳性 | $\text{True Negatives(TN)}$真阴性  |
>
> 2. 准确率/误差率：
>
>    |       指标        |                计算                | 含义                 |
>    | :---------------: | :--------------------------------: | -------------------- |
>    | $\text{Accuracy}$ | $\cfrac{\text{TP+TN}}{\text{ALL}}$ | 预测==正确==的总占比 |
>    |  $\text{Error}$   |        $1-\text{Accuracy}$         | 预测==错误==的总占比 |
>
>
> 3. 敏感性/特异性：
>
>    - 敏感性/特异性含义：
>
>      |             指标             |               计算                | 含义                  |
>      | :--------------------------: | :-------------------------------: | --------------------- |
>      | $\text{Sensitivity(Recall)}$ | $\cfrac{\text{TP}}{\text{TP+FN}}$ | 检出$\text{TP}$的能力 |
>      |     $\text{Specificity}$     | $\cfrac{\text{TN}}{\text{TN+FP}}$ | 避免$\text{FP}$的能力 |
>
>    - $\text{ROC}$曲线：即$\text{Specificity}-\text{(1-Specificity)}$曲线，$\text{AUC}$值为其横轴积分
>
>      <img src="https://i-blog.csdnimg.cn/direct/4cb40a2ca425439ca1ac4f7708469ffa.png" alt="image-20241113221128573" width=250 />  
>
> 3. 召回/精度：   
>
>    |             指标             |               计算                | 含义                        |
>    | :--------------------------: | :-------------------------------: | --------------------------- |
>    | $\text{Recall(Sensitivity)}$ | $\cfrac{\text{TP}}{\text{TP+FN}}$ | 真实阳性中，$\text{TP}$比例 |
>    |      $\text{Precision}$      | $\cfrac{\text{TP}}{\text{TP+FP}}$ | 预测阳性中，$\text{TP}$比例 |
>
> 3. $\displaystyle\text {F}_{\beta}\text{-score}$：  
>
>    |                     指标                      |                             计算                             | 含义                                                         |
>    | :-------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
>    | $\displaystyle\text {F}_{\beta}\text{-score}$ | $(1\text{+}\beta^2) \text{×} \cfrac{\text{Precision} \text{×} \text {Recall }}{\beta{}\text{×}\text {Precision+} \text{Recall}}$ | <span style="color:red;">$\beta{}\text{<1}$</span>/<span style="color:green;">$\beta{}\text{>1}$</span>时<span style="color:red;">$\text{Preci.}$</span>/<span style="color:green;">$\text{Recall}$</span>影响更大 |
>    |   $\displaystyle\text {F}_1\text{-score }$    | $2 \text{×} \cfrac{\text {Precision} \text{×} \text {Recall}}{\text {Precision+} \text {Recall}}$ | 二者均等重要的调和平均                                       |

# $\textbf{2. }$决策树

> ## $\textbf{2.1. }$决策树概述
>
> > :one:决策树的结构：
> >
> > <img src="https://i-blog.csdnimg.cn/direct/4f1a5733426848ca8cfb97a4f6d286b6.png" alt="image-20241113231755874" width=350 />  
> >
> > 1. 对应关系：中间(根)结点$\xleftrightarrow{对应}$对某属性的测试，分支$\xleftrightarrow{对应}$属性值，叶结点$\xleftrightarrow{对应}$实例所属类别
> > 2. 分类流程：把实例从根结点一层层排列到叶子结点
> >
> > :two:决策树的分类策略
> >
> > 1. 属性的选择：基于启发式/统计测试，使得信息增益/增益率/$\text{Gini}$指标等度量值最好
> > 2. 终止划分条件：无样本剩下 $\text{or}$ 给定的某节点所有样本属于一类 $\text{or}$ 没有属性用于下一步划分
> >
> > :three:有关符号
> >
> > | 集合符号  | 集合样本数  | 含义                                                      |
> > | :-------: | :---------: | :-------------------------------------------------------- |
> > |   ${D}$   |    $|D|$    | 训练样本集，含$m$个类别$C_i\text{⊆}D(i\text{=}1\dots{}m)$ |
> > | $C_{i,D}$ | $|C_{i,D}|$ | $D$中$C_i$类样本的集合                                    |
>
> ## $\textbf{2.2. }$决策树的度量指标
>
> >:one:信息增益：
> >
> >1. 信息熵：划分后集合的混乱程度
> >
> >      - 集合：$D$包含了$n$个类别$C_i$(如下)
> >
> >        |   类别   |               $C_1$                |             **$C_2$**              |             **$C_3$**              | **......** |               $C_n$                |
> >        | :------: | :--------------------------------: | :--------------------------------: | :--------------------------------: | :--------: | :--------------------------------: |
> >        |  元素数  |             $N_{C_1}$              |             $N_{C_2}$              |             $N_{C_3}$              |   ......   |             $N_{C_n}$              |
> >        | 元素频率 | $P_1\text{=}\cfrac{N_{C_1}}{N_总}$ | $P_2\text{=}\cfrac{N_{C_2}}{N_总}$ | $P_3\text{=}\cfrac{N_{C_3}}{N_总}$ |   ......   | $P_n\text{=}\cfrac{N_{C_n}}{N_总}$ |
> >
> >       - 熵值：$\text{Info}(D)\text{=}\displaystyle{}-\sum_{i=1}^{n}P_i\log{P_i}$ (单位为$\text{bits}$)
> >
> >2. 条件熵：按属性划分的信息熵
> >     - 划分：$D\xrightarrow[划分为v个子集]{有v个值的属性A}\{D_1,D_2,\dots,D_v\}$，注意$D_i$由于其它属性存在$\text{→}$内部依然分类
> >      - 熵值：$D$关于属性$A$划分的熵为$\text{Info}_A(D)\text{=}\displaystyle{}\sum_{j=1}^{v}\cfrac{|D_j|}{|D|}\text{×}\text{Info}(D_j)$
> > 
> >3. 信息增益：
> >
> >     - 含义：==划分导致的不确定性降低程度==，即$\text{Gain=Info}(D)-\text{Info}_A(D)$ 
> > 
> >     - 意义：<span style="color:red;">选择有最大信息增益的属性来划分</span> 
> > 
> >       <img src="https://i-blog.csdnimg.cn/direct/992b741ee41b4950a9e92299b8f41c4e.png" alt="image-20241114011949346" width=530 />  
> > 
> >:two:信息增益率：
> >
> >1. 划分熵：$\displaystyle{}\text{SplitInfo}(A)\text{=}-\sum_{j=1}^m \cfrac{\left|D_j\right|}{|D|} \log _2\left(\frac{\left|D_j\right|}{|D|}\right)\text{→}$划分越均匀值越低$\xrightarrow{衡量}$划分有效$?$ 
> >
> >2. 信息增益率：$\text{GainRatio}(A)\text{=}\cfrac{\operatorname{Gain}(A)}{\operatorname{SplitInfo}(A)}\text{→}$避免信息增益的偏向性
> >3. 意义：<span style="color:red;">选择有最大信息增益率的属性来划分</span> 
> >
> >:three:$\text{Gini}$指数
> >
> >1. 数据集的$\text{Gini}$指数：
> >
> >     - 定义：$\displaystyle{}\text{Gini}(D)\text{=}1-\sum_{j=1}^n p_j^2$，其中$p_j$为类别$j$样本数占总样本数比例
> > 
> >      - 含义：衡量样本分布的均匀程度$\text{→}\begin{cases}\text{Gini}值越大\text{→}样本越分散\text{→}纯度低\\\\
> >         \text{Gini}值越小\text{→}样本越集中\text{→}纯度高
> >         \end{cases}$
> > 
> >        <img src="https://i-blog.csdnimg.cn/direct/de228ac9bbec4d5f9bedcc4aa6d6815b.png" alt="image-20241114023252163" width=330 />  
> > 
> >2. 基于属性$A$分裂的$\text{Gini}$指数
> >     - 定义：对于$D\xrightarrow{属性A(含有v个值)}\{D_1,D_2,...,D_v\}$有$\displaystyle{}\text{Gini}_A(D)\text{=}\sum_{i=1}^{v}\cfrac{\left|D_i\right|}{|D|} \text{Gini}\left(D_i\right)$ 
> >     - 含义：衡量$D$经过$A$分裂后的整体不纯度
> > 
> > 3. 基于属性$A$分裂的不纯度减少
> >     - 定义：$\Delta{\text{Gini}}(A)\text{=}\text{Gini}(D)-\text{Gini}_A(D)$ 
> >     - 意义：<span style="color:red;">选择能够使$\Delta{\text{Gini}(A)}$最大$/\Delta{\text{Gini}_A}$最小最小的属性进行结点分裂</span> 
> >
> > :upside_down_face:度量的对比和总结
> > 
> >|       指标        | 分裂属性选择                                                 | 分裂过程倾向于             |
> >| :---------------: | ------------------------------------------------------------ | -------------------------- |
> >|     信息增益      | 使信息增益$\text{Gain}(A)$最大的属性                         | 多值属性                   |
> >|    信息增益率     | 使信息增益率$\text{GainR.}(A)$最大的属性                     | 不平衡分裂(某些子集极小)   |
> >| $\text{Gini}$指数 | 使不纯度减少$\Delta{\text{Gini}}(A)$最大==(纯度增加)==的属性 | 多值属性$\&$分裂后纯度提高 |
> 
> ## $\textbf{2.3. }$一些经典的决策树算法
> 
>> :one:$\text{ID3}$(迭代二分$3$)
> >
>> 1. 思想：默认属性值离散$\text{→}$结点分裂时(遍历每个特征的信息增益)选择==信息增益最大==的特征
> > 2. 特点：倾向于选择多指特征(不合理)，对噪声敏感，方法简单应用广
> >
> > :two:$\text{C4.5}$算法：基于$\text{ID3}$的改进
> >
> > 1. 离散属性选择：直接选择结点分裂时==信息增益<span style="color:red;">率</span>==最大的特征$\text{→}$克服了对值属性的倾向性
> >
> > 2. 连续属性离散化：
> >
> >    - 排序：对连续属性$A$，其在$D$中取有$m$个离散属性值，排序后得到$\left\{a_1, a_2, \ldots, a_m\right\}$ 
> >
> >    - 分割：对$\left\{a_1, a_2, \ldots, a_m\right\}$有$m\text{-}1$种方式一分为二，选择==信息增益率最大的==分割以进行
> >
> >      <img src="https://i-blog.csdnimg.cn/direct/9c7fd55aafae4ee2a4ac790155c71474.png" alt="image-20241114163536502" width=300 />  
> >
> >    - 递归：可对划分得子集$\left\{a_1, a_2, \ldots, a_k\right\}$或$\left\{a_{k+1}, a_{k+2}, \ldots, a_m\right\}$进一步按此划分
> >
> >    - 终止：直到达到某阈值，$\left\{a_1, a_2, \ldots, a_m\right\}$被分为互不交叉的$K$块(转化为$K$个离散值)
> >
> > 3. 对缺失数据的处理：
> >
> >    - 含义：数据的某个属性的值会缺失，需要根据已知值来估计(以填充)
> >    - 策略：($\text{Quinlan}$)计算每个属性$a_i$值出现的概率$P(a_i)\text{→}$为缺失值$e_i$赋予概率分布$P(a_i)$ 
> >
> > 4. 生成规则：
> >
> >    - 逻辑：将根$\text{→}$叶路径转化为如下$\text{IF-THEN}$规则，由此决策树也变为$\text{IF-THEN}$规则集合
> >
> >      ```txt
> >      IF [中间节点所有条件] THEN [根节点类别]
> >      ```
> >
> >    - 存储：规则(路径)会被存储在数组中，每行对应每个路径
> >
> > :three:$\text{CART}$算法：采用二元化分的二叉决策树
> >
> > 1. 划分方式：
> >    - 离散属性：计算每种二元分裂的$\text{Gini}$指数，选择$\text{Gini}$最小(纯度最高)的方式分裂
> >    - 连续属性：排序后考虑每个分割点，选择使得$\text{Gini}$最小的分割点进行分割
> > 2. 递归划分：在划分得到的子集上递归地进行下一次划分$\text{→}$直到达到终止条件
> > 3. 终止条件：
> >    - 叶结点：样本数为$1$，或者小于某给定值$N_{\min}$
> >    - 属性：结点中样本同属一类，即无更多属性可供分裂
> >    - 高度：决策树高度达到用户预设
> 
> ## $\textbf{2.4. }$ 决策树的其它有关内容
> 
>> :one:决策树的过拟合
> >
>> 1. 欠拟合与过拟合：
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/89966019ada8417c8b5f4dce197b8c9d.png" alt="image-20241114180902178" width=450 /> 
> >
> >    |  类型  | 模型表现        | 成因                                                 |
> >    | :----: | :-------------- | :--------------------------------------------------- |
> >    | 欠拟合 | 训练集❌/测试集❌ | 模型过于简单                                         |
> >    | 过拟合 | 训练集✅/测试集❌ | 决策树分支过多$\text{→}$对噪声的过分拟合，训练集太小 |
> >
> > 2. 欠拟合的解决：剪枝
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/7df083b597bc493bbaa427af44b09f99.png" alt=" " width=300 /> 
> >
> >    |  方式  |   时机   | 操作                                 | 剪枝标准                            |
> >    | :----: | :------: | :----------------------------------- | :---------------------------------- |
> >    | 预剪枝 | 树构建时 | 使结点终止分裂                       | 结点实例少于某值/分裂不使纯度提高时 |
> >    | 后剪枝 | 树构建后 | 中间节点$\xrightarrow{替换为}$叶节点 | 剪枝后能否降低错误率                |
> >
> > :two:集成学习
> >
> > 1. 含义：单个机器学习(弱学习算法)$\xrightarrow[整合]{某种规则}$集成学习(强学习算法)，由此获得更好的效果  
> >
> > 2. $\text{Bagging}$：并行逻辑
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/27805d9e5fff4dfbbeea68d141b55c40.png" alt="werhgjhgdhtgerwr" width=450 /> 
> >
> >    - 流程：产生$S$个训练集$\text{→}$训练$S$个分类器$\text{→}$预测时票选出$S$个预测结果中出现最多的
> >
> > 3. $\text{AdaBoost}$：串行逻辑
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/7c04aefd6b6e4042831f09f39297caab.png" alt="image-20241114204954384" width=550 />  
> >
> >    |   操作   | 含义                                                         |
> >    | :------: | :----------------------------------------------------------- |
> >    |  初始化  | 对于含$N$个样本的数据集，赋予每个样本相同的权值$\cfrac{1}{N}$ |
> >    |   训练   | 使用当前权重训练基学习器，并计算其在训练集上的误差           |
> >    |  更新权  | 降低被正确分类的样本的权值，提升被错误分类样本的权值         |
> >    | 线性组合 | 让误差大的基学习器比例大，误差小的比例小，进行线性组合       |
> >
> > :three:梯度与梯度提升树：样本对损失函数的负梯度$\xrightarrow{当作/估计}$该样本残差
> >
> > 1. 梯度提升：对样本$\begin{bmatrix}(x_1,y_1)\\(x_2,y_2)\\...\\(x_n,y_n)\end{bmatrix}$拟合(回归)，使损失函数$J\text{=}\displaystyle{}\sum_{i}L(F(x_i),y_i)$最小
> >
> >    - 拟合：对当下的拟合$\begin{bmatrix}F(x_1)\\F(x_2)\\...\\F(x_n)\end{bmatrix}$有残差$\begin{bmatrix}h(x_1)\text{=}y_1\text{-}F(x_1)\\h(x_2)\text{=}y_2\text{-}F(x_2)\\......\\h(x_n)\text{=}y_n\text{-}F(x_n)\end{bmatrix}$ 需调整$ch$来最小化$J$
> >    - 梯度：$\small\cfrac{\partial J}{\partial F(x_i)}\text{=}F(x_i)-y_i\text{ ←}
> >      \begin{cases}
> >      \displaystyle{}L(y_i, F(x_i))\text{=}\cfrac{(y-F(x))^\alpha}{\alpha}(以其为例, 可选其它)\\\\
> >      \displaystyle{}\cfrac{\partial J}{\partial F(x_i)}=\cfrac{\displaystyle{}\partial \sum_i L(y_i, F(x_i))}{\partial F(x_i)}=\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}
> >      \end{cases}$
> >    - 下降：$\small{}F(x_i)\text{:=}F(x_i)\text{+}h(x_i)\xrightarrow{\displaystyle{}h(x_i)=y_i-F(x_i)=\frac{\partial J}{\partial F(x_i)}}F(x_i)-\cfrac{\partial J}{\partial F(x_i)}$ 
> >
> > 2. 梯度提升树$\text{(GBDT)}$：
> >
> >    - 原理概述：由多个决策树串联的集成学习，可描述为$\displaystyle{} F(x) \text{=} F_0(x) + \sum_{m=1}^M \gamma_m h_m(x)$
> >
> >      |    参数    | 含义                                                         |
> >      | :--------: | :----------------------------------------------------------- |
> >      |   $F(x)$   | 用于预测$y$的预测模型                                        |
> >      |  $F_0(x)$  | 初始模型，一般初始化为$\cfrac{1}{n}\displaystyle{}\sum_{i=1}^n{}y_i$ |
> >      |  $h_m(x)$  | 第$m$个弱学习器(一般为决策树)                                |
> >      | $\gamma_m$ | 第$m$个决策树的步长稀疏/学习率，控制每棵树对总模型的贡献     |
> >
> >    - 核心步骤：
> >
> >      |  大步  |     小步     | 操作                                                         |
> >      | :----: | :----------: | :----------------------------------------------------------- |
> >      | 初始化 | $\text{N/A}$ | 设定$F_0(x)$为所有目标值的平均，例如$F_0(x)\text{=}\cfrac{1}{n}\displaystyle{}\sum_{i=1}^n{}y_i$ |
> >      | 迭代树 |   计算残差   | 用$\displaystyle{}r_{x_i,m} \text{=} \cfrac{-\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}$当作当前$F_{m-1}(x_i)$与$y_i$残差 |
> >      | 迭代树 |   拟合残差   | 让$h_m(x_i)$尽可能接近$r_{x_i,m}\text{→}$让当前$h_m(x_i)$学习上一步误差 |
> >      | 迭代树 |   更新模型   | $F_m(x) \text{=} F_{m-1}(x) \text{+} \gamma_m h_m(x)$        |
> >      |  终止  | $\text{N/A}$ | 达到最大迭代次数$M/$总体损失函数小于某值$\text{→}$停止迭代   |

# $\textbf{3. }$贝叶斯分类器

> :one:$\text{Bayes}$定律
>
> 1. 推导：
>
>    - $P\left(B_i| A\right)\text{=}\cfrac{P\left(B_i\right) P\left(A|B_i\right)}{P(A)}\Leftarrow\begin{cases}P(A|B_i)\text{=}\cfrac{P(AB_i)}{P(B_i)}\\\\P(B_i|A)\text{=}\cfrac{P(AB_i)}{P(A)}\end{cases}$ 
>    - $P(B_i|A)\text{=}\cfrac{P(B_i)P(A|B_i)}{\displaystyle\sum_{j}P(A|B_j)P(B_j)}\Leftarrow{}{P(A)}\text{=}\displaystyle\sum_{j}P(AB_j)\text{=}\sum_{j}P(A|B_j)P(B_j)$ 
>
> 2. 解释：
>
>    |    参数    | 含义                                     | 示例                         |
>    | :--------: | :--------------------------------------- | :--------------------------- |
>    |  $P(B_i)$  | 先验概率，指对事件$B_i$发生的主观臆测    | 某种疾病在人群中的发病率     |
>    | $P(B_i|A)$ | 后验概率，指观察到$A$发生后$B_i$发生概率 | 对个体实施检测后个体患病概率 |
>
> :two:$\text{Bayes}$定律与分词(断句)：令$X$为待分词句子，$Y\text{=}\{W_1,W_2,...,W_n\}$为一种可能的分词结果
>
> 1. $\text{Bayes}$分解：$P(Y | X) \text{∝} P(Y)\text{×}P(X|Y)\xrightarrow[认为P(X|Y)\text{=}1]{基于该句子某种断句\text{→}必定生成该句子}P(Y | X) \text{∝} P(Y)$  
>
> 2. 联合概率展开：
>
>    - $P(Y)\text{=}P(W_1)$
>
>      ```txt
>      W1 →
>      ```
>
>    - $P(Y)\text{=}P(W_1)\text{×}P(W_2|W1)$
>
>      ```txt
>      W1 W2 →
>      ```
>
>    - $P(Y)\text{=}P(W_1)\text{×}P(W_2|W1)\text{×}P(W_3|W2,W_1)$
>
>      ```txt
>      W1 W2 W3 →
>      ```
>
>    - $P(Y)\text{=}P(W_1)\text{×}P(W_2|W1)\text{×}P(W_3|W2,W_1)\text{×}\cdots\text{×}P\left(W_n| W_1, W_2, \ldots, W_{n-1}\right)$ 
>
>      ```txt
>      W1 W2 W3 ... Wn.
>      ```
>
> 3. $k$阶马可夫假设：
>
>    - 含义：认为某个单词只由其前$k$个单词确定，以解决稀疏性问题
>    - 简化：$P(Y)\text{=}P(W_1)\text{×}P(W_2|W_1)\text{×}P(W_3|W_2)\text{×}\cdots\text{×}P\left(W_n|W_{n-1}\right)$  
>
> :three:朴素贝叶斯分类器
>
> 1. 模型描述：
>
>    - 假设：决定各分类的属性之间是相互独立(简单粗暴/但也损失了分类精度)
>    - 前提：样本$X(a_1,a_2,...,a_n)$有$n$个属性$\{A_1,A_2,...,A_n\}$且有$m$个类$\{C_1,C_2,...,C_m\}$
>
>    - 分类：将$X(a_1,a_2,...,a_n)$归类为$C_i\xLeftrightarrow{等价}P\left(C_i | X\right)\text{>}P\left(C_j|X\right)$，其中$C_j$为除$C_i$任一类
>
> 2. 模型分析：
>
>    - $P\left(C_i|X\right)\text{=}\cfrac{P\left(X|C_i\right) P\left(C_i\right)}{P(X)}\xRightarrow{P(X)为常数}$最大化$P\left(X|C_i\right) P\left(C_i\right)$以分类
>    - $P(X|C_i)P(C_i)\text{=}P(A_1\text{=}a_1,A_2\text{=}a_2,\cdots,A_n\text{=}a_n|C_i)P(C_i)\xRightarrow{各属性独立}\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)$ 
>
> 3. 模型流程：
>
>    | 阶段 | 操作                                                         |
>    | :--: | :----------------------------------------------------------- |
>    | 准备 | 确定样本的属性，获取相应的样本                               |
>    | 训练 | 对每个类别计算$P(C_i)\text{→}$对每个属性计算$P(a_k|C_i)$     |
>    | 应用 | 对新样本$\Lambda$计算其对每个类别的$P(\Lambda{}|C_i)P(C_i)\text{→}$找到使之最大的$C_i$以归类之 |
>
> 4. $\text{Underflow}$问题：
>
>    - 问题：多概率相乘$\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)$很可能会快速变为$0$
>    - 解决：取对数$\text{→}\log{\left(\displaystyle{}\prod_{k=1}^nP(a_k|C_i)P(C_i)\right)}$ 
>
> :four:示例：判断学历为大学，年薪$\text{30-40}$，薪水$\text{20000-30000}$的员工的性别
>
> ​    $\begin{array}{|cccccc|}
> \hline \text { 样本 } & \text { 性别 } & \text { 工作内容 } & \text { 学历 } & \text { 年龄 } & \text { 薪水 } \\
> \hline 1 & \text { 女 } & \text { 送货 } & \text { 大学 } & 20-30 & 20000-30000 \\
> \hline 2 & \text { 男 } & \text { 包装 } & \text { 大学 } & >40 & >40000 \\
> \hline 3 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & 30-40 & 20000-30000 \\
> \hline 4 & \text { 男 } & \text { 包装 } & \text { 高中 } & 30-40 & 20000-30000 \\
> \hline 5 & \text { 男 } & \text { 送华 } & \text { 大学 } & >40 & 30000-40000 \\
> \hline 6 & \text { 女 } & \text { 烘烤 } & \text { 高中 } & 20-30 & 20000-30000 \\
> \hline 7 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & 20-30 & <20000 \\
> \hline 8 & \text { 女 } & \text { 包装 } & \text { 大学 } & 30-40 & 20000-30000 \\
> \hline 9 & \text { 男 } & \text { 烘烤 } & \text { 大学 } & >40 & 20000-30000 \\
> \hline 10 & \text { 男 } & \text { 包装 } & \text { 大学 } & 20-30 & <20000 \\
> \hline
> \end{array}$  
>
> 1. $\begin{cases}P(包装|女)\text{×}P(大学|女)\text{×}P(30\text{-}40|女)\text{×} P(20000\text{-}30000|女)\text{×}P(女)\text{=}0.0222\\\\P(包装|男)\text{×}P(大学|男)\text{×}P(30\text{-}40|男)\text{×} P(20000\text{-}30000|男)\text{×}P(男)\text{=}0.0315\end{cases}$
> 2. 故根据给定条件，应归类为男性

# $\textbf{4. KNN}$算法

> :one:算法概念与思想
>
> 1. 基本思想：若与$x$最邻近的$k$个样本$\{x_1,x_2,...,x_n\}$大都属于$A$类别$\text{→}x_i$也属于$A$类别
> 2. 算法流程：选定$k$值，计算输入$x$与样本所有点的距离$\text{dist}(x,x_i)$
>    - 分类任务：对$k$个最邻近进行投票(如$x_i$属于$v$就为$v$类投$1$票)，选出数量最多的类作为$x$的类
>    - 回归任务：对$k$个最邻近取平均，作为$x$的相应值
> 3. 算法特点：为$\text{Lazy-Learning}$方法，即无需训练模型，待分类数据到达时马上开始分类
>
> :two:算法有关的问题
>
> 1. 关于$k$及其选择：
>    - 影响：$k$太小容易对噪声敏感(过拟合)，太大可能会包含太多其它类别的点
>    - 选择：$k$一定是奇数(避免平局)，另外可通过测试确定$k$(选择$k\text{=1,3,5...}$时使错误率最小的$k$)
> 2. $\text{Majority Voting}$问题
>    - 含义：某一类别占比太大，以至分类时总会选择到它
>    - 解决：加权投票，如$x_i$属于𝑣就为为$v$类投$\cfrac{1}{\text{dist}(x,x_i)^2}$票$\text{→}$使靠$x$更近的结点话语权更大