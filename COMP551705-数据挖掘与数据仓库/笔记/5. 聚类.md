<img src="https://i-blog.csdnimg.cn/direct/6714da6e55a94e5caad154e67dae8455.png" alt="image-20241112204045174" width=600 /> 

[toc]

有关[$\text{Github}$仓库](https://github.com/DANNHIROAKI/XJTU-CS-Courses)，欢迎来$\text{Star}$ 

# $\textbf{1. }$聚类概念及概述

> :one:聚类的概念
>
> 1. 目标：
>
>    - 把数据划为多个子集，相同子集内元素尽可能相似，不同子集尽可能相异\
>
>    - 因此聚类有赖于相似度计算$+$聚类算法(且前者更重要些)
>
> 2. 性质：无监督学习，源于分簇数目不定$+$没有分类标签
>
> :two:类别：硬聚类(一个点只能属于一个簇)，软聚类(一个点可属于多个簇)

# $\textbf{2. }$数据类型$\textbf{\&}$距离计算

> :one:区间标度变量(数值变量)：变量标度为线性
>
> 1. 含义：线形标度的**连续**度量(如温度/焓/年份)，差值有意义但零值无意义
>
> 2. 标准化：$x_i\xrightarrow[\displaystyle{}平均值\text{: }m\text{=}\cfrac{1}{n} \sum_{i\text{=}1}^n x_{i}]{\displaystyle{}平均偏差\text{: }s\text{=}\cfrac{1}{n} \sum_{i\text{=}1}^n|x_{i}-m|}z_i\text{=}\cfrac{x_i-m}{s}$，注意平均偏差比标准差更鲁棒
>
> 3. 距离计算：$\text{Minkowski}$距离$\text{dist}(i,j)\text{=}\sqrt[\alpha]{\displaystyle{}\sum_{d=1}^{p}|z_{i_d}-z_{j_d}|^{\alpha}}\to\begin{cases}\alpha{}\text{=1}为\text{Manhatta}距离\\\\\alpha{}\text{=2}为\text{Euclidean}距离\end{cases}$ 
>
>    :heavy_plus_sign:比例标度型：变量的标度为非线性(如遵守$Ae^{Bt}$)，取对数后即变为线性，处理方式不变
>
> :two:布尔变量：只有$0/1$两种值
>
> 1. 布尔型列联表：对于对象$i$与对象$j$
>
>    | $i/j$ |                     $1$                     |                     $0$                     |
>    | :---: | :-----------------------------------------: | :-----------------------------------------: |
>    |  $1$  | $a$ (对象$i$值为$1/$对象$j$值为$1$的属性数) | $b$ (对象$i$值为$1/$对象$j$值为$0$的属性数) |
>    |  $0$  | $c$ (对象$i$值为$0/$对象$j$值为$1$的属性数) | $d$ (对象$i$值为$0/$对象$j$值为$0$的属性数) |
>
> 2. 距离计算：
>
>    |     系数类型     |                             计算                             | 含义                                  | 适用属性 |
>    | :--------------: | :----------------------------------------------------------: | :------------------------------------ | :------: |
>    |     简单匹配     | $d(i, j)\text{=}\cfrac{b\text{+}c}{a\text{+}b\text{+}c\text{+}d}$ | 所有属性中，两对象属性相异的比例      |   对称   |
>    | $\text{Jaccard}$ |   $d(i, j)\text{=}\cfrac{b\text{+}c}{a\text{+}b\text{+}c}$   | 排除全$0$属性后，两对象属性相异的比例 |  非对称  |
>
> 3. 距离计算示例：
>
>    | $\text{Name}$ | $\text{Gender}$ | $\text{$\text{F}$ever}$ |  $\text{Cough}$  | $\text{Test-1}$ | $\text{Test-2}$  | $\text{Test-3}$  | $\text{Test-4}$  |
>    | :-----------: | :-------------: | :---------------------: | :--------------: | :-------------: | :--------------: | :--------------: | :--------------: |
>    | $\text{Jack}$ | $\text{M(N/A)}$ |     $\text{Y(=1)}$      | $$\text{N(=0)}$$ | $\text{P(=1)}$  | $$\text{N(=0)}$$ | $$\text{N(=0)}$$ | $$\text{N(=0)}$$ |
>    | $\text{Mary}$ | $\text{F(N/A)}$ |     $\text{Y(=1)}$      | $$\text{N(=0)}$$ | $\text{P(=1)}$  | $$\text{N(=0)}$$ |  $\text{P(=1)}$  | $$\text{N(=0)}$$ |
>
>    - 属性：性别为对称属性将其忽略，其余为不对称属性(将有设为$1/$没有设为$2$)
>    - 系数：$a\text{=2}/b\text{=1/}c\text{=0}/d\text{=3}$故$\text{Jaccard}$系数为$\cfrac{1\text{+}0}{\text{2+1+0}}$ 
>
> :three:枚举(标称)变量：
>
> 1. 含义：二元变量推广为多元变量，如$\text{Color}$可取<span style="color:red;">$\text{R}$</span>/<span style="color:orange;">$\text{Y}$</span>/<span style="color:blue;">$\text{B}$</span>三种状态
>
> 2. 距离计算：
>    - 简单匹配：令$m$为对象$ij$匹配(相同)属性数，$p$为属性总数(维度)，则距离为$d(i, j)\text{=}\cfrac{p-m}{p}$ 
>    - 转化法：将枚举变成布尔，如$\text{Color}$(<span style="color:red;">$\text{R}$</span>/<span style="color:orange;">$\text{Y}$</span>/<span style="color:blue;">$\text{B}$</span>)$\xrightarrow{每个状态设一个二元变量}$是否为(<span style="color:red;">红</span>/<span style="color:orange;">黄</span>/<span style="color:blue;">蓝</span>)六种状态
>
> :four:序数型
>
> 1. 含义：
>
>    |    类型    | 含义                                         |   示例   |
>    | :--------: | :------------------------------------------- | :------: |
>    | 离散序数型 | 特殊的枚举型，只不过每个状态的排序是有意义的 |   军衔   |
>    | 连续序数型 | 类似于连续变量，但其无单位                   | 豆瓣评分 |
>
> 2. 距离计算：连续序数型与连续型基本一致，以下为离散序数型的
>
>    - 秩概念：对于$x_i$值域$\text{\{Value 1,Value 2,...,Value }M\}$，令其秩$r_i$值域为$\{1,2,...,M\}$ 
>    - 秩变换：认为$x_i\text{=Value }n\xLeftrightarrow{等价于}r_i\text{=}n$ 
>    - 归一化：$z_i\text{=}\cfrac{r_i-1}{M-1}$
>    - 距离计算：$\text{dist}(i,j)\text{=}\sqrt[\alpha]{\displaystyle{}\sum_{d=1}^{p}|z_{i_d}-z_{j_d}|^{\alpha}}$ 
>
> :five:混合计算：以下表为例
>
> | 对象 | 布尔型$1$ | 布尔型$2$ | 布尔型$3$ |   连续型$A$    |   连续型$B$    | 序数型$\Delta$(共$10$类) |
> | :--: | :-------: | :-------: | :-------: | :------------: | :------------: | :----------------------: |
> | $i$  |    $0$    |    $1$    |    $1$    | $0.12$(归一化) | $0.45$(归一化) |         第$2$类          |
> | $j$  |    $1$    |    $1$    |    $0$    | $0.54$(归一化) | $0.19$(归一化) |         第$5$类          |
>
> 1. 距离计算：
>    - 布尔型：按照规则$d_{ij}^{(f)} \text{=} \begin{cases} 
>      0,  \text{若 } x_{if} = x_{jf} \\ 
>      1,  \text{若 } x_{if} \neq x_{jf} 
>      \end{cases}$，令$d_{ij}^{(1)} \text{=} 1/d_{ij}^{(2)} \text{=} 0/d_{ij}^{(3)} \text{=} 1$ 
>    - 连续型：按照规则$d_{ij}^{(f)}\text{=} \cfrac{|x_{if} - x_{jf}|}{\max_h x_{hf} - \min_h x_{hf}}$，令$\begin{cases}d_{ij}^{(A)} \text{=} |0.12 - 0.54|\\\\d_{ij}^{(B)}\text{=}|0.45 - 0.19|\end{cases}$ 
>    - 序数型：获得归一化的秩$\begin{cases}z_{i, \text{序数型}} \text{=} \cfrac{2 - 1}{10 - 1}\\\\z_{j, \text{序数型}} \text{=} \cfrac{5 - 1}{10 - 1}\end{cases}$后按规则$d_{ij}^{(\Delta)} \text{=} |z_{i, \text{序数型}} - z_{j, \text{序数型}}|$得到距离
> 2. 合并距离：每个属性的子距离总和$/$属性数量

# $\textbf{3. }$聚类的方法

> |   方法   | 概述                                                         | 特点                         |
> | :------: | ------------------------------------------------------------ | ---------------------------- |
> | 划分方法 | 将数据集划为$k$个子集，对应$k$个簇                           | 适合中小数据库的球状聚类     |
> | 层次方法 | <span style="color:red;">自上而下</span>/<span style="color:green;">自下而上</span>地<span style="color:red;">拆分</span>/<span style="color:green;">组合</span>数据集，来得到簇 | 适合发现嵌套关系             |
> | 基于密度 | 用数据点密度定义簇(高密度为簇/低密度为簇边界)                | 可过滤噪声与$\text{Outlier}$ |
>
> ## $\textbf{3.1.}$ 划分聚类方法
>
> > :one:$k\text{-Means}$算法
> >
> > 1. 算法流程：
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/d6a8ba2c5b90475c9661f7c748ee9c3c.png" alt="image-20241115220949925" width=350 /> 
> >
> > 2. 终止条件：
> >
> >    - 指标：每个顶点到各自簇中心的距离平方的和，即$\displaystyle{}E\text{=}\sum_{i=1}^k \sum_{p \in C_i}\left\|p-m_i\right\|^2$
> >    - 标准：两次迭代之间，$E$的变化小于某个阈值
> >
> > 3. 算法分析：
> >
> >    - 复杂度：$O(nk*\text{iters})$
> >    - 缺点：常限于局部最优，$k$难以确定，簇的均值必须有定义，对噪声/离群点敏感
> >
> > :two:$k\text{-Medoids}$算法：以$\text{PAM}$算法为例
> >
> > 1. 算法流程：注意其中总代价$S$表示所有点到各自$\text{Medoids}$的距离之和
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/0323900fe341489c9f2a28c6ddb8e38b.png" alt="绘图2EHAHATAR1" width=500 />   
> >
> > 2. 算法分析：复杂度为$O(k(n-k)^2)$，较$k\text{-Means}$更鲁棒
>
> ## $\textbf{3.2.}$ 层次聚类方法
>
> > :one:簇间距离的度量：令$\text{Dist = }$簇$C_i$中任一点$\xleftrightarrow{距离}$簇$C_j$中任一点
> >
> > |            方式             | 簇$C_i/C_j$距离                  | 特点                          |
> > | :-------------------------: | -------------------------------- | ----------------------------- |
> > |   $\text{Single Linkage}$   | 为$\text{Dist}$最小值(最近连接)  | ==能处理非球聚类==/对噪声敏感 |
> > |  $\text{Complete Linkage}$  | 为$\text{Dist}$最大值(最远连接)  | 倾向打破大的簇                |
> > | $\text{Centroids Distance}$ | 为二者质心间距离(中心连接)       | 适合处理均匀分布的数据        |
> > |   $\text{Group Average}$    | 为二者间所有点距的平均(平均连接) | 均衡了最近/最远点             |
> >
> > :two:聚类树($\text{Dendrogram}$)：$\text{AGNES}$(自下而上)/$\text{DIANA}$(自上而下)算法
> >
> > <img src="https://i-blog.csdnimg.cn/direct/a768817fd4b84f23940fce43b908b83c.png" alt="image-20241116003914534" width=500 /> 
> >
> > 1. $\text{AGNES}$算法：自下而上
> >    - 初始化：视每点为一个单独簇
> >    - 合并簇：计算每两个簇间的距离$\text{→}$合并距离最近的两个簇
> >    - 终止：重复合并簇步骤，直到所有的样本最终被合并到一簇
> > 2. $\text{DIANA}$算法：自下而上
> >    - 初始化：视整个数据集为一个大簇
> >    - 簇划分：选定簇中与其他点距离最大的点(新簇种子)$\text{→}$将原簇其它点重新分配到原簇/新簇
> >    - 终止：重复簇划分步骤，直到每个点成为一个独立的簇
> >
> > :three:层次聚类的缺点与改进
> >
> > 1. 缺点：事件复杂度为$O(n^2)$，扩展性差，已做出的划分不可分割
> > 2. 改进：$\text{BIRCH/ROCK/CURE....}$ 
>
> ## $\textbf{3.3.}$ 密度聚类方法: 以$\textbf{DBScan}$为例
>
> > :one:基本要素
> >
> > 1. 密度定义有关参数：
> >
> >    |        参数         | 含义                                                         |
> >    | :-----------------: | ------------------------------------------------------------ |
> >    | $\Large\varepsilon$ | 邻域半径，用于确定某点的邻域范围(即$\varepsilon\text{-}$邻域) |
> >    |   $\text{MinPts}$   | 邻域内最小点数，用于判断邻域高密度与否                       |
> >
> > 2. 对点的划分：
> >
> >    <img src="https://i-blog.csdnimg.cn/direct/fe36b1ae483f435b99aeaa5bc0d2b7cc.png" alt="image-20241116012041497" width=250 /> 
> >
> >    | 点类型 | 描述                                                      |
> >    | :----: | --------------------------------------------------------- |
> >    | 核心点 | 邻域内点数大于$\text{MinPts}$(包括了自己哦)               |
> >    | 边界点 | 邻域内点数小于$\text{MinPts}$，但其位于某个核心点的邻域内 |
> >    | 噪声点 | 完全不搭边的点                                            |
> >
> > 3. 一些定义：
> >
> >    |            定义            | 描述                                                         |  特性  |
> >    | :------------------------: | ------------------------------------------------------------ | :----: |
> >    | $p\xrightarrow{密度直达}q$ | $p$为核心点，且$p$的$\varepsilon\text{-}$邻域中有$q$         | 不对称 |
> >    | $p\xrightarrow{密度可达}q$ | 对核心点链$\{p_1\text{(=}p),p_2,p_3,...,p_n\text{(=}q)\}$有$p_i\xrightarrow{密度直达}p_{i+1}$ | 不对称 |
> >    | $p\xrightarrow{密度相连}q$ | 存在点$o$使得$p\xrightarrow{密度可达}o\xleftarrow{密度可达}q$ |  对称  |
> >
> > :two:算法流程
> >
> > <img src="https://i-blog.csdnimg.cn/direct/79bd220d2c2f46649b8ff8852fe12637.png" alt="image-20241116021825058" width=500 /> 
> >
> > 1. 初始化：输入数据集$D$，超参数$\varepsilon/\text{MinPts}$ 
> >
> > 2. 主循环：遍历$D$中每个点，对未放问点$p\text{∈}D$执行以下操作
> >
> >    - 若$p$的邻域内稠密(点数$\text{≥MinPts}$)：标记被扩展的$p/q_j$为已访问
> >
> >      |   操作    | 描述                                                         |
> >      | :-------: | ------------------------------------------------------------ |
> >      |  扩展$p$  | 建立一个新簇$C\text{→}$将$p$及其邻居$\{q_1,q_2,...q_m\}$放入$C$簇 |
> >      | 扩展$q_j$ | 遍历邻居$\{q_1,q_2,...q_m\}\text{→}$若$q_j$为核心点$\text{→}$将$q_j$邻居中未访问点再加入$C$ |
> >
> >    - 若$p_i$的邻域内稀疏(点数$\text{<MinPts}$)：记$p$为噪声$\text{→}$标记$p$为已访问
> >
> > 3. 终止：运行主循环一直到所有样本被分簇/归类为噪声，最终输出若干簇$C_i$和噪声集$O$ 
> >
> > :three:算法示例：$\varepsilon\text{=2}/\text{MinPts=3}$ 
> >
> >    $\begin{array}{|c|c|c|c|c|c|c|c|c|}
> > \hline
> >           & (1, 2) & (1, 3) & (3, 1) & (2, 2) & (9, 8) & (8, 9) & (9, 9) & (18, 18) \\[-5pt]
> > \hline
> > (1, 2)     & \textcolor{red}{0.0} & \textcolor{red}{1.0} & 2.2 & \textcolor{red}{1.0} & 10.0 & 9.9 & 10.6 & 23.3 \\[-5pt]
> > \hline
> > (1, 3)     & \textcolor{red}{1.0} & \textcolor{red}{0.0} & 2.8 & \textcolor{red}{1.4} & 9.4 & 9.2 & 10.0 & 22.7 \\[-5pt]
> > \hline
> > (3, 1)     & 2.2 & 2.8 & \textcolor{red}{0.0} & \textcolor{red}{1.4} & 9.2 & 9.4 & 10.0 & 22.7 \\[-5pt]
> > \hline
> > (2, 2)     & \textcolor{red}{1.0} & \textcolor{red}{1.4} & \textcolor{red}{1.4} & \textcolor{red}{0.0} & 9.2 & 9.2 & 9.9 & 22.6 \\[-5pt]
> > \hline
> > (9, 8)     & 10.0 & 9.4 & 9.2 & 9.2 & \textcolor{red}{0.0} & \textcolor{red}{1.4} & \textcolor{red}{1.0} & 13.5 \\[-5pt]
> > \hline
> > (8, 9)     & 9.9 & 9.2 & 9.4 & 9.2 & \textcolor{red}{1.4} & \textcolor{red}{0.0} & \textcolor{red}{1.0} & 13.5 \\[-5pt]
> > \hline
> > (9, 9)     & 10.6 & 10.0 & 10.0 & 9.9 & \textcolor{red}{1.0} & \textcolor{red}{1.0} & \textcolor{red}{0.0} & 12.7 \\[-5pt]
> > \hline
> > (18, 18)   & 23.3 & 22.7 & 22.7 & 22.6 & 13.5 & 13.5 & 12.7 & \textcolor{red}{0.0} \\[-5pt]
> > \hline
> > \end{array}$  
> >
> > 1. 考察$(1,2)$：是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},(1,3),(2,2)\}$ 
> >    - 考察$(1,3)$：是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},\textcolor{orange}{(1,3)},(2,2)\}$ 
> >    - 考察$(2,2)$：是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},\textcolor{orange}{(1,3)},\textcolor{orange}{(2,2)},(3,1)\}$ 
> >      - 考察$(3,1)$：不是核心点，簇$C_1\text{=}\{\textcolor{orange}{(1,2)},\textcolor{orange}{(1,3)},\textcolor{orange}{(2,2)},\textcolor{orange}{(3,1)}\}$ 
> > 2. 考察$(9,8)$：是核心点，簇$C_2\text{=}\{\textcolor{orange}{(9,8)},(8,9),(9,9)\}$ 
> >    - 考察$(8,9)$：是核心点，簇$C_2\text{=}\{\textcolor{orange}{(9,8)},\textcolor{orange}{(8,9)},(9,9)\}$ 
> >    - 考察$(9,9)$：是核心点，簇$C_2\text{=}\{\textcolor{orange}{(9,8)},\textcolor{orange}{(8,9)},\textcolor{orange}{(9,9)}\}$ 
> > 3. 考察$(18,18)$：不是核心点，直接丢到噪声集$O\text{=}\{(18,18)\}$ 

# $\textbf{4. }$彩蛋: 网页数据挖掘之$\textbf{PageRank}$算法

> :one:关于$\text{Web}$图
>
> 1. $\text{Web}$图结构：每个结点代表一个$\text{URL}$，$A\xrightarrow{权值}B$表示$A$有一定(权值)的概率访问$B$ 
>
>    <img src="https://i-blog.csdnimg.cn/direct/c18fd7f4b6c744ee81e9e0307fad2685.png" alt="image-20241117154518762" width=300 />  
>
> 2. $\text{Web}$图假设：
>
>    - 数量假设：一个结点入度越高，则该网页越重要
>    - 质量假设：越是质量高的页面指向$A$，$A$越重要
>
> :two:算法描述：
>
> 1. 数学模型：$\displaystyle{}\text{PR}(a)_{i+1}\text{=}\cfrac{1{-}\alpha}{n}\text{+}\alpha\sum_{i=0}^n \cfrac{\text{PR}(\mathrm{Ti})_i}{\text{L(Ti)}}$，其中$\alpha$为阻尼因子
>
>    |            参数            | 含义                                 |
>    | :------------------------: | :----------------------------------- |
>    |    $\text{PR}(a)_{i+1}$    | 当前节点$a$的$\text{PR}$值           |
>    | $\text{PR}(\mathrm{Ti})_i$ | 指向节点$a$的各个结点的$\text{PR}$值 |
>    |       $\text{L(Ti)}$       | 指向节点$a$的各个结点的出度          |
>
> 2. 算法流程：
>
>    - 初始化：将所有结点$\text{PR}$值都设为某一值(通常为$\cfrac{1}{n}$)
>    - 主循环：按照$\displaystyle{}\text{PR}(a)_{i+1}\text{=}\cfrac{1{-}\alpha}{n}\text{+}\alpha\sum_{i=0}^n \cfrac{\text{PR}(\mathrm{Ti})_i}{\text{L(Ti)}}$ 更新每个网页的$\text{PR}$值
>    - 终止：一直迭代到$\text{PR}$值得不变
>
> 3. 矩阵化：
>
>    - 输入：有向图(结点个数为$n$)，初始向量$\textbf{R}_{0(n×1)}$，转移矩阵$\textbf{M}_{(n×n)}$，阻尼因子$\alpha$
>    - 迭代：$\textbf{R}_{t+1}\text{=}\alpha\textbf{MR}_t\text{+}\cfrac{1-\alpha}{n}$，直到$\textbf{R}_t$变化不大
>
> :three:示例：令$\alpha\text{=}1$，很简单只展示第一轮迭代
>
>   <img src="https://i-blog.csdnimg.cn/direct/71a2147eb28846ce90f9150d97ced50d.png" alt="在这里插入图片描述" width=200 /> $\xRightarrow{}\begin{array}{|c|c|c|c|}
> \hline \text { Iter } & \text { PR(B) } & \text { PR(B) } & \text { PR(C) } \\
> \hline \text { 初始化 } & \cfrac{1}{4} & \cfrac{1}{4} & \cfrac{1}{4} \\
> \hline \text { 第一轮 } & \cfrac{3}{8} & \cfrac{1}{8} & \cfrac{3}{8} \\
> \hline
> \end{array}$ 