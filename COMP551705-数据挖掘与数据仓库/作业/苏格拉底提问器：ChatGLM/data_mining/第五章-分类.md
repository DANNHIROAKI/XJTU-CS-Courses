# 数据挖掘

# 第五章：分类

###### 范 铭

###### 智能网络与网络安全教育部重点实验室

###### 西安交通大学网络空间安全学院

1. (^1) 分类的定义与基本问题
2. (^2) 分类的性能评测
3. (^3) 决策树
4. 4
5. 5
   贝叶斯分类
6. (^6) 其他学习问题
   k-nearest neighbor(k-NN)算法

###### ：掌握分类的定义、模型、评价指标等问

###### 题，掌握决策树、贝叶斯分类中的典型分类方法

###### 5. 1 分类的定义与基本问题

 分类：

```
 预测类标签
 基于训练数据和类标签构造一个模型，并分类新数据
```

 数值预测（回归）：建连续函数/模型，预测未知/缺失值

 典型应用：

```
 信用卡/贷款审批:
 医疗诊断:肿瘤是癌或良性？
 欺诈检测: 交易欺诈?
 网页分类
```

###### 5. 1 分类的定义与基本问题

 监督：训练数据(观察，测量等)都带有标签，指示数据的
类别

 根据训练集分类新数据

 训练集的类别(标签)未知

 给定一个观察，测量等的集合，目标是建立数据的类或
簇

###### 5. 1 分类的定义与基本问题

 分类：一个两步的过程

```
 模型构建:
```

- 假定每个元组/样本 属于一个类, 由类标签属性设定
- 用于构建模型的元组集合称为训练集training set
- 模型可以表示为分类规则,决策树, 数学公式
   模型使用: 分类将来/未知对象
- 估计模型的准确率：比较测试样本的已知标签/由模型
  预测(得到)标签

```
I.I.D假设。Data are independent and
identically distributed。词袋模型与股票收益率
```

###### 5. 1 分类的定义与基本问题

```
training
data
Classification
algorithm
Classifier
(model)
if age < 31 or Car Type
=Sports
then Risk = High
学习阶段：利用分类算法训练数据集来构造分类模型
```

**Age Car Type Risk**
20 Combi High
18 Sports High
40 Sports High
50 Family Low
35 Minivan Low
30 Combi High
32 Family Low
40 Combi Low

###### 5. 1 分类的定义与基本问题

测试阶段：利用检验数据评估分类模型的准确率，

```
如果准确率可以接受，则可用于新数据的分类
Test data
Classifier
(model)
Age Car Type Risk
27 Sports High
34 Family Low
66 Family High
44 Sports High
Risk
High
Low
Low
High
```

###### 5. 1 分类的定义与基本问题

```
New data
Classifier
(model)
分类阶段：利用模型预测新数据的类标签
```

**Age Car Type Risk**
27 Sports
34 Minivan
55 Family
34 Sports

```
Risk
High
Low
Low
High
```

###### 5. 1 分类的定义与基本问题

######  应用举例 1 ：文本分类

```
 BOW模型
```

###### 5. 1 分类的定义与基本问题

######  应用举例 2 ：语义关系识别

###### 5. 1 分类的定义与基本问题

######  应用举例 3 ：学习依赖关系

```
具有独
立知识表达的最小
知识对象
```

###### 5. 1 分类的定义与基本问题

######  应用举例 4 ：重打包程序检测

###### 5. 1 分类的定义与基本问题

1. (^1) 分类的定义与基本问题
2. (^2) 分类的性能评测
3. (^3) 决策树
4. 4
5. 5
   贝叶斯分类
6. (^6) 其他学习问题
   k-nearest neighbor(k-NN)算法

###### 5. 2 分类的性能评测

######  Holdout method, random subsampling

######  交叉验证 Cross-validation

######  自助法Bootstrap

###### 5. 2 分类的性能评测

 Holdout method

 给定数据随机分成两个部分

- 训练集 (e.g., 2 / 3 ) 用于模型构造, 测试集 (e.g., 1 / 3 ) 用于正确率估计
   Divide D into D 1 and D 2
   Use D 1 to construct the classifier d
   Then estimate R(d,D 2 ) to calculate the estimated misclassification error
  of d
- Unbiased and efficient, but removes D 2 from training dataset D

 随机抽样: a variation of holdout

- 重复holdout k次, accuracy = 所有正确率的平均值

###### 5. 2 分类的性能评测

 随机分割数据为 k 互不相交的子集, 每一个大小近似相等

 在i-th 迭代中, 使用Di 为测试集其他的为训练集

 步骤：

1. Construct classifier d from D
2. Partition D into V datasets D 1 , ..., DV
3. Construct classifier di using D \ Di
4. Calculate the estimated misclassification error R(di,Di) of di using
   test sample Di
5. Final misclassification estimate: Weighted combination of
   individual misclassification errors

###### 5. 2 分类的性能评测

```
d
d 1
d 2
d 3
```

###### 5. 2 分类的性能评测

```
Actual class\Predicted
class
C 1 ¬ C 1
C 1 True Positives (TP) False Negatives (FN)
¬ C 1 False Positives (FP) True Negatives (TN)
```

Actual class\Predicted
class

```
buy_computer
= yes
buy_computer
= no Total
buy_computer = yes 6954 46 7000
buy_computer = no 412 2588 3000
Total 7366 2634 10000
 感兴趣的类定为“正类” ，对应的为“负类”
 正样本/负样本
```

###### 5. 2 分类的性能评测

```
Actual class\Predicted
class
C 1 ¬ C 1
C 1 True Positives (TP) False Negatives (FN)
¬ C 1 False Positives (FP) True Negatives (TN)
```

 默认假设FP与FN的代价是相同
的
 疾病诊断、机场安检
https://www.answerminer.com/blog/confusion-matrix-explained

###### 5. 2 分类的性能评测

```
A\P C ¬C
C TP FN P
¬C FP TN N
P’ N’ All
```

######  准确度，误差率

```
 分类器准确度, or 识别率: 测
试元组被正确识别的比例
Accuracy = (TP + TN)/All
 误差率:
1 – accuracy, or (FP + FN)/All
```

###### 5. 2 分类的性能评测

######  准确度，误差率

```
 Class Imbalance Problem
类
分布不平衡问题:
One class may be rare, e.g.
fraud, or HIV-positive
```

###### 5. 2 分类的性能评测

 灵敏性:检测出TP的能力；

```
Sensitivity= TP/TP+FN
```

 特异性: 避免FP的能力

```
Specificity= TN/TN+FP
A\P C ¬C
C TP FN P
¬C FP TN N
P’ N’ All
```

###### 5. 2 分类的性能评测

例子来源：http://hi.baidu.com/jqcat/item/ 77 f 50 ef 5400 d 8 e 17 cf 9 f 325 f

```
假如我们有一个由 25 个男性和 75 个女性组成的人群，其中有
些人由于长相或打扮等原因使得其性别判断不太容易，特别
是对于只有 3 岁的小孩子Jack来说。Jack正确地识别出 25 个男
性中的 20 个，但是他也把 10 个女性误判为男性，那么他在性
别判断方面的能力该如何评估呢？
```

###### 5. 2 分类的性能评测

例子来源：
http://hi.baidu.com/jqcat/item/ 77 f 50 ef 5400 d 8 e 17 cf 9 f 325 f

```
A\P C ¬C
C 20 5 25
¬C 10 65 75
P’ N’ All
Sensitivity =？
Specificity=？
```

###### 5. 2 分类的性能评测

例子来源：
http://hi.baidu.com/jqcat/item/ 77 f 50 ef 5400 d 8 e 17 cf 9 f 325 f

```
Sensitivity =？
Specificity=？
A\P C ¬C
C^25025
¬C^07575
P’ N’ All
```

###### 5. 2 分类的性能评测

例子来源：
http://hi.baidu.com/jqcat/item/ 77 f 50 ef 5400 d 8 e 17 cf 9 f 325 f

```
Sensitivity =？ 把所有结果都当作男性
Specificity=？
A\P C ¬C
C^25025
¬C^75075
P’ N’ All
```

###### 5. 2 分类的性能评测

例子来源：
http://hi.baidu.com/jqcat/item/ 77 f 50 ef 5400 d 8 e 17 cf 9 f 325 f

```
Sensitivity =？ 把所有结果都当作女性
Specificity=？
A\P C ¬C
C^02525
¬C^07575
P’ N’ All
```

###### 5. 2 分类的性能评测

 Precision:精度– 被分类器标记为正
类的样本中实际上属于“正类”的比例

 Recall:召回率– what % of positive
tuples did the classifier label as
positive?

 信息检索

 精度和召回率逆关系

 F measure (F 1 or F-score):精度和
召回的调和平均值

###### 5. 2 分类的性能评测

 Fβ :用于度量精确度和召回率需要不同情况下的F-

```
score
```

 β> 0 度量了查全率对查准率的相对重要性；β= 0 时退

```
化为标准的查准率; β> 1 时查全率有更大影响 ; β< 1
时查准率有更大影响.
```

 商品推荐、恶意软件检测

###### 5. 2 分类的性能评测

真实类\预测类 c=a nyceesr^ canncoer^ =^ Total Reco(%gn)ition

```
cancer = yes 90 210 300 30. 00
cancer = no 140 9560 9700 98. 56
Total 230 9770 10000
Precision =? Recall =? Accuracy=?
```

###### 5. 2 分类的性能评测

Precision (A)=? Recall (A) =?
Accuracy=?
https://www.overfit.cn/post/d 03 f 3 e 77 c 727446 f 9 f 1716737 f 3 b 6 f 91

###### 5. 2 分类的性能评测

 ROC曲线：接收者操作特征(Receiver Operating Characteristic)曲线
，反映敏感性和特异性连续变量的综合指标

 AUC (Area Under Curve) ：ROC曲线下的面积

```
From Darlene Goldstein’s PPT
```

###### 5. 2 分类的性能评测

```
From Darlene Goldstein’s PPT
```

1. (^1) 分类的定义与基本问题
2. (^2) 分类的性能评测
3. (^3) 决策树
4. 4
5. 5
   贝叶斯分类
6. (^6) 其他学习问题
   k-nearest neighbor(k-NN)算法

###### 5. 3 决策树

```
通过把实例从根节点排列到叶子节点来分类
 叶子节点即为实例所属的分类
 树中节点是对某个属性的测试
 每一个后继分支对应于该属性的
一个可能值。
age?
overcast
student? credit rating?
```

<= (^30) > 40
no yes yes
yes
31 .. 40
no
no yes excellent fair
age income student 信誉^ 购算买机计
<= 30 high no fair no
<= 30 high no excellent no
31 ... 40 high no fair yes

> 40 medium no fair yes
> 40 low yes fair yes
> 40 low yes excellent no
> 31 ... 40 low yes excellent yes
> <= 30 medium no fair no
> <= 30 low yes fair yes
> 40 medium yes fair yes
> <= 30 medium yes excellent yes
> 31 ... 40 medium no excellent yes
> 31 ... 40 high yes fair yes
> 40 medium no excellent no

###### 5. 3 决策树

```
通过把实例从根节点排列到叶子节点来分类
```

 叶子节点即为实例所属的分类
 树中节点是对某个属性的测试
 每一个后继分支对应于该属性的一个可能值。

###### 5. 3 决策树

通过把实例从根节点排列到叶子节点来分类实例

###### 5. 3 决策树

 自顶向下、递归、分治的构建方式

- 开始，所有的训练样本位于根节点
- 属性是分类属性(若是连续值,事先离散化)
- 基于选择的属性，样本被递归地分割
- 基于启发式/统计测试来选择属性 (例如 信息增益)

```
 一个给定节点的所有样本属于一个类别
 没有属性剩下用于进一步划分 – 运用多数投票来标记此
节点
 没有样本剩下
```

###### 5. 3 决策树

Make Tree (Training Data D)
{
Partition(D)
}
Partition(Data S)
{
if (all points in S are in the same class) then
return
for each attribute A do
evaluate splits on attribute A;
use best split found to partition S into S 1 and S 2
Partition(S 1 )
Partition(S 2 )
}

###### 5. 3 决策树

 分裂规则，决定给定节点上的样本如何分裂

 选定具有最好度量得分的属性

 信息增益、增益率、Gini指标

```
 D为训练样本集，样本属于m个不同的类Ci(i= 1 ..m)
 Ci,D是D中的Ci类的样本集合
 |Ci,D|和|D|分别表示各自的样本个数
```

###### 5. 3 决策树-信息增益

 熵(Entropy) ：信息论中广泛使用的一个度量标准，表示

```
系统的不确定性或随机性
```

 令 pi 为D中的任一元组属于类 Ci概率为 |Ci, D|/|D|

 分类D中样本的熵

 对于二元分类：给定包含正反样例的样例集D ，那么D相对
这个布尔型分类的熵为：

##### Info(D) = - p+log 2 p+ - p-log 2 p-

其中p+是在D中正例的比例，p-是在D中负例的比例。规定 0 log 0 = 0 。

```
Entropy = ( 1 / 6 )log( 1 / 6 )-( 5 / 6 )log( 5 / 6 )= 0. 650
Entropy = ( 3 / 6 )log( 3 / 6 )-( 3 / 6 )log( 3 / 6 )= 1
Entropy = - ( 0 / 6 )log( 0 / 6 )-( 6 / 6 )log( 6 / 6 )= 0
```

###### 5. 3 决策树-信息增益

```
C 1 0
```

**C**^2 6

```
C 1 1
```

**C**^2 5

```
C 1 3
```

**C**^2 3

 一个有v个值的属性A可将D分成v个子集{D 1 ,D 2 ,...,Dv}，A划
分的熵为

 以属性A划分得到的信息增益

 具有高信息增益的属性是给定集合中具有高区分度的属性；

```
通过计算D中样本的每个属性的信息增益，来得到一个属性
的相关性的排序。
```

###### 5. 3 决策树-信息增益

```
( ) || || ( )
1 j
v
j
A j Info D
D
Info D  D 
```

##### 

```
Gain(A)  Info(D)  InfoA(D)
```

age income student credit_rating buys_computer
youth high no fair no
youth high no excellent no
middle_aged high no fair yes
senior medium no fair yes
senior low yes fair yes
senior low yes excellent no
middle_aged low yes excellent yes
youth medium no fair no
youth low yes fair yes
senior medium yes fair yes
youth medium yes excellent yes
middle_aged medium no excellent yes
middle_aged high yes fair yes
senior medium no excellent no

 计算每个属性的熵，首先是Age

```
 for age=‘<= 30 ’ : s 11 = 2 s 21 = 3 I(s 11 , s 21 ) =
0. 971
 for age=’ 31 .. 40 ’: s 12 = 4 s 22 = 0 I(s 12 , s 22 ) =
0
 for age=‘> 40 ’: s 13 = 2 s 23 = 3 I(s 13 , s 23 ) =
0. 971
```

 Class P: 买电脑 = “yes” , Class N: 买电脑 = “no”

###### 5. 3 决策树-信息增益

```
Info ( D )  194 log 2 ( 194 ) 154 log 2 ( 154 ) 0. 940
Gain ( age ) Info ( D ) Infoage ( D ) 0. 246
( _ ) 0. 048
( ) 0. 151
( ) 0. 029



Gain credit rating
Gain student
Gain income
```

###### 下一步？

###### 5. 3 决策树-信息增益

###### 5. 3 决策树-信息增益

```
.
.
.
.
.
.
Data Set:
A set of classified objects
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益

# Shape

Color Outline Dot
1 green dashed no triange
2 green dashed yes triange
3 yellow dashed no square
4 red dashed no square
5 red solid no square
6 red solid yes triange
7 green solid no square
8 green dashed no triange
9 yellow solid yes square
10 red solid no square
11 green solid yes square
12 yellow dashed yes square
13 yellow solid no square
14 red dashed yes triange

```
Attribute
From a PPT of Blaž Zupan and Ivan Bratko
```

- 5 triangles
- 9 squares
- class probabilities
- entropy

```
.
```

.

```
.
.
.
.
```

###### 5. 3 决策树-信息增益

```
.
```

.

..

```
.
.
```

..

```
.
.
.
.
Color?
red
yellow
green
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益

```
From a PPT of Blaž Zupan and Ivan Bratko
.
```

.

..

```
.
.
```

..

```
.
.
.
.
Color?
red
yellow
green
```

###### 5. 3 决策树-信息增益

```
.
```

.

..

```
.
.
```

..

```
.
.
.
.
Color?
red
yellow
green
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益

- Attributes
  - Gain(Color) = 0. 246
  - Gain(Outline) = 0. 151
  - Gain(Dot) = 0. 048
- Heuristics: attribute with the highest gain is chosen

```
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益

```
.
```

.

```
.
.
.
.
```

..

```
.
.
.
.
Color?
red
yellow
green
Gain(Outline) = 0. 971 – 0 = 0. 971 bits
Gain(Dot) = 0. 971 – 0. 951 = 0. 020 bits
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益

```
From a PPT of Blaž Zupan and Ivan Bratko
.
```

.

..

```
.
```

...

```
.
.
.
.
Color?
red
yellow
green
.
.
Outline?
dashed
solid
Gain(Outline) = 0. 971 – 0. 951 = 0. 020 bits
Gain(Dot) = 0. 971 – 0 = 0. 971 bits
```

###### 5. 3 决策树-信息增益

```
From a PPT of Blaž Zupan and Ivan Bratko
.
```

.

..

```
.
```

...

```
.
.
.
.
Color?
red
yellow
green
.
.
Outline?
dashed
solid
Dot?
no
yes
.
.
```

###### 5. 3 决策树-信息增益

```
.
```

.

..

```
.
.
Color
Dot square Outline
red yellow green
triangle square
yes no
triangle square
dashed solid
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益

 信息增益倾向于选择多值属性（为什么？）

###### Gain ratio

```
gain_ratio(income) = 0. 029 / 1. 557 = 0. 019 gain_ratio(age)=^ ？
```

###### 5. 3 决策树-信息增益率

Outlook Temperature Humidity Windy Play?
sunny hot high false No
sunny hot high true No
overcast hot high false Yes
rain mild high false Yes
rain cool normal false Yes
rain cool normal true No
overcast cool normal true Yes
sunny mild high false No
sunny cool normal false Yes
rain mild normal false Yes
sunny mild normal true Yes
overcast mild high true Yes
overcast hot normal false Yes
rain mild high true No

###### 5. 3 决策树-信息增益率

###### 5. 3 决策树-信息增益率

Split
info(“Outlook”)=info( 5 , 4 , 5 )= 1. 577

###### 5. 3 决策树-信息增益率

```
gain("Outlook")info([ 9 , 5 ])-info([ 2 , 3 ],[ 4 , 0 ],[ 3 , 2 ])  0. 940 - 0. 693
 0. 247 bits
```

###### 5. 3 决策树-信息增益率

计算Huminity与Windy的增益率？

###### 5. 3 决策树-信息增益率

###### 5. 3 决策树-信息增益率

From a PPT of Blaž Zupan and Ivan Bratko

```
.
.
```

..

```
.
.
```

..

```
.
.
.
.
Color?
red
yellow
green
```

###### 5. 3 决策树-信息增益率

**A |v(A)| Gain(A) GainRatio(A)**
Color 3 0. 247 0. 156
Outline 2 0. 152 0. 152
Dot 2 0. 048 0. 049

```
From a PPT of Blaž Zupan and Ivan Bratko
```

###### 5. 3 决策树-信息增益率

###### 5. 3 决策树-Gini index

数据 D 包含 n 类别的样本, Gini index定义为

pj 类别 j 在 D 中的频率

数据集 D 基于属性A 分裂为 D 1 和 D 2 , Gini index定义为

```
 不纯度减少:
 具有最小giniA(D) 的属性(or不纯度减少最大的) 用
于分裂节点 (需要枚举所有可能的分裂情况)
( )
| |
( ) | |
| |
( ) | |
```

(^11) *D* (^2) *gini D* 2
*D
D gini D
gini D D
A*  

#####  gini ( A ) gini ( D ) giniA ( D )

###### 5. 3 决策树-Gini index

**Age RID Risk**
17 1 high
20 5 high
23 0 high
32 4 low
43 2 high
68 3 low

```
Attribute list for ‘Age’
Attribute list for ‘Car Type’
Car Type RID Risk
family 0 high
sport 1 high
sport 2 high
family 3 low
truck 4 low
family 5 high
```

###### 5. 3 决策树-Gini index

按照Age属性进行划分，可能的划分点包括：
Age 17 , Age 20 , Age 23 , Age 32 , Age 43 ,
Age 68

#### Tuple count High Low

#### Age<= 17 1 0

#### Age> 17 3 2

```
G(Age<= 17 ) = 1 - ( 12 + 02 ) = 0
G(Age> 17 ) = 1 - (( 3 / 5 )^2 +( 2 / 5 )^2 ) = 1 - 13 / 25 =
12 / 25
GSPLIT = ( 1 / 6 ) * 0 + ( 5 / 6 ) * ( 12 / 25 ) = 2 / 5
```

###### 5. 3 决策树-Gini index

```
Tuple count High Low
Age<= 20 2 0
Age> 20 2 2
```

**Tuple count High Low**
Age<= 23 3 0
Age> 23 1 2

```
G(Age 23 ) = 1 - ( 12 + 02 ) = 0
G(Age> 23 ) = 1 - (( 1 / 3 )^2 +( 2 / 3 )^2 )
= 1 - ( 1 / 9 ) - ( 4 / 9 ) = 4 / 9
GSPLIT = ( 3 / 6 ) * 0 + ( 3 / 6 ) * ( 4 / 9 ) = 2 / 9
G(Age<= 20 ) = 1 - ( 12 + 02 ) = 0
G(Age> 20 ) = 1 - (( 1 / 2 )^2 +( 1 / 2 )^2 ) = 1 / 2
GSPLIT = ( 2 / 6 ) * 0 + ( 4 / 6 ) * ( 1 / 2 ) = 1 / 3
```

###### 5. 3 决策树-Gini index

#### Tuple count High Low

#### Age<= 32 3 1

#### Age> 32 1 1

```
G(Age 32 ) = 1 - (( 3 / 4 )^2 +( 1 / 4 )^2 )
= 1 - ( 10 / 16 ) = 6 / 16 = 3 / 8
G(Age> 32 ) = 1 - (( 1 / 2 )^2 +( 1 / 2 )^2 ) = 1 / 2
GSPLIT = ( 4 / 6 )*( 3 / 8 ) + ( 2 / 6 )*( 1 / 2 )
```

= ( 1 / 8 ) + ( 1 / 6 )= 14 / 48 = 7 / 24

###### 最小的 GSPLIT 是按 Age 23 对数据集划分

###### 5. 3 决策树-Gini index

```
Age 23
Risk = High Risk = Low
Age> 23
```

###### 经过第一次划分形成的决策树

###### 5. 3 决策树-Gini index

```
Attribute lists for Age 23 :
```

**Age RID Risk
17 1 high
20 5 high
23 0 high**

```
Car Type RID Risk
family 0 high
sport 1 high
family 5 high
```

**Age RID Risk
32 4 low
43 2 high
68 3 low**

```
Car Type RID Risk
sport 2 high
family 3 low
truck 4 low
Attribute lists for Age> 23
```

###### 5. 3 决策树-Gini index

######  对类别性属性划分的评测：如果类别性属性取

##### 值有n个，则需要对 2 n种组合的每个中都要测试

(^) **Tuple count High Low**

###### Car type= {sport} 1 0

###### Car type ={family] 0 1

###### Car type = {truck} 0 1

```
G(Car type {sport}) = 1 – 12 – 02 = 0
G(Car type {family}) = 1 – 02 – 12 = 0
G(Car type {truck}) = 1 – 02 – 12 = 0
```

###### 5. 3 决策树-Gini index

```
Car Type RID Risk
sport 2 high
family 3 low
truck 4 low
```

G(Car type  { sport, family }) = 1 - ( 1 / 2 )^2 - ( 1 / 2 )^2 = 1 / 2
G(Car type  { sport, truck }) = 1 / 2
G(Car type  { family, truck }) = 1 - 02 - 12 = 0

GSPLIT(Car type  { sport }) = ( 1 / 3 ) * 0 + ( 2 / 3 ) * 0 = 0
GSPLIT(Car type  { family }) = ( 1 / 3 ) * 0 + ( 2 / 3 )*( 1 / 2 ) = 1 / 3
GSPLIT(Car type  { truck }) = ( 1 / 3 ) \* 0 + ( 2 / 3 )*( 1 / 2 ) = 1 / 3
GSPLIT(Car type  { sport, family}) = ( 2 / 3 )*( 1 / 2 )+( 1 / 3 )* 0 = 1 / 3

GSPLIT(Car type  { sport, truck}) = ( 2 / 3 )*( 1 / 2 )+( 1 / 3 )* 0 = 1 / 3
GSPLIT(Car type  { family, truck }) = ( 2 / 3 )* 0 +( 1 / 3 )* 0 = 0

###### 5. 3 决策树-Gini index

```
Age 23
Risk = High
Risk = Low
Age> 23
Risk = High
Car type  {sport}
Car type  {family, truck}
```

###### 最小的GSPLIT 对应Car type  {sport}的划分。

###### 5. 3 决策树-Gini index

```
.
```

.

..

```
.
.
```

..

```
.
.
.
.
Color?
red
yellow
green
```

###### 5. 3 决策树-Gini index

###### 5. 3 决策树-Gini index

###### 5. 3 决策树

###### 度量的对比

```
信息增益 Information gain:
偏向于多值属性
增益率 Gain ratio:
倾向于不平衡的分裂，其中一个子集比其他小得多
Gini index:
偏向于多值属性
当类数目较大时，计算困难
倾向于导致大小相等的分区和纯度
```

###### 5. 3 决策树

######  ID 3 算法 (Information Gain)

######  C 4. 5 算法 (Gain ratio)

######  CART算法 (Gini Index)

######  PUBLIC算法

######  SLIQ算法

######  SPRINT算法

###### 5. 3 决策树-ID 3 算法

######  ID 3 算法(Iterative Dichotomiser 3 迭代二叉树 3

###### 代)是一个由Ross Quinlan发明的决策树算法。

######  ID 3 在各级节点上选择属性，用信息增益作为属

###### 性选择标准

######  建立在奥卡姆剃刀的基础上：越是小型的决策

###### 树越优于大的决策树

###### 5. 3 决策树

###### 如

###### 果对于同一现象有两种不同的假说，

###### 我们应该采取比较简单的那一种

 Given two models of similar
generalization errors, one should prefer
the simpler model over the more complex
model

###### 5. 3 决策树-ID 3 算法

######  信息增益倾向于特征取值的数目较多的特征，

###### 这样不太合理。

######  ID 3 算法对噪声较为敏感。

######  ID 3 算法由于其理论的清晰，方法简单，学习能

###### 力强，适于处理大规模的学习问题，得到极大

###### 的关注。

###### 5. 3 决策树-C 4. 5 算法

######  C 4. 5 是在ID 3 基础上发展起来的算法，由

###### Quinlan在 1993 年提出。C 4. 5 克服了ID 3 的不足：

```
 用信息增益率来选择属性，它克服了用信息增益选
择属性时偏向多值属性的不足
 在树构造过程中或者完成之后，进行剪枝
 能够完成对连续属性的离散化处理
 能够对于不完整数据进行处理，例如能对未知的属
性值进行处理
 K 次迭代交叉验证
```

Quinlan, J. R. C 4. 5 : Programs for Machine Learning. Morgan Kaufmann Publishers,
1993

###### 5. 3 决策树-C 4. 5 算法

######  划分属性选择：理论和实验表明，采用信息增益率比

```
采用信息增益更好，克服了ID 3 方法选择偏向多值属性的
问题。
```

######  对连续数据的处理：ID 3 算法假设属性值是离散值，

```
但在实际环境中，有很多属性是连续的。C 4. 5 使用一系
列处理过程来对连续的属性划分成离散的属性。
 Step 1 根据训练数据集D中各个属性值对训练数据集进行排序；
 Step 2 利用其中各属性的值对该训练数据集动态地进行划分；
 Step 3 在划分后的得到的不同的结果集中确定一个阈值，该阈
值将训练数据集划分为两个部分；
 Step 4 针对这两边各部分的值分别计算它们的增益或增益率，
以保证选择的划分使得增益最大。
```

###### 5. 3 决策树-C 4. 5 算法

 设在集合D中，连续属性A的取值为{a 1 , a 2 ,...,am}，则任
何在ai和a(i+ 1 )之间的任意取值都可把D分为两部分，即

```
 可以看到已共有m- 1 种分割情况。
 对属性A的m- 1 种分割的任意一种情况，作为该属性的两个离散取
值，重新构造该属性的离散值，再计算每种分割所对应的信息增
益率gainratio(A)，选择最大增益率的分割作为属性A的分枝，即
threshold(A)=ak ， 其中，gainratio(ak)=max[gain-ratio(ai)]。
 则连续属性A可以分割为
A>threshold(A)
A≤ threshold(A)
A
```

######  对缺失数据的处理：在某些情况下，可供使用的数据

```
可能会缺少某些属性的值，需要根据此属性值已知的其
他实例来估计这个缺少的属性值。
 Quinlan 采用的策略：为属性A中的每个可能值赋予一个概
率，而不是简单地将最常见的值赋给ei。
```

###### 5. 3 决策树-C 4. 5 算法

######  生成规则：

```
 只要生成了决策树后，就可把树转换成一个IF-THEN规则
的集合。
 C 4. 5 算法处理规则与其他算法不同在于它把规则存储在一
个数组中，每一行都代表着树中的一个规则，即从树根到
叶子之间的一个路径。
```

###### 5. 3 决策树-C 4. 5 算法

 ID 3 和C 4. 5 的算法，对于相对小的数据集是很有效的。当
这些算法用于非常大的数据集时，有效性和可伸缩性就
成了关注的问题。

 大部分决策树算法都限制训练样本驻留主存。这一限制
就制约了这些算法的可伸缩性。由于训练样本在主存和
高速缓存换进换出，决策树的构造可能效率低下。

 目前已经提出了一些强调可伸缩性的决策树算法，如
SLIQ和SPRINT，它们都能处理分类属性和连续值属性。

###### 5. 3 决策树 - C 4. 5 算法

```
 CART(Classification and Regression Trees)算法是由
L.Breiman，J.Friedman，R.Olshen和C.Stone在著作
Classification and regression trees中提出的一种二叉决
策树算法。
 CART算法与Quinlan提出的ID 3 算法和C 4. 5 不同的是，
使用的属性度量标准是Gini指标。
```

Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. ( 1984 ) Classification
and regression trees. Belmont, CA: Wadsworth International Group.

###### 5. 3 决策树-CART算法

 CART算法采用了二元划分，在分支节点上进行Gini值的
测试，如果满足一定纯度则划分到左子树，否则划分到
右子树，最终生成一棵二叉决策树。

 在只有二元分裂的时候，对于训练数据集D中的属性A将
D分成的D 1 和D 2

###### 5. 3 决策树 - CART 算法

 对于离散值属性，在算法中递归的选择该属性产生最小

```
Gini指标的子集作为它的分裂子集。
```

 对于连续值属性，必须考虑所有可能的分裂点。其策略
类似于ID 3 中介绍的信息增益处理方法

###### 5. 3 决策树 - CART 算法

 CART算法在满足下列条件之一，即视为叶节点不再进行
分支操作
 所有叶节点的样本数为 1 、样本数小于某个给定的最小值Nmin或
者样本都属于同一类的时候；
 决策树的高度达到用户设置的阈值，或者分支后的叶节点中的
样本属性都属于同一个类；
 当训练数据集中不再有属性作为分支选择的时候。

###### 5. 3 决策树 - CART 算法

模型过于简单，在训练数据集合与
测试数据集上都有较高的错误率

```
一棵树可能过分拟合训练数据
```

 分枝太多,某些反映训练数据中的异常，噪音/孤立点

 对未参与训练的样本的低精度预测

```
Overfitting
```

###### 5. 3 决策树

 噪声造成的过拟合(因为它拟合了误标记的训练记录)

 根据少量训练数据做出的模型也容易过拟合。(由于具有代

```
表性的样本，在没有多少训练数据的情况下，学习算法仍
然细化模型；当决策树叶节点没有足够代表性样本时，很
可能做出错误的预测)
```

 大量的候选属性和少量的训练记录最后导致了模型的过分

```
拟合
```

###### 5. 3 决策树

 噪声造成的过分拟合(因为它拟合了误标记的训练记录)

###### 5. 3 决策树

 根据少量训练数据做出的模型也容易过拟合。(由于具有代

```
表性的样本，在没有多少训练数据的情况下，学习算法仍然
细化模型；当决策树叶节点没有足够代表性样本时，很可能
做出错误的预测)
```

###### 5. 3 决策树

```
http://scott.fortmann-roe.com/docs/BiasVariance.html
```

###### 5. 3 决策树

均方误差

```
偏置 方差
```

 多样性预测定理（Diversity

```
Prediction Theorem）：使用
不同的模型对某一事物进行预
测，加权平均的结果会更加接
近真实值。（ 多样性优于能力 ）
均值与真实值的
平方离差
个体估值与真实值
平方离差的均值 群体多样性
```

###### 5. 3 决策树

 过拟合给决策树带来不必要的复杂性

 根据训练数据很难评测决策树对于未知数据的分类能力

 预裁剪（Pre-Pruning）：在建树过程中裁剪

 后裁剪（Post-Pruning）：在建树完成后裁剪

###### 5. 3 决策树

```
训练数据
此处剪枝 测试数据
决策树深度
```

错

误

率

```
剪枝
```

###### 5. 3 决策树

 Stop the algorithm before it becomes a fully-grown

```
tree
```

 More restrictive stopping conditions:

- Stop if number of instances is less than some
  user-specified threshold
- Stop if expanding the current node does not
  improve impurity measures (e.g., information gain).

###### 5. 3 决策树

 Grow decision tree to its entirety

 Trim the nodes of the decision tree in a bottom-up

```
fashion
```

 If generalization error improves after trimming,

```
replace sub-tree by a leaf node.
```

 Class label of leaf node is determined from majority

```
class of instances in the sub-tree
```

###### 5. 3 决策树

```
是使用一系列学习器进行学
```

习，并使用某种规则把各个学习结果进行整合，从而获得

比单个学习器更好的学习效果的一种机器学习方法

多个弱学习器通过一定的集成方法可以集成为
一个强学习器

```
 强学习算法: 准确率很高的学习算法
 弱学习算法: 准确率不高,仅比随机猜测略好
```

###### 5. 3 决策树-集成学习

 算法描述：

```
1. 给出一个训练数据集D
2. 产生多个分类器，h 1 , h 2 , ..., hL
3. 可选项：决定每个对应的权重w 1 , w 2 , ..., wL
4. 分类预测新的数据，根据Σi wi×hi > θ
```

 关键问题:

```
1. 如何产生不同分类器 h 1 ,h 2 ,...
2. 如何集成各个分类器 f(h 1 (x),h 2 (x),...)
```

###### 5. 3 决策树-集成学习

Bagging算法是最早，也是最具有指导意义和实施最简单，而且

效果惊人的好的集成学习算法。Bagging 算法的多样性是通过由有放

回抽取训练样本来实现的，用这种方式随机产生多个训练数据的子集

，在每一个训练集的子集上训练一个分类器，最终分类结果是由多个

分类器的分类结果多数投票而产生的。

方法：产生N个训练集，然后训练N个基分类器，再对N个基分类

器进行投票。适用于不稳定的学习过程，即数据集的一个小变动会产

生大的差别，例如决策树、多层感知器。

###### 5. 3 决策树-集成学习

Boosting 算法通过顺序给训练集中的数据项重新加权创造不同的基础学

习器。Boosting 算法的最终模型是一系列基础学习器的线性组合而且系数依

赖于各个基础学习器的表现。虽然 Boosting 算法有很多的版本，但是使用

目前最广泛的是 AdaBoost 算法。

方法：重复应用一个基学习器来修改训练数据集，这样在预定数量的迭
代下可以产生一系列的基学习器。在训练开始，所有的数据项都被初始化为

同一个权重，在这次初始化之后，每次增强的迭代都会生成一个适应加权之

后的训练数据集的基础学习器。每一次迭代的错误率都会计算出来，而且正

确划分的数据项的权重会被降低，然后错误划分的数据项权重将会增大。权

重小的样本被再次抽到的概率降低。

###### 5. 3 决策树-集成学习

**https://medium.com/swlh/gradient-boosting-trees-for-classification-a-beginners-
guide- 596 b 594 a 14 ea**

###### 5. 3 决策树-集成学习

 GBDT广泛应用于工业界，特别在金融领域的风险控制

 梯度提升（ **Gradient Boosting** ）是一类由 **Gradient descent**

```
和 Boosting 组成的机器学习模型，包含三个部分：加性模
型、损失函数和弱学习器。
```

 基本思想：

 以分步逼近方式拟合一个加性模型 ；
 在每个步骤，引入一个弱学习器来弥补现有弱学习器的不足。

 不足是由梯度标识的。Adaboost中，不足是通过高权重样本来标识。

###### 5. 3 决策树-梯度提升树（GBDT）

```
 通俗解释（以Regression任务为例）：
 给你一组样本数据 ，任务是拟合一个模型
以最小化平方损失。
 你的朋友想帮助你并给你一个模型 F ，你发现 F 很好但不完美。有
一些错误： 而 ， F( 而 ...
 改进这个模型要遵循两个规则：
① 不能对 F 进行修改，包括结构与参数；
② 可以增加一个辅助模型 h （回归树），改进后模型就成为
。
```

###### 5. 3 决策树-梯度提升树（GBDT）

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 我们希望：
 或者说：
 即使找不到完美的 h ，但是可能会找到一个近似的 h 。
```

###### 5. 3 决策树-梯度提升树（GBDT）

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 怎么找 h ：只需要从下面数据中拟合。 比 更好。
称为残差（residuals）。
 如果 不够好，可以迭代上述过程。
```

###### 5. 3 决策树-梯度提升树（GBDT）

**12 .. hhttttpp::////wenw.wwi.k cipcse.d niae.uo.regd/uw/hikoim/Ger/vaidpi/etneat_cdhe/MsceLnctourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 梯度下降是一种一阶迭代优化算法，用于寻找一个可微函数的局部
最小值。其原理是在当前点上沿函数梯度的反方向多步移动，因为
这是最陡峭的下降方向。
```

###### 5. 3 决策树-梯度提升树（GBDT）

```
 平方损失函数
 需要调整 来最小化
 可以将 视为变量，并对其导数
 残差可以看作负梯度
```

###### 5. 3 决策树-梯度提升树（GBDT）

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 可用梯度下降法更新算法
 用 "梯度 "的概念来总结刚才的算法
 从一个初始模型开始，例如， ，迭代直到收敛：
计算负梯度 ；
拟合 的回归树 ；
```

###### 5. 3 决策树-梯度提升树（GBDT）

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 上述算法中，可以使用不同的损失函数。
 平方损失易于计算，但易受边界点影响；模型更关注边界点，
影响整体性能。
 绝对值损失对边界点更鲁棒。
```

###### 5. 3 决策树-梯度提升树（GBDT）

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 上述算法中，可以使用不同的损失函数。
 Huber 损失能增强平
方误差的鲁棒性。
 通常残差不等于负
梯度！
```

###### 5. 3 决策树-梯度提升树（GBDT）

**[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).**

```
 基本框架：
 以分步逼近方式拟合一个加性模型；
 在每个步骤，引入一个新模型来弥补已有模型的不足；
 不足是由负梯度标识的；
 对于每类损失函数，设计一个梯度提升算法。
```

https://explained.ai/gradient-boosting/descent.html

###### 5. 3 决策树-梯度提升树（GBDT）

[http://www.](http://www./) ccs. neu.edu/home/vip/teach/MLcourse/ 4 _ boosting/slides/ gradient_boosting. pdf ( 2016 ).

```
 梯度提升树用于分类
```

**https://blog.paperspace.com/gradient-boosting-for-classification/**

```

 损失函数采用交叉熵损失函数
```

###### 5. 3 决策树-梯度提升树（GBDT）

```
比率对数
p表示预测样本为yi的概率
 梯度提升树用于分类
```

https://blog.paperspace.com/gradient-boosting-for-classification/

```
 STEP I： 用一个常量作为初始化模型
 STEP 2 : for m = 1 to M:
```

- ；
- 利用 拟合回归树； 表示第
  m个树的第 j 个叶节点。叶节点包含多个样本，可能
  是多值的，需要确定唯一的数值。
- 对于树中的叶节点，我们要计算输出值

###### 5. 3 决策树-梯度提升树（GBDT）

```
 梯度提升树用于分类
```

https://blog.paperspace.com/gradient-boosting-for-classification/

```
的 计算与STEP 2. 1 类似。区别是， STEP 2. 1 针对所有样本；这个
步骤只对属于叶节点的样本。
很难直接求解！
可以利用泰勒公式近似求解。
```

###### 5. 3 决策树-梯度提升树（GBDT）

- Observed*log(odds)+log( 1 +elog(odds))

```
 梯度提升树用于分类
```

https://blog.paperspace.com/gradient-boosting-for-classification/

###### 5. 3 决策树-梯度提升树（GBDT）

```
 梯度提升树用于分类
```

https://blog.paperspace.com/gradient-boosting-for-classification/

```
 STEP 3 : 输出由M棵树组成的模型
```

###### 5. 3 决策树-梯度提升树（GBDT）

```
 梯度提升树用于分类（以泰坦尼克号幸存者预测为例）
```

https://blog.paperspace.com/gradient-boosting-for-classification/

```
 选择初始分类器：
```

###### 5. 3 决策树-梯度提升树（GBDT）

```
F 0 =ln( 4 / 2 )
 梯度提升树用于分类（以泰坦尼克号幸存者预测为例）
```

https://blog.paperspace.com/gradient-boosting-for-classification/

```
叶节点可能是多值的，造成不一
致。不能直接把之前得到的单叶
和这棵树相加。
```

###### 5. 3 决策树-梯度提升树（GBDT）

```
 梯度提升树用于分类（以泰坦尼克号幸存者预测为例）
```

https://blog.paperspace.com/gradient-boosting-for-classification/

```
经验表明，小的学习率可以用测试
集取得更好预测，通常较小 0. 1
新的概率值是什么？
```

###### 5. 3 决策树-梯度提升树（GBDT）

```
ln( 4 / 2 )+ 0. 1 * 4. 29 = 1. 12 p=^ e^1.^12 /^1 +e^1.^12 =^0.^75
```

 梯度提升树用于分类（以泰坦尼克号幸存者预测为例）

###### 5. 3 决策树-梯度提升树（GBDT）

1. (^1) 分类的定义与基本问题
2. (^2) 分类的性能评测
3. (^3) 决策树
4. 4
5. 5
   贝叶斯分类
6. (^6) 其他学习问题
   k-nearest neighbor(k-NN)算法

###### 是一种基于

###### 的分类方法。它采用监督学习方式，

###### 分类前必须知道分类类别；通过训练样本数据

###### ，有效处理未分类数据。

######  它与神经网络和决策树相比有很多优势，准确

###### 率高、对噪声健壮性、学习过程简单等。

###### 5. 4 贝叶斯分类

###### 英国数学家托马斯·贝叶斯

###### (Thomas Bayes)在 1763 年发表

###### 的论著中，首先提出了这个定理

###### 。

```
主要用来描述两个条件概率P(A|B) 和 P(B|A) 之间的关
系
```

###### 5. 4 贝叶斯分类

######  逆概问题

 一所学校里面有 60 % 的男生， 40 % 的女生。男生总是穿

```
长裤，女生则一半穿长裤一半穿裙子。穿长裤的学生是男
生的概率是多大吗？
```

###### 5. 4 贝叶斯分类

 称为先验概率，它反映了各种“原因”发生的

```
可能性大小，一般是以往经验的总结，在这次试验
前已经知道。现在若产生了事件B，这个信息将有
助于探讨事件发生的“原因”。
```

 条件概率 称为后验概率，它反映了试验

```
之后对各种“原因”发生的可能性大小的新知识。
```

###### 5. 4 贝叶斯分类

```
1
( | ) ( ) ( | )
( ) ( | )
i i i
i i i
P A B P A P B A
P A P B A



```

### 

```
P ( Ai | B )
P ( Ai )
```

例：贝叶斯定理在检测吸毒者时很有用。假设一个常规的检

测结果的 均为 99 %.

```
 被检者吸毒时，每次检测呈阳性（+）的概率为 99 %。
 检者不吸毒时，每次检测呈阴性（-）的概率为 99 %。
```

假设某公司将对其全体雇员进行一次鸦片吸食情况的检测，

已知 0. 5 %的雇员吸毒。我们想知道，每位医学检测呈阳性
的雇员吸毒的概率有多高P(D |+)？令 “D”为雇员吸毒事件，
“ N”为雇员不吸毒事件， “ +”为检测呈阳性事件。

###### 5. 4 贝叶斯分类

 P(D)代表雇员吸毒的概率，不考虑其他情况，该值为 0. 005 。这个值就
是D的先验概率。
 P(N)代表雇员不吸毒的概率，该值为 0. 995 ，也就是 1 - P(D)。
 P(+|D)代表吸毒者阳性检出率，由于阳性检测准确性是 99 %，因此该
值为 0. 99 。
 P(+|N)代表不吸毒者阳性检出率，也就是出错检测的概率，该值为

1. 01 ，因为对于不吸毒者，其检测为阴性的概率为 99 %。
    P(+)代表不考虑其他因素的影响的阳性检出率。我们可以通过全概率
   公式计算得到：此概率 = 吸毒者阳性检出率（ 0. 5 % x 99 % =
2. 495 %)+ 不吸毒者阳性检出率（ 99. 5 % x 1 % = 0. 995 %)。
   P(+)= 0. 0149 是检测呈阳性的概率。

###### 5. 4 贝叶斯分类

根据上述描述，可以计算某人检测呈阳性时确实吸毒的条件概率P(D|+)：

尽管检测结果可靠性很高，但是只能得出结论：如果某人检测呈阳性，
那么此人是吸毒的概率只有 33 %，也就是说不吸毒的可能性较大。测试
的条件（本例中指D，雇员吸毒）越难发生，发生误判的可能性越大。

###### 5. 4 贝叶斯分类

例：现在随机选择一个碗，从中摸出一颗糖，发现是水果

糖。请问这颗水果糖来自一号碗的概率有多大？

###### 5. 4 贝叶斯分类

 先验概率：假定H 1 表示一号碗，H 2 表示二号碗。在取
出水果糖之前，这两个碗被选中的概率相同。因此，
P(H 1 )=P(H 2 )= 0. 5 ，即没有做实验之前，来自一号碗的
概率是 0. 5 。

 后验概率：再假定E表示水果糖，所以问题就变成了在
已知E的情况下，来自一号碗的概率有多大，即求
P(H 1 |E)。即在E事件发生之后，对P(H 1 )的修正。

 根据条件概率公式，得到

###### 5. 4 贝叶斯分类

 已知P(H 1 )= 0. 5 ，P(E|H 1 )为一号碗中取出水果糖的概
率等于 0. 75 ，那么求出P(E)就可以得到答案。根据全概
率公式，

 将数字代入原方程，得到

 这表明，来自一号碗的概率是 0. 6 。也就是说，取出水
果糖之后，H 1 事件的可能性得到了增强。

###### 5. 4 贝叶斯分类

###### 5. 4 贝叶斯分类

###### 5. 4 贝叶斯分类

 中文分词：

```
 南京市长江大桥
```

- 南京市/长江大桥;
- 南京/市长/江大桥
   乒乓球拍卖完了
- 乒乓球/拍卖完了
- 乒乓球拍/卖完了

###### 5. 4 贝叶斯分类

#####  分词问题：令 X 为句子，Y 为词串，需要寻找使

##### 得 P(Y|X) 最大的 Y ：

##### P(Y|X) ∝ P(Y)*P(X|Y)

######  这种分词方式（词串）的可能性 乘以 这个词串生

###### 成我们的句子的可能性。

######  可以近似地将 P(X|Y) 看作是 1 ，因为任意假想的

###### 一种分词方式总是精准地生成的。

```
http://blog.csdn.net/memory 513773348 /article/details/ 16966987
```

###### 5. 4 贝叶斯分类

#####  分词问题变成极大化 P(Y) ，也就是寻找一种分词

###### 使得这个词串的概率最大化。

#####  设Y= W 1 , W 2 , ..., Wn ， P(Y)=P(W 1 , W 2 , ..., Wn )

######  根据联合概率的公式展开：

##### P(Y)=P(W 1 , W 2 , ..., Wn ) = P(W 1 ) * P(W 2 |W 1 ) *

##### P(W 3 |W 2 , W 1 ) * ...*P(Wn | W 1 , W 2 , ..., Wn- 1 )

(^) http://blog.csdn.net/memory 513773348 /article/details/ 16966987

###### 5. 4 贝叶斯分类

######  数据稀疏问题：语料库再大也无法统计出

##### P(Wn | W 1 , W 2 , ..., Wn- 1 )

######  假设句子中一个词的出现概率只依赖于它前面的

##### 有限的 k 个词（k= 1 , 2 , 3 ）。

```
http://blog.csdn.net/memory 513773348 /article/details/ 16966987
```

###### 5. 4 贝叶斯分类

#####  P(Y)=P(W 1 , W 2 , ..., Wn )=P(W 1 )* P(W 2 |W 1 )*

##### P(W 3 |W 2 )* ...* P(Wn|Wn- 1 ) （假设每个词只依赖于

##### 它前面的一个词）。而统计 P(Wn|Wn- 1 ) 就不再受

###### 到数据稀疏问题的困扰了。

```
http://blog.csdn.net/memory 513773348 /article/details/ 16966987
```

###### 5. 4 贝叶斯分类

```
 南京市长江大桥
```

- 南京市/长江大桥 168 , 000
- 南京/市长/江大桥 68 , 800
   乒乓球拍卖完了
- 乒乓球/拍卖完了 13 , 200
- 乒乓球拍/卖完了 27 , 400

http://blog.csdn.net/memory 513773348 /article/details/ 16966987

###### 5. 4 贝叶斯分类

 朴素贝叶斯分类器是基于一个很强的假定：决定各分
类的属性之间是相互独立的。

 每个数据样本用一个n维向量 表示
，分别描述样本n个属性 的度量。假
定有m个类 ，给定一个未分类样本
，朴素贝叶斯分类器将它分配给类 i ，当且仅当

###### 5. 4 贝叶斯分类

```
X ( x 1 , x 2 , , xn )
A 1 , A 2 , , An
C 1 , C 2 ,, Cm
```

*P* ( *Ci* | *X* )  *P* ( *Cj* | *X* ), 1  *j*  *m* , *j*  *i*

 根据贝叶斯定理 ，最大化

```
即可进行分类。
```

 由于 可看作常数，因此只要使 最大
即可

 这样极大地减少了计算代价： 只需要统计类的分布

###### 5. 4 贝叶斯分类

```
( | ) ( | ) ( )
( )
P Ci X P X Ci P Ci
 P X P ( Ci | X )
P ( X ) P ( X | Ci ) P ( Ci )
1 1 2 2
1
( | ) ( ) ( , , , | ) ( )
( | ) ( )( )
i i n n i i
n
k k i i
P X C P C P A x A x A x C P C
P x C P C

   
```

## 

```

由各属性独立
```

###### 5. 4 贝叶斯分类

https://www.cs.cornell.edu/courses/cs 4780 / 2018 fa/lectures/lecturenote 05 .html

###### 5. 4 贝叶斯分类

https://www.cs.cornell.edu/courses/cs 4780 / 2018 fa/lectures/lecturenote 05 .html

```
Case # 1 : Categorical features
```

###### 5. 4 贝叶斯分类

https://www.cs.cornell.edu/courses/cs 4780 / 2018 fa/lectures/lecturenote 05 .html

```
Case# 2 : Multinomial features
x 1 +x 2 +x 3 =n.
```

###### 5. 4 贝叶斯分类

https://www.cs.cornell.edu/courses/cs 4780 / 2018 fa/lectures/lecturenote 05 .html

```
Case # 3 : Continuous features (Gaussian Naive Bayes)
```

###### 5. 4 贝叶斯分类

###### 5. 4 贝叶斯分类

 例:数据来自于某个生产饼干公司的员工的信息

```
样本 性别 工作内容 学历 年龄 薪水
1 女 送货 大学 20 - 30 20000 - 30000
2 男 包装 大学 〉 40 〉 40000
3 男 烘烤 大学 30 - 40 20000 - 30000
4 男 包装 高中 30 - 40 20000 - 30000
5 男 送货 大学 〉 40 30000 - 40000
6 女 烘烤 高中 20 - 30 20000 - 30000
7 男 烘烤 大学 20 - 30 < 20000
8 女 包装 大学 30 - 40 20000 - 30000
9 男 烘烤 大学 〉 40 20000 - 30000
10 男 包装 大学 20 - 30 < 20000
```

###### 5. 4 贝叶斯分类

 我们来判断一下工作内容为包装，学历为大学，年龄在

```
30 - 40 之间，薪水在 20000 - 30000 之间的员工的性别。
```

 首先根据训练样本计算各属性相对于不同分类结果的条

```
件概率：
```

P（工作内容=包装|性别=女）= 1 / 3
P（学历=大学|性别=女）= 2 / 3
P（年龄= 30 - 40 |性别=女）= 1 / 3
P（薪水= 20000 - 30000 |性别=女）= 1

###### 5. 4 贝叶斯分类

P（工作内容=包装|性别=男）= 3 / 7
P（学历=大学|性别=男）= 6 / 7
P（年龄= 30 - 40 |性别=男）= 2 / 7
P（薪水= 20000 - 30000 |性别=男）= 3 / 7

P（性别=女）= 3 / 10
P（性别=男）= 7 / 10

###### 5. 4 贝叶斯分类

P（包装|女）*P（大学|女）*P（ 30 - 40 |女）*
P（ 20000 - 30000 |女）*P（女）= 0. 0222

P（包装|男）*P（大学|男）*P（ 30 - 40 |男）*
P（ 20000 - 30000 |男）*P（男）= 0. 0315

因为 0. 0315 > 0. 0222 ，因此，我们认为具有上述属性的员工
的性别为男性。

###### 5. 4 贝叶斯分类

###### 5. 4 贝叶斯分类

###### 5. 4 贝叶斯分类

###### 5. 4 贝叶斯分类

###### 症状 职业 疾病

打喷嚏 护士 感冒

打喷嚏 农夫 过敏

头痛 建筑工人 脑震荡

头痛 建筑工人 感冒

打喷嚏 教师 感冒

头痛 教师 脑震荡

###### 打喷嚏的建筑工人?

###### 5. 4 贝叶斯分类

```
Underflow问题：计算多个概率的乘积，而概率的值往往都
较小，相乘之后可能会underflow，变成 0 ，得到错误的结
果。使用频率来代替概率，有可能出现概率为 0 的情况。
```

###### 5. 4 贝叶斯分类

풍� 푷풓� 푪 ∗푷풓� �� 푪 ∗푷풓� �� 푪 ∗...∗푷풓� � 푪 =
풍� 푷풓� 푪 +풍� 푷풓� �� 푪 +풍� 푷풓� �� 푪 +...
+풍� 푷풓� � 푪

###### 5. 4 贝叶斯分类

 优点

- 易于实现
- 大多数情况下取得好结果，可以与决策树和经过挑选
  的神经网络分类器相媲美

 缺点

- 假设: 类条件独立性, 损失精度
- 实际中, 变量间存在依赖
   医院：患者：简介：年龄，家族病史等
   症状：发烧，咳嗽等疾病：肺癌，糖尿病等
   Dependencies among these cannot be modeled by NBC

1. (^1) 分类的定义与基本问题
2. (^2) 分类的性能评测
3. (^3) 决策树
4. 4
5. 5
   贝叶斯分类
6. (^6) 其他学习问题
   k-nearest neighbor(k-NN)算法

```
：如果一个样本在特征空间中的 最相似(
最邻近)的样本中的大多数属于某一个类别，则该样本
也属于这个类别
是一种 算法，分类器不需要使用
训练集进行训练(分类模型是在测试样本到来以后才生
成)，训练时间复杂度为
的计算复杂度和训练集中文档数目成正比，
分类时间复杂度为 ， 为训练集中数据对象总数
```

Lazy-learning：仅存储样本数据，直到待测试数据到达时，才进行分类处
理
Eager-learning：在待测试数据达到之间，构建分类模型

###### 5. 5 k-nearest neighbor(k-NN)算法

输入：训练数据集 D， x’ 为测试对象

处理： , 计算 ，

选择 ， 为与测试对象最近的k个对象的集合

输出：

```
 v 为类别标识
 xi 为 x’ 的第I 个最邻近数据对象, yi 为 xi 的类别
 当v= yi 是 true ，I(v= yi ) 返回 1 ，否则返回 0
```

###### 5. 5 k-nearest neighbor(k-NN)算法

######  绿色圆要被决定赋予哪个类，是红色三角形还是

##### 蓝色四方形？如果k= 3 ，由于红色三角形所占比例

###### 为 2 / 3 ，绿色圆将被赋予红色三角形那个类，如果

##### k= 5 ，由于蓝色四方形比例为 3 / 5 ，因此绿色圆被

###### 赋予蓝色四方形类。

###### 5. 5 k-nearest neighbor(k-NN)算法

######  例：需要判断纸巾的品质好坏，品质好坏可以抽

###### 像出两个向量，一个是“酸腐蚀的时间”，一个是“

###### 能承受的压强”。如果样本空间如下：

```
向量X 1
耐酸时间（秒）
向量X 2
圧强(公斤/平方米) 品质Y
7 7 坏
7 4 坏
3 4 好
1 4 好
```

###### 5. 5 k-nearest neighbor(k-NN)算法

######  如果 X 1 = 3 和 X 2 = 7 ，这个纸巾的品质是什么呢

###### ？假设K= 3 ，下面计算（ 3 ， 7 ）到所有点的距离

###### 。

```
向量X 1
耐酸时间（秒）
向量X 2
圧强(公斤/平方米) 计算到^ (^3 ,^7 )的距离 向量Y
7 7 ( 7 - 3 )^2 +( 7 - 7 )^2 = 16 坏
7 4 ( 7 - 3 )^2 +( 4 - 7 )^2 = 25 N/A
3 4 ( 3 - 3 )^2 +( 4 - 7 )^2 = 9 好
1 4 ( 1 - 3 )^2 +( 4 - 7 )^2 = 13 好
```

###### 5. 5 k-nearest neighbor(k-NN)算法

 k太小，则对噪音数据过于敏感

 k太大，则可能会包含其他类别的点

 当只有两类中，为避免 ， k应为奇数

```
 从k= 1 开始用一个测试数据集评测分类器的错误率
 重复k=k+ 2
 选择最小错误率对应的k值
```

###### 5. 5 k-nearest neighbor(k-NN)算法

解决方法：不同距离下数据对象对待测数据分类影响力不同，

引入权重，权重和数据对象、测试对象之间距离相关

###### 5. 5 k-nearest neighbor(k-NN)算法

 Distance between neighbors could be dominated by
some attributes with relatively large numbers (e.g.,
income in our example). Important to normalize some
features (e.g., map numbers to numbers between 0 - 1 )
Exmaple: Income
Highest income = 500 K
Davis’s income is normalized to 95 / 500 , Rachel income is normalized to
215 / 500 , etc.)

###### 5. 5 k-nearest neighbor(k-NN)算法

Customer Age Income
(K)

```
No.
cards
```

John 35 / 63 =

1. 55

```
35 / 200 =
0. 175
```

¾=

1. 75
   Rachel 22 / 63 = 0. 3
   4

```
50 / 200 =
0. 25
```

2 / 4 =

1. 5
   Hannah 63 / 63 =
   1

200 / 200 = 1 ¼=

1. 25
   Tom 59 / 63 =
2. 93

170 / 200 = 0. 85 ¼=

1. 25
   Nellie 25 / 63 =
2. 39

```
40 / 200 =
0. 2
```

4 / 4 =
1
David 37 / 63 =

1. 58

```
50 / 200 =
0. 25
2 / 4 =
0. 5
Response
No
Yes
No
No
Yes
```

###### 5. 5 k-nearest neighbor(k-NN)算法

 Distance works naturally with numerical attributes
D(Rachel&Johm)= sqrt [( 35 - 37 )^2 +( 35 - 50 )^2 +( 3 - 2 )^2 ]= 15. 1 6
What if we have nominal attributes?
Example: married
Customer Married Income
(K)

```
No.
cards
John Yes 35 3
Rachel No 50 2
Hannah No 200 1
Tom Yes 170 1
Nellie No 40 4
David Yes 50 2
```

###### 5. 5 k-nearest neighbor(k-NN)算法

Customer ID Debt Income Marital Status Risk

```
Abel High High Married Good
Ben Low High Married Doubtful
Candy Medium Very low Unmarried Poor
Dale Very high Low Married Poor
Ellen High Low Married Poor
Fred High Very low Married Poor
George Low High Unmarried Doubtful
Harry Low Medium Married Doubtful
Igor Very Low Very High Married Good
Jack Very High Medium Married Poor
```

###### 5. 5 k-nearest neighbor(k-NN)算法

######  K = 3

######  Distance

######  Voting scheme

Customer ID Debt Income Marital Status Risk
Zeb High Medium Married?
Yong Low High Married?
Xu Very low Very low Unmarried?
Vasco High Low Married?
Unace High Low Divorced?
Trey Very low Very low Married?
Steve Low High Unmarried?

###### 5. 5 k-nearest neighbor(k-NN)算法

 易于理解与实现
 相对较低的错误率
 适合类标签比较多的数据对象的分类

 当训练数据集较大时，待测对象与样本中数据对象的计算代
价大

 大训练数据集很难存放在内存中
 无关属性：Imagine instances described by 20 features but
only 3 are relevant to target function; nearest neighbor is
easily misled when instance space is high-dimensional
 非平衡样本数据集

###### 5. 5 k-nearest neighbor(k-NN)算法

###### 5. 5 k-nearest neighbor(k-NN)算法

1. (^1) 分类的定义与基本问题
2. (^2) 分类的性能评测
3. (^3) 决策树
4. 4
5. 5
   贝叶斯分类
   k-nearest neighbor(k-NN)算法

 Classification is a form of data analysis that can be used to
extract models describing important data classes or to predict
future data trends.
 Predictive accuracy, computational speed, robustness,
scalability, and interpretability are the criteria for the
evaluation of classification methods.
 ID 3 , C 4. 5 , and CART are greedy algorithms for the induction of
decision trees.
 Naïve Bayesian classification is based on Bayes, theorem of
posterior probability.
 K-nearest neighbor algorithm (k-NN) is a method
for classifying objects based on closest training examples in
the feature space

###### 总结

 示意图：用于描述事物原理、结构等的略图，通常由点、线、矩形等
几何形状或少量自然图像构成

###### 作业：示意图识别

```
文本表示或标
记语言描述
```

######  要求： 不超过 4 人一组；

###### 最后一周课堂上 PPT 展示，每组 5 分钟。

###### 作业：示意图识别